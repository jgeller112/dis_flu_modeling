[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Perceptual Disfluency and Recognition Memory: A Mathematical and Computational Analysis of RT Distributions",
    "section": "",
    "text": "Reproducible document for the paper"
  },
  {
    "objectID": "intro.html#the-perceptual-disfluency-effect",
    "href": "intro.html#the-perceptual-disfluency-effect",
    "title": "2  Perceptual Disfluency and Recognition Memory: A Response Time Distributional Analysis",
    "section": "2.1 The perceptual disfluency effect",
    "text": "2.1 The perceptual disfluency effect\nThe relationship between perceptual disfluency and long-term memory has a long and storied history. While it is not quite clear where the term perceptual disfluency effect originated from, the idea behind it goes back to the late 80s with the work of Nairne (1988). Under the term perceptual-interference effect, Narine employed the technique of backward masking with hash marks ( e.g., ####) with a quick presentation time to make word encoding noisier during study. Because the word is presented and masked so quickly, considerable effort is needed to identify the word. Since then, different types of perceptual disfluency manipulations have shown to elicit a similar effect, such as high-level blurring (Rosner et al., 2015), word inversion (Sungkhasettee et al., 2011) , small text size (Halamish, 2018), handwritten cursive (Geller et al., 2018), and other unusual or difficult-to-read typefaces (Geller & Peterson, 2021; Weissgerber & Reinhard, 2017; Weltman & Eakin, 2014).\nGiven the simplicity and ease in which perceptual disfluency can be implemented, it is not surprising researchers began touting the educational implications of such a manipulation. Perceptual disfluency as a possible educational intervention started to garner more support with the publication of Diemand-Yauman et al. (2011). Across two experiments, Diemand-Yauman et al. (2011) showed placing learning materials in disfluent typefaces (e.g., Comic Sans, Bodoni MT, Haettenschweiler, Monotype Corsiva) improved memory in the lab (when learning about space aliens) and in a high school classroom where students learned about a variety of different content areas  (i.e., AP English, Honors English, Honors Physics, Regular Physics, Honors U.S. History, and Honors Chemistry) from PowerPoints with information presented in difficult to read typefaces.\nUnfortunately, the replicability of effects related to perceptual disfluency has come under scrutiny. A recent case in point is the typeface Sans Forgetica, developed through a collaboration among marketers, psychologists, and graphic designers. Originally launched with the strong claim that it enhances memory retention, the typeface features backward-slanting letters and gaps within each letter to promote ‘desirable difficulties.’ The theory suggests that these design features require individuals to ‘generate’ the missing parts of each word, thus aiding in memory retention. However, several subsequent studies have failed to replicate these claims, finding Sans Forgetica to be forgettable (Cushing & Bodner, 2022; Geller et al., 2020; Huff et al., 2022; Roberts et al., 2023; Taylor et al., 2020; Wetzler et al., 2021). Similar results have been found for various other perceptual disfluency interventions, such as small font sizes (Rhodes & Castel, 2008), difficult-to-hear stimuli (Rhodes & Castel, 2009), minor blurring (Yue et al., 2013), and alternative typefaces (Rummer et al., 2015).\nThe mixed evidence above caused Kuhl et al. (2014) and others (e.g., Oppenheimer & Alter, 2014) to suggest researchers begin exploring boundary conditions of the disfluency effect. The importance of testing the effects of disfluency in the presence of other variables is key to its usefulness as an educational intervention. Geller et al. (2018), for instance, found level of disfluency (more vs. less disfluent) mattered. Using easy-to-read and hard-to-read handwritten cursive words, Geller et al. showed that there is a Goldilocks zone for perceptual disfluency effects. That is, stimuli cannot be too easy to read (e.g., print words) or too hard to read (hard-to-read cursive). Only when the stimulus was moderately difficult (easy-to-read cursive), or just right, did memory improve. In another paper, Geller & Peterson (2021) demonstrated that memory benefits for disfluent stimuli are more robust when test expectancy is low. That is, a disfluency effect is only seen when participants are not told about an upcoming memory test. The authors reasoned that knowing about a memory test engaged a strategy where all stimuli get processed to a deeper level, regardless of how perceptually disfluent the stimulus is. Additionally, a few studies have noted the importance of individual differences. Eskenazi & Nix (2021) showed better spellers remembered more words and meanings than poor spellers when placed in a disfluent font.\nThough perceptual disfluency can occur in certain scenarios, its usefulness in educational settings, where students anticipate tests, could be limited. However, Geller and Peterson (2021) contended that perceptual disfluency has practical implications, particularly due to our reliance on incidental memory in everyday decision-making. For this to be effective, though, predicting when and where disfluency will occur is crucial."
  },
  {
    "objectID": "intro.html#theoretical-accounts-of-the-disfluency-effect",
    "href": "intro.html#theoretical-accounts-of-the-disfluency-effect",
    "title": "2  Perceptual Disfluency and Recognition Memory: A Response Time Distributional Analysis",
    "section": "2.2 Theoretical accounts of the disfluency effect",
    "text": "2.2 Theoretical accounts of the disfluency effect\nTo achieve this aim, we require a better understanding of the mechanisms involved in eliciting the disfluency effect. Several theories have been proposed to explain this phenomenon. Geller et al. (2018) provided a review of three theories put forth to explain the disfluency effect. The metacognition account of disfluency (Alter, 2013) posits disfluency acts as a signal to engage in more system 2 thinking (Kahneman, 2011), or deeper levels of processing (Craik & Lockhart, 1972). Within this account, disfluency arises after the stimulus has been identified. As a results, the type of disfluency experienced does not matter.\nThe compensatory processing account (Mulligan, 1996) suggests that the disfluency effect is a result of increased top-down processing from lexical and semantic levels of representation. This framework is largely based on the popular interactive activation mode (IA model) of word recognition (McClelland & Rumelhart, 1981). In the IA model, visual input activates representations at three levels of representation: the feature, letter, and word levels. Activation in the IA model is both feed-forward and feedbackward. Thus, when there is perceptual noise (such as by a mask), there is increased top-down processing from higher, lexical or semantic, levels to aid in word identification. It is this increased top-down processing that produces better memory.\nRecently, Ptok et al. (2019) put forth a limited capacity and stage-specific model to explain conflict-encoding effects like the perceptual disfluency effect. Within their model, memory effects arising from encoding conflict rely on (1) the stage or level of processing tapped by the task and (2) monitoring and control processes. Across six experiments, Ptok et al. demonstrated better recognition memory for target words when shown with incongruent versus congruent semantic distractor words  (i.e., category labels of size, animacy, or gender; e.g., “Chair - Alive” vs. “Chair - Inanimate), but no memory benefit for incongruent versus congruent response distractor words (e.g., Lisa -”left”/ Lisa - “right”). While both tasks resulted in conflict evinced by longer RTs to targets preceded by incongruent primes, only when the encoding task focused attention on target meaning (i.e., semantic categorization) rather than response selection did a memory benefit arise. In a follow-up set of experiments, Ptok et al. (2020) replicated this pattern of findings, and in addition, provided physiological evidence from cognitive pupillometry (i.e., the study of eye pupil size and how it relates to cognitive processing). They observed larger pupil size (which has been taken as an index of cognitive control; see (Wel & Steenbergen, 2018), for a review) for semantic incongruent and response incongruent primes, but only observed a memory benefit for semantic incongruent conditions. Interestingly, they also showed that these memory benefits can be mitigated by manipulating endogenous attention. Ptok et al. (2020) (Experiment 3) were able to eradicate the conflict encoding benefit by having participants sit in a chinrest and focus on the task. This is similar to what has been found in the perceptual disfluency literature. For example, having participants study words in anticipation for a test can eradicate the benefit of perceptual disfluency Geller & Peterson (2021). In addition, requiring participants to make judgments of learning (a metamemory judgment on a scale of 0-100 indicating how likely it is they will recall the word on a later memory test) after each studied word also eradicates the disfluency effect (Besken & Mulligan, 2013; Rosner et al., 2015). Taken together, this highlights the critical role of both the kind of processing done on the to-be-remembered stimuli, and control processes in eliciting a disfluency effect. \nAll three of these theories propose potential loci for the perceptual disfluency effect. In the metacognitive account, the disfluency effect occurs at a later postlexical stage, after word recognition has taken place. The compensatory processing account (Mulligan, 1996) links the perceptual disfluency effect directly to the word recognition process. That is, disfluent words receive more top-down processing from lexical or semantic levels during encoding. Lastly, the stage-specific model proposed by Ptok et al. (2019) associates perceptual disfluency effects with a specific stage of processing, namely the semantic level, but it also considers general attentional and cognitive control processes that are not solely tied to the word recognition process itself."
  },
  {
    "objectID": "intro.html#moving-beyond-the-mean-modeling-rt-distributions",
    "href": "intro.html#moving-beyond-the-mean-modeling-rt-distributions",
    "title": "2  Perceptual Disfluency and Recognition Memory: A Response Time Distributional Analysis",
    "section": "2.3 Moving beyond the mean: modeling RT distributions",
    "text": "2.3 Moving beyond the mean: modeling RT distributions\n\n2.3.1 Ex-Gaussian distribution\nTo test the different stages or loci involved in the perceptual disfluency effect, it is necessary to use methods that allow for a more fine-grained analysis of processes during encoding. In the perceptual disfluency literature (and learning and memory more broadly), it is common to use measures such as mean accuracy and RTs to assess differences between a disfluent and fluent condition (Geller et al., 2018; Geller & Peterson, 2021; Rosner et al., 2015). While this approach is often deemed as acceptable practice, there has been a call to go beyond traditional RT methods when making inferences (Balota & Yap, 2011).\nThere are a several reasons for making the shift from traditional RT analyses to analyses that utilize the whole RT distribution. One reason is traditional approaches fail to capture the nuances inherent in RT distributions. Namely, RT distributions are unimodal and positively skewed. A standard analysis based on means can conceal effects that change only the shape of the tail of the distribution, only the location, or both the location and the shape of the distribution. A perfect example of this comes from the Stroop literature. The classic Stroop finding shows words presented in an incongruent color font (the word “red” printed in “blue” font) increases RTs compared to words in a baseline condition (e.g., XXXXX presented in a color font). The Stroop interference effect is something you can bet your house on. A more inconsistent finding is seeing facilitation (shortened RTs) when the word and color are congruent (i.e., “Green” presented in “Green”) compared to a baseline condition. Using ex-Gaussian analyses, Heathcote et al. (1991), provided an answer for this conundrum. When looking at ex-Gaussian parameters, there was both facilitation (from congruent trials) and interference (from incongruent trials) on mu. For \\(\\sigma\\) , the analysis showed interference, but no facilitation. For \\(\\tau\\), there was interference for both congruent and incongruent conditions. Comparing this to a mean RT analysis, they showed the standard interference Stroop effect, but no facilitation. Given that the algerbraic mean of the ex-Gaussian is \\(\\mu\\) + \\(\\tau\\) , the failure to observe a facilitation effect in the standard mean analysis likely arose from facilitation on \\(\\mu\\) and interference on \\(\\tau\\) canceling each other out. A finding such as this would be impossible looking solely at mean RTs.\nAnother reason to transition away from traditional analyses is that RTs provide only a coarse measure of processing during encoding. RTs capture the total sum of various task-related factors, ranging from non-decisional components like stimulus encoding and motor responses to decisional components.\nLastly, from a statistical standpoint, RTs present significant challenges. Specifically, they often violate two crucial assumptions: they are not normally distributed and their variance is frequently heterogeneous. Such violations can lead to biased results when making statistical inferences, as pointed out by (Wilcox, 1998).\nOne alternative to standard RT analyses is to examine RT distributions using mathematical models that capture the nuances of the RT distribution and consider various statistical properties, such as the shape ($\\mu$), spread ($\\sigma$), and skewness ($\\tau$) of the distribution. One popular choice is the ex-Gaussian distribution [Balota & Yap (2011); Ratcliff, 1979]. As the name suggests, the ex-Gaussian distribution decomposes RTs into three parameters: mu (μ) representing the mean of the Gaussian component, sigma (σ) representing the standard deviation of that Gaussian component, and tau (τ) representing the mean and standard deviation of an exponential component capturing the tail of the distribution. The alerbric mean of ex-Gaussian is a combination of \\(\\mu\\) + \\(\\tau\\) . Together the three parameters represent different aspects of the distribution’s location and shape.\nExploring effects from a distributional perspective has provided a richer understanding of how different experimental manipulations affect word recognition. Experimental manipulations can produce several distinct patterns. One pattern involves a shift of the entire RT distribution to the right, without increasing the tail or skew. A pattern such as this would suggest a general effect and would manifest as an effect on \\(\\mu\\), but not \\(\\tau\\) . As an example, semantic priming effects–where responses are faster to targets when preceded by a semantically related prime compared to an unrelated prime–can be nicely explained by a simple shift in the RT distribution (Balota et al., 2008). Alternatively, an experimental manipulation could produce a pattern where the RT distribution is skewed or stretched in the slower condition. This suggests that the manipulation only impacts a subset of trials, and is visible as an increase in \\(\\tau\\). An example of an effect that only impacts \\(\\tau\\) is the transposed letter effect in visual word recognition (Johnson et al., 2012). The TL effect involves misidentification of orthographically similar stimuli that with transposed internal like, like mistaking “JUGDE” for “JUDGE” (Perea & Lupker, 2003). Finally, you could observe a pattern wherein an experimental manipulation results in both changes in \\(\\mu\\) and \\(\\tau\\), which would shift and stretch the RT distribution. Recognizing low frequency words have been shown to not only shift the RT distribution, but also stretch the RT distributions(Andrews & Heathcote, 2001; Balota & Spieler, 1999; Staub, 2010).\nThe ex-Gaussian model while mostly descriptive in nature has been used as a theoretical tool to map model parameters onto cognitive processes—please note this is highly controversial (Heathcote et al., 1991; Matzke & Wagenmakers, 2009). For example, the \\(\\mu\\) and \\(\\sigma\\) parameters have been tied to early, non-analytic, processing. In the area of semantic priming, the selective effect on \\(\\mu\\) has been taken as evidence for an automatic spreading activation process (or head-start), according to which the activation of a node in a semantic network spreads automatically to interconnected nodes, preactivating a semantically related word (Balota et al., 2008; Wit & Kinoshita, 2015). The exponential component (\\(\\tau\\)) has been tied to later, more analytic, processing (Balota & Spieler, 1999) . Specifically, increases in \\(\\tau\\) have been attributed to working memory and attentional processes (Fitousi, 2020; Kane & Engle, 2003). For instance, Johnson et al. (2012) tied \\(\\tau\\) differences for the TL effect to a post-lexical checking mechanism that arose from a failure to identify the stimulus on a select number of trials rather than a broader, lexical, effect occurring on every trial. Taken together, each parameter of the ex-Gaussian distribution seem to reflect a litany of cognitive processes that can be lumped into early vs late stages of cognitive processing.\n\n\n2.3.2 Drift-diffusion model (DDM)\nContrary to the ex-Gaussian distribution discussed above, the DDM (see Ratcliff et al., 2016, for a comprehensive introduction) is a process-model and it’s parameters can be linked to latent cognitive constructs (Gomez et al., 2013). The DDM is a popular computational model commonly used in binary speeded decision tasks such as the LDT. The DDM model assumes a decision is a cumulative process that begins at stimulus onset and ends once a noisy accumulation of evidence has reached a decision threshold.The DDM has led to important insights into cognition in a wide range of choice tasks, including perceptual-, memory-, and value-based decisions (Myers et al., 2022).\nIn the DDM, RTs are decomposed into several parameters that represent distinct cognitive processes. The full DDM is composed of several parameters and varies depending on the flavor of DDM model one is using. The most relevant to our purposes here are the drift rate (\\(v\\)) and non-decisional time (ndt; \\(T_{er}\\)) parameters. Drift rate (\\(v\\)) represents the rate at which evidence is accumulated towards a decision boundary. In essence, it is a measure of how quickly information is processed to make a decision. A higher \\(v\\) means evidence is being accumulated more quickly, leading to faster decisions. The \\(v\\) is linked with the decision-process itself and is seen as an index of global processing demands placed on the cognitive system by task difficulty, memory load, or other concurrent processing demands (to the extent that concurrent processes compete for the same cognitive resources (Russell J. Boag et al., 2019). Drift rates have also been implicated as a locus of reactive inhibitory control (Braver, 2012), in which critical events (e.g., the need to update WM or task switch) trigger inhibition of prepotent response drift rates (Russell J. Boag et al., 2019; Russell James Boag et al., 2019).\nThe \\(T_{er}\\) parameter represents the time taken for processes other than the decision-making itself. This includes early sensory processing (like visual or auditory processing of the stimulus) and late motor processes (like executing the response).\nThe DDM has been shown to be a valuable tool for studying the effects of different experimental manipulations on cognitive processes in visual word recognition. For example, Gomez & Perea (2014) demonstrated certain manipulations can deferentially affect specific parameters of the model. For instance manipulating the orientation of words (rotating them by 0, 90, or 180 degrees) affected the \\(T_{er}\\) component, but not \\(v\\). In contrast, word frequency (high-frequency words vs. low-frequency words) primarily influenced both the drift rate and non-decision time. These findings highlight the sensitivity of the DDM in identifying and differentiating the impact of various stimulus manipulations on different cognitive processes involved in decision-making."
  },
  {
    "objectID": "intro.html#the-current-experiments",
    "href": "intro.html#the-current-experiments",
    "title": "2  Perceptual Disfluency and Recognition Memory: A Response Time Distributional Analysis",
    "section": "2.4 The Current Experiments",
    "text": "2.4 The Current Experiments\nIn the present experiments, we pursued two aims related to perceptual disfluency. The first aim was to replicate the perceptual disfluency effect. To optimize our chances for observing this effect, we utilized a disfluency manipulation known to enhance memory in the literature—blurring (Rosner et al., 2015). Additionally, we employed a surprise recognition test (Geller & Peterson, 2021). The second, more pivotal aim, was to enrich the toolkit of researchers exploring perceptual disfluency. Through the application of distribution techniques, like the ex-Gaussian analysis and DDM, our goal was showcase how these techniques can grant us a more profound insight into the influence of encoding difficulty on memory. Overall, these endeavors will help ascertain when and if perceptual disfluency proves beneficial for memory.\n\n\n\n\n\n\nAlter, A. L. (2013). The Benefits of Cognitive Disfluency. Current Directions in Psychological Science, 22(6), 437–442. https://doi.org/10.1177/0963721413498894\n\n\nAndrews, S., & Heathcote, A. (2001). Distinguishing common and task-specific processes in word identification: A matter of some moment? Journal of Experimental Psychology: Learning, Memory, and Cognition, 27(2), 514–544. https://doi.org/10.1037/0278-7393.27.2.514\n\n\nBalota, D. A., & Spieler, D. H. (1999). Word frequency, repetition, and lexicality effects in word recognition tasks: Beyond measures of central tendency. Journal of Experimental Psychology: General, 128(1), 32–55. https://doi.org/10.1037/0096-3445.128.1.32\n\n\nBalota, D. A., & Yap, M. J. (2011). Moving beyond the mean in studies of mental chronometry the power of response time distributional analyses. Current Directions in Psychological Science, 20(3), 160166. http://cdp.sagepub.com/content/20/3/160.short\n\n\nBalota, D. A., Yap, M. J., Cortese, M. J., & Watson, J. M. (2008). Beyond mean response latency: Response time distributional analyses of semantic priming. Journal of Memory and Language, 59(4), 495–523. https://doi.org/10.1016/j.jml.2007.10.004\n\n\nBesken, M., & Mulligan, N. W. (2013). Easily perceived, easily remembered? Perceptual interference produces a double dissociation between metamemory and memory performance. Memory & Cognition, 41(6), 897903. http://link.springer.com/article/10.3758/s13421-013-0307-8\n\n\nBoag, Russell James, Strickland, L., Heathcote, A., Neal, A., & Loft, S. (2019). Cognitive control and capacity for prospective memory in complex dynamic environments. Journal of Experimental Psychology: General, 148(12), 2181–2206. https://doi.org/10.1037/xge0000599\n\n\nBoag, Russell J., Strickland, L., Loft, S., & Heathcote, A. (2019). Strategic attention and decision control support prospective memory in a complex dual-task environment. Cognition, 191, 103974. https://doi.org/10.1016/j.cognition.2019.05.011\n\n\nBraver, T. S. (2012). The variable nature of cognitive control: a dual mechanisms framework. Trends in Cognitive Sciences, 16(2), 106–113. https://doi.org/10.1016/j.tics.2011.12.010\n\n\nCarpenter, S. K., Pan, S. C., & Butler, A. C. (2022). The science of effective learning with spacing and retrieval practice. Nature Reviews Psychology, 1(9), 496–511. https://doi.org/10.1038/s44159-022-00089-1\n\n\nCraik, F. I. M., & Lockhart, R. S. (1972). Levels of processing: A framework for memory research. Journal of Verbal Learning and Verbal Behavior, 11(6), 671–684. https://doi.org/10.1016/S0022-5371(72)80001-X\n\n\nCushing, C., & Bodner, G. E. (2022). Reading aloud improves proofreading (but using sans forgetica font does not). Journal of Applied Research in Memory and Cognition, 11, 427–436. https://doi.org/10.1037/mac0000011\n\n\nDiemand-Yauman, C., Oppenheimer, D. M., & Vaughan, E. B. (2011). Fortune favors the: Effects of disfluency on educational outcomes. Cognition, 118(1), 111115. https://doi.org/10.1016/j.cognition.2010.09.012\n\n\nEskenazi, M. A., & Nix, B. (2021). Individual differences in the desirable difficulty effect during lexical acquisition. Journal of Experimental Psychology: Learning, Memory, and Cognition, 47(1), 45–52. https://doi.org/10.1037/xlm0000809\n\n\nFitousi, D. (2020). Decomposing the composite face effect: Evidence for non-holistic processing based on the ex-Gaussian distribution. Quarterly Journal of Experimental Psychology, 73(6), 819–840. https://doi.org/10.1177/1747021820904222\n\n\nGeller, J., Davis, S. D., & Peterson, D. J. (2020). Sans forgetica is not desirable for learning. Memory, 28(8), 957–967. https://doi.org/10.1080/09658211.2020.1797096\n\n\nGeller, J., & Peterson, D. (2021). Is this going to be on the test? Test expectancy moderates the disfluency effect with sans forgetica. Journal of Experimental Psychology: Learning, Memory, and Cognition, 47(12), 1924–1938. https://doi.org/10.1037/xlm0001042\n\n\nGeller, J., Still, M. L., Dark, V. J., & Carpenter, S. K. (2018). Would disfluency by any other name still be disfluent? Examining the disfluency effect with cursive handwriting. Memory and Cognition, 46(7), 11091126. https://doi.org/10.3758/s13421-018-0824-6\n\n\nGomez, P., & Perea, M. (2014). Decomposing encoding and decisional components in visual-word recognition: A diffusion model analysis. Quarterly Journal of Experimental Psychology, 67(12), 2455–2466. https://doi.org/10.1080/17470218.2014.937447\n\n\nGomez, P., Perea, M., & Ratcliff, R. (2013). A diffusion model account of masked versus unmasked priming: Are they qualitatively different? Journal of Experimental Psychology: Human Perception and Performance, 39(6), 1731–1740. https://doi.org/10.1037/a0032333\n\n\nHalamish, V. (2018). Can very small font size enhance memory? Memory & Cognition, 46(6), 979–993. https://doi.org/10.3758/s13421-018-0816-6\n\n\nHeathcote, A., Popiel, S. J., & Mewhort, D. J. (1991). Analysis of response time distributions: An example using the Stroop task. Psychological Bulletin, 109(2), 340–347. https://doi.org/10.1037/0033-2909.109.2.340\n\n\nHuff, M. J., Maxwell, N. P., & Mitchell, A. (2022). Distinctive Sans Forgetica font does not benefit memory accuracy in the DRM paradigm. Cognitive Research: Principles and Implications, 7(1). https://doi.org/10.1186/s41235-022-00448-9\n\n\nJames, W. (1890). The principles of psychology, vol i. Henry Holt; Co. https://doi.org/10.1037/10538-000\n\n\nJohnson, R. L., Staub, A., & Fleri, A. M. (2012). Distributional analysis of the transposed-letter neighborhood effect on naming latency. Journal of Experimental Psychology: Learning, Memory, and Cognition, 38(6), 1773–1779. https://doi.org/10.1037/a0028222\n\n\nKahneman, D. (2011). Thinking, fast and slow (2011. Farrar, Straus and. http://scholar.google.com/scholar?cluster=12402339161604655296&hl=en&oi=scholarr\n\n\nKane, M. J., & Engle, R. W. (2003). Working-memory capacity and the control of attention: The contributions of goal neglect, response competition, and task set to Stroop interference. Journal of Experimental Psychology: General, 132(1), 47–70. https://doi.org/10.1037/0096-3445.132.1.47\n\n\nMatzke, D., & Wagenmakers, E.-J. (2009). Psychological interpretation of the ex-gaussian and shifted wald parameters: A diffusion model analysis. Psychonomic Bulletin & Review, 16(5), 798817. http://link.springer.com/article/10.3758/PBR.16.5.798\n\n\nMcClelland, J. L., & Rumelhart, D. E. (1981). An interactive activation model of context effects in letter perception: I. An account of basic findings. Psychological Review, 88(5), 375–407. https://doi.org/10.1037/0033-295X.88.5.375\n\n\nMulligan, N. W. (1996). The effects of perceptual interference at encoding on implicit memory, explicit memory, and memory for source. Journal of Experimental Psychology: Learning, Memory, and Cognition, 22(5), 1067–1087. https://doi.org/10.1037/0278-7393.22.5.1067\n\n\nMyers, C. E., Interian, A., & Moustafa, A. A. (2022). A practical introduction to using the drift diffusion model of decision-making in cognitive psychology, neuroscience, and health sciences. Frontiers in Psychology, 13. https://www.frontiersin.org/articles/10.3389/fpsyg.2022.1039172\n\n\nNairne, J. S. (1988). A framework for interpreting recency effects in immediate serial recall. Memory & Cognition, 16(4), 343–352. https://doi.org/10.3758/BF03197045\n\n\nOppenheimer, D. M., & Alter, A. L. (2014). The search for moderators in disfluency research. Applied Cognitive Psychology, 28(4), 502504. https://doi.org/10.1002/acp.3023\n\n\nPerea, M., & Lupker, S. J. (2003). Transposed-letter confusability effects in masked form priming. Masked Priming: State of the Art, 97120. http://defiant.ssc.uwo.ca/faculty/lupkerpdfs/Perea%20&%20Lupker,%202003,%20chapter.pdf\n\n\nPtok, M. J., Hannah, K. E., & Watter, S. (2020). Memory effects of conflict and cognitive control are processing stage-specific: evidence from pupillometry. Psychological Research, 85(3), 1029–1046. https://doi.org/10.1007/s00426-020-01295-3\n\n\nPtok, M. J., Thomson, S. J., Humphreys, K. R., & Watter, S. (2019). Congruency encoding effects on recognition memory: A stage-specific account of desirable difficulty. Frontiers in Psychology, 10. https://doi.org/10.3389/fpsyg.2019.00858\n\n\nRatcliff, R., Smith, P. L., Brown, S. D., & McKoon, G. (2016). Diffusion Decision Model: Current Issues and History. Trends in Cognitive Sciences, 20(4), 260–281. https://doi.org/10.1016/j.tics.2016.01.007\n\n\nRhodes, M. G., & Castel, A. D. (2008). Memory predictions are influenced by perceptual information: Evidence for metacognitive illusions. Journal of Experimental Psychology: General, 137(4), 615625. https://doi.org/10.1037/a0013684\n\n\nRhodes, M. G., & Castel, A. D. (2009). Metacognitive illusions for auditory information: Effects on monitoring and control. Psychonomic Bulletin and Review, 16(3), 550554. https://doi.org/10.3758/PBR.16.3.550\n\n\nRoberts, B. R. T., Hu, Z. S., Curtis, E., Bodner, G. E., McLean, D., & MacLeod, C. M. (2023). Reading text aloud benefits memory but not comprehension. Memory & Cognition. https://doi.org/10.3758/s13421-023-01442-2\n\n\nRoediger, H. L., & Karpicke, J. D. (2006). Test-Enhanced Learning. Psychological Science, 17(3), 249–255. https://doi.org/10.1111/j.1467-9280.2006.01693.x\n\n\nRohrer, D., & Taylor, K. (2007). The shuffling of mathematics problems improves learning. Instructional Science, 35(6), 481–498. https://doi.org/10.1007/s11251-007-9015-8\n\n\nRosner, T. M., Davis, H., & Milliken, B. (2015). Perceptual blurring and recognition memory: A desirable difficulty effect revealed. Acta Psychologica, 160, 11–22. https://doi.org/10.1016/j.actpsy.2015.06.006\n\n\nRummer, R., Schweppe, J., & Schwede, A. (2015). Fortune is fickle: Null-effects of disfluency on learning outcomes. Metacognition and Learning, 114. http://link.springer.com/article/10.1007/s11409-015-9151-5\n\n\nSlamecka, N. J., & Graf, P. (1978). The generation effect: Delineation of a phenomenon. Journal of Experimental Psychology: Human Learning and Memory, 4(6), 592–604. https://doi.org/10.1037/0278-7393.4.6.592\n\n\nStaub, A. (2010). The effect of lexical predictability on distributions of eye fixation durations. Psychonomic Bulletin & Review, 18(2), 371–376. https://doi.org/10.3758/s13423-010-0046-9\n\n\nSungkhasettee, V. W., Friedman, M. C., & Castel, A. D. (2011). Memory and metamemory for inverted words: Illusions of competency and desirable difficulties. Psychonomic Bulletin and Review, 18(5), 973978. https://doi.org/10.3758/s13423-011-0114-9\n\n\nTaylor, A., Sanson, M., Burnell, R., Wade, K. A., & Garry, M. (2020). Disfluent difficulties are not desirable difficulties: The (lack of) effect of sans forgetica on memory. Memory, 28(7), 850–857. https://doi.org/10.1080/09658211.2020.1758726\n\n\nWeissgerber, S. C., & Reinhard, M. A. (2017). Is disfluency desirable for learning? Learning and Instruction, 49, 199–217. https://doi.org/10.1016/j.learninstruc.2017.02.004\n\n\nWel, P. van der, & Steenbergen, H. van. (2018). Pupil dilation as an index of effort in cognitive control tasks: A review. Psychonomic Bulletin & Review, 25(6), 2005–2015. https://doi.org/10.3758/s13423-018-1432-y\n\n\nWeltman, D., & Eakin, M. (2014). Incorporating unusual fonts and planned mistakes in study materials to increase business student focus and retention. INFORMS Transactions on Education, 15(1), 156165. https://doi.org/10.1287/ited.2014.0130\n\n\nWetzler, E. L., Pyke, A. A., & Werner, A. (2021). Sans Forgetica is Not the “Font” of Knowledge: Disfluent Fonts are Not Always Desirable Difficulties. SAGE Open, 11(4), 21582440211056624. https://doi.org/10.1177/21582440211056624\n\n\nWilcox, R. R. (1998). How many discoveries have been lost by ignoring modern statistical methods? American Psychologist, 53(3), 300–314. https://doi.org/10.1037/0003-066x.53.3.300\n\n\nWit, B. de, & Kinoshita, S. (2015). The masked semantic priming effect is task dependent: Reconsidering the automatic spreading activation process. Journal of Experimental Psychology: Learning, Memory, and Cognition, 41(4), 1062–1075. https://doi.org/10.1037/xlm0000074\n\n\nYue, C. L., Castel, A. D., & Bjork, R. A. (2013). When disfluency isand is nota desirable difficulty: The influence of typeface clarity on metacognitive judgments and memory. Memory & Cognition, 41(2), 229241. http://link.springer.com/article/10.3758/s13421-012-0255-8"
  },
  {
    "objectID": "expt1a.html",
    "href": "expt1a.html",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "",
    "text": "4 Method: Experiment 1\nThe preregistered analysis plan can be found here: https://osf.io/q3fjn. All raw and summary data, materials, and R scripts for pre-processing, analysis, and plotting for Experiments 1 can be found at https://osf.io/6sy7k/\nAll models presented no divergences, and all chains mixed well and produced comparable estimates (Rhat &lt; 1.01)."
  },
  {
    "objectID": "expt1a.html#set-up",
    "href": "expt1a.html#set-up",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "3.1 Set-up",
    "text": "3.1 Set-up\nBelow are the packages you should install to ensure this document runs properly.\n\n\nCode\n#load packages\nlibrary(plyr)\nlibrary(easystats)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(cowplot)\nlibrary(ggeffects)\nlibrary(here)\nlibrary(data.table)\nlibrary(ggrepel)\nlibrary(brms)\nlibrary(ggdist)\nlibrary(emmeans)\nlibrary(tidylog)\nlibrary(tidybayes)\nlibrary(hypr)\nlibrary(cowplot)\nlibrary(tidyverse)\nlibrary(colorspace)\nlibrary(ragg)\nlibrary(cowplot)\nlibrary(ggtext)\nlibrary(MetBrewer)\nlibrary(ggdist)\nlibrary(modelbased)\nlibrary(flextable)\nlibrary(cmdstanr)\nlibrary(brms)\nlibrary(easystats)\nlibrary(gt)\n\nprocess_ER_column &lt;- function(x) {\n  # Check if ER column value is numeric, if not, replace with Inf\n  x &lt;- ifelse(is.numeric(x), x, Inf)\n  \n  return(x)\n}"
  },
  {
    "objectID": "expt1a.html#figure-theme",
    "href": "expt1a.html#figure-theme",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "3.2 Figure Theme",
    "text": "3.2 Figure Theme\n\n\nCode\nbold &lt;- element_text(face = \"bold\", color = \"black\", size = 16) #axis bold\ntheme_set(theme_minimal(base_size = 15, base_family = \"Arial\"))\ntheme_update(\n  panel.grid.major = element_line(color = \"grey92\", size = .4),\n  panel.grid.minor = element_blank(),\n  axis.title.x = element_text(color = \"grey30\", margin = margin(t = 7)),\n  axis.title.y = element_text(color = \"grey30\", margin = margin(r = 7)),\n  axis.text = element_text(color = \"grey50\"),\n  axis.ticks =  element_line(color = \"grey92\", size = .4),\n  axis.ticks.length = unit(.6, \"lines\"),\n  legend.position = \"top\",\n  plot.title = element_text(hjust = 0, color = \"black\", \n                            family = \"Arial\",\n                            size = 21, margin = margin(t = 10, b = 35)),\n  plot.subtitle = element_text(hjust = 0, face = \"bold\", color = \"grey30\",\n                               family = \"Arial\", \n                               size = 14, margin = margin(0, 0, 25, 0)),\n  plot.title.position = \"plot\",\n  plot.caption = element_text(color = \"grey50\", size = 10, hjust = 1,\n                              family = \"Arial\", \n                              lineheight = 1.05, margin = margin(30, 0, 0, 0)),\n  plot.caption.position = \"plot\", \n  plot.margin = margin(rep(20, 4))\n)\npal &lt;- c(met.brewer(\"Veronese\", 3))\n\n\n\n\nCode\n## flat violinplots\n### It relies largely on code previously written by David Robinson \n### (https://gist.github.com/dgrtwo/eb7750e74997891d7c20) and ggplot2 by H Wickham\n#check if required packages are installed\n#Load packages\n# Defining the geom_flat_violin function. Note: the below code modifies the \n# existing github page by removing a parenthesis in line 50\n\ngeom_flat_violin &lt;- function(mapping = NULL, data = NULL, stat = \"ydensity\",\n                             position = \"dodge\", trim = TRUE, scale = \"area\",\n                             show.legend = NA, inherit.aes = TRUE, ...) {\n  layer(\n    data = data,\n    mapping = mapping,\n    stat = stat,\n    geom = GeomFlatViolin,\n    position = position,\n    show.legend = show.legend,\n    inherit.aes = inherit.aes,\n    params = list(\n      trim = trim,\n      scale = scale,\n      ...\n    )\n  )\n}\n# horizontal nudge position adjustment\n# copied from https://github.com/tidyverse/ggplot2/issues/2733\nposition_hnudge &lt;- function(x = 0) {\n  ggproto(NULL, PositionHNudge, x = x)\n}\nPositionHNudge &lt;- ggproto(\"PositionHNudge\", Position,\n                          x = 0,\n                          required_aes = \"x\",\n                          setup_params = function(self, data) {\n                            list(x = self$x)\n                          },\n                          compute_layer = function(data, params, panel) {\n                            transform_position(data, function(x) x + params$x)\n                          }\n)\n\n\n\n\nCode\n#| message: false\n#| cache: false\ntheme_set(\n  theme_linedraw() +\n    theme(panel.grid = element_blank())\n)\nbayesplot::color_scheme_set(scheme = \"brewer-Spectral\")\noptions(digits = 3)"
  },
  {
    "objectID": "expt1a.html#participants",
    "href": "expt1a.html#participants",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "4.1 Participants",
    "text": "4.1 Participants\nWe preregistered a sample size of 216. All participants were recruited through the university subject pool(SONA). A design with a sample size of 216 can detect effect sizes of δ≥ 0.2 with a probability of at least 0.90, assuming a one-sided criterion for detection that allows for a maximum Type I error rate of α=0.05. Per our exclusion criteria, we retained participants that were native English speakers, were over the age of 17, had overall accuracy on the LDT greater than 80%, and did not complete the experiment more than once. Due to our exclusion criteria, we oversampled participants. Because of this we randomly chose 36 participants from each list to reach our target sample size."
  },
  {
    "objectID": "expt1a.html#apparatus-and-stimuli",
    "href": "expt1a.html#apparatus-and-stimuli",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "4.2 Apparatus and stimuli",
    "text": "4.2 Apparatus and stimuli\nThe experiment was run using PsychoPy software and hosted on Pavlovia (www.pavlovia.org). You can see an example of the experiment by navigating to this website: https://run.pavlovia.org/Jgeller112/ldt_dd_l1_jol_context.\nWe used 84 words and 84 nonwords for the LDT. Words were obtained from the the LexOPS package (LexOPS?) in R. All of our words were matched on a number of different lexical dimensions. All words were nouns, 4-6 letters in length, had a known proportion of between 90%-100%, had a low neighborhood density (OLD20 score between 1-2), high concreteness, imageability, and word frequency. Our nonwords were were created using the English Lexicon Project. Stimuli can be found at our OSF project page.\n\n4.2.1 Blurring\nBlurred stimuli were processed through the imager package in R (Barthelme, 2023) and a personal script (https://osf.io/gr5qv). Each image was processed through a high blur filter (Gaussian blur of 15) and low blur filter (Gaussian blur of 10). The code to generate these images is collected here: These pictures were then imported into PsychoPy as picture files. See Figure 1 for an examples of a clear, low blurred, and high blurred word."
  },
  {
    "objectID": "expt1a.html#design",
    "href": "expt1a.html#design",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "4.3 Design",
    "text": "4.3 Design\nWe created two lists: One list (84 words; 28 clear, 28 low blur, 28 high blur) served as a study (old) list for the LDT task while the other list served as a test (new) list (84 words; 28 clear, 28, LB, 28, HB) for our recognition memory test that occurred after the LDT. We counterbalanced each list so each word served as an old word and a new world and were presented in clear, low blur, and high blur across participants. This resulted in six counterbalanced lists. Lists were assigned to participants so that across participants each word occurs equally often in the six possible conditions: clear old, LB old, HB old, clear new, LB new, and HB new. For the LDT task, we generated a set of 84 legal nonwords that we obtained from the English Lexicon Project. These 84 nonwords were used across all 6 lists."
  },
  {
    "objectID": "expt1a.html#procedure",
    "href": "expt1a.html#procedure",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "4.4 Procedure",
    "text": "4.4 Procedure\nThe experiment consisted of two phases: an encoding phase (LDT) and a test phase. During the encoding phase, a fixation cross appeared at the center of the screen for 500 ms. The fixation cross was immediately replaced by a letter string in the same location. To continue to the next trial, participants had to decide if the letter string presented on screen was a word or not by either pressing designated keys on the keyboard (“m” or “z”) or by tapping on designated areas on the screen (word vs. nonword). After the encoding phase, participants were given a surprise old/new recognition memory test. During the test phase, a word appeared in the center of the screen that either had been presented during study (“old”) or had not been presented during study (“new”). Old words occurred in their original typeface, and following the counterbalancing procedure, each of the new words was presented as clear, low blurred, or high blurred. All words were individually randomized for each participant during both the study and test phases and progress was self-paced. After the experiment, participants were debriefed. The entire experiment lasted approximately 15 minutes."
  },
  {
    "objectID": "expt1a.html#analysis-plan",
    "href": "expt1a.html#analysis-plan",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "4.5 Analysis Plan",
    "text": "4.5 Analysis Plan\nAll models were fit using The Stan modeling language (Grant et al., 2017) with the brms (Bürkner, 2017a) package in in R . We only analyzed trials where the target stimulus was a word (Experiment 1a and 1b) or non-animal (Experiment 2). For the DDM, we excluded trials with RTs lower than 200 ms or higher than 2500 ms. For the accuracy analysis, we only excluded trials with RTs lower than 200 ms or higher than 2500 ms (% of trials). For Experiments 1 a and 1b, we fit a model with blurring (clear vs. high blur vs. low blur). For Experiment 2, we fit a model with blurring and word frequency. The models included maximal random-effect structures justified by the design (Barr et al., 2013). This included random intercepts for participants and items, and a random slope by blurring level for each varying random intercept. Contrast codes for each variable were created using the hypr package in R ((Schad et al., 2019)). We fit the models twice: Once with contrast codes for high blur vs. clear and for high blur vs. Low blur and once with the low blur vs. clear contrast.\nIn all experiments reported here, the statistical model was run with four chains of 5,000 Markov chain Monte Carlo iterations, with 1,000-iteration warmups for 4 chains (16,000 samples in total). Convergence and stability of the Bayesian sampling is quantified by the Rˆ(R hat) diagnostics below 1.01 and Effective Sample SIze (ESS) greater than 1000 ((Bürkner, 2017a)). For both the RT data and accuracy data, we report our models with with weakly-informative priors for the population-level parameters. Using a weakly-informative prior as opposed to a default (which is an uniform prior where all effects are equally likely) allows for the calculation of evidence ratio (Bayes Factor) for one-sides tests. For the ex-Gaussian analysis we used a weak prior (i.e., N ~ (0, 100)). For the population-level effects in the accuracy and signal detection analyses, we used a Cauchy distribution with the mean of 0 and scale of 0.35 (cauchy ~ 0, 0.35)) recommended by (Kinoshita et al., 2023) for logistic regression.\nFor the marginal means and differences , we report the expected values under the posterior distribution and their 90% credible intervals (Cr. I.). For marginal mean differences, we also report the posterior probability that a difference δ is not zero. If a hypothesis states that δ \\&gt; 0, then it would be considered strong evidence for this hypothesis would be if zero is not included in the 90% Cr. I. of δ and the posterior P(δ \\&gt; 0) is close to one (by a reasonably clear margin). To extract the estimated marginal means from the posterior distribution of the fitted models we used a combination of emmeans R package (Lenth, 2023) bayestestr(Makowski et al., 2019), and brms (Bürkner, 2017b).\nModel quality was thoroughly assessed via predictive prior and posterior checks, Rhat and divergence diagnostics. In order to assess the evidence in favor or against our hypotheses, we used Evidence Ratio (ER, a generalization of Bayes factors allowing for directional hypotheses). An ER above 3 indicates moderate to substantial evidence for our hypothesis, below 0.3 indicates moderate to substantial evidence for the null hypothesis, and anything in between is inconclusive evidence (Morey & Rouder, 2022).\n\n4.5.1 Ex-Gaussian\nFor the ex-Gaussian analyses we excluded trials with RTs lower than 200 ms or higher than 2500 ms as well as incorrect responses (% of trials).\nWe used the ex-Gaussian distribution to model response times, with both the mean of the Gaussian component 𝜇 and the scale parameter (\\(sigma\\)) of the exponential component 𝛽 (equaling the inverse of the rate parameter 𝜆) being allowed to vary between conditions. In addition, to better visualize the distributional features of the latency data, we computed the delta plots for all variables.\nLastly, to model response accuracy, we used the Bernoulli distribution with a logit link. We only excluded trials with RTs lower than 200 ms or higher than 2500 ms (% of trials).\n\n\n4.5.2 Diffusion Model\nWe used a hierarchical-Bayesian variant of the Wiener diffusion model ((Vandekerckhove et al., 2011)) with accuracy coding. This model accounts for the entire data (i.e., RT distributions of correct and error trials) with three latent parameters: (a) the drift rate, a measure of the efficiency of information processing in the decision process, (b) the boundary separation, a measure of response caution that controls the speed-accuracy trade-off (this was fixed to .5), and (c) the non-decision time on-coding parametrization – to each dataset.\nFor the DDM, we excluded trials with RTs lower than 200 ms or higher than 2500 ms.\n\n\n4.5.3 Recognition memory\nFor our recognition memory data, we fit a Bayesian generalzied linear multilevel model with a Bernoulli distribution with a probit link. In its simplest from, SDT models are regressions with a probit link. To estimate the SDT parameter of interest (d’), we fit a Bayesian hierarchical generalized linear model with a binomial distribution probit link function to participant responses (their actual response (old vs. new)) as a function of actual item status (old vs. new) and blurring level. Traditional SDT analyses have proven to be an informative and efficient approach to analyzing binary accuracy data. However, considering the deficiency in precision and power in traditional analyses compared to mixed effects analyses (cite), it is worth considering a Bayesian generalized linear mixed effect approach to SDT (see (Zloteanu & Vuorre, 2023) for a nice tutorial on Bayesian SDT models)."
  },
  {
    "objectID": "expt1a.html#section",
    "href": "expt1a.html#section",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "4.6 ",
    "text": "4.6"
  },
  {
    "objectID": "expt1a.html#accuracy",
    "href": "expt1a.html#accuracy",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.1 Accuracy",
    "text": "5.1 Accuracy\nThe data file is cleaned (participants &gt;=.8, no duplicate participants, no participants &lt; 17. )\n\n\nCode\n# get data from osf\nblur_acc &lt;- read_csv(\"https://osf.io/xv5bd/download\") %&gt;%\n    dplyr::filter(lex==\"m\")\n\n\nblur_acc_new&lt;- blur_acc %&gt;%\n  dplyr::filter(rt &gt;= .2 & rt &lt;= 2.5)\n\nhead(blur_acc)\n\n\n# A tibble: 6 × 14\n   ...1 participant age   date  string study blur     rt  corr lex   list  bad_1\n  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1     1      198679 18    2022… BLOKE  old   C     2.68      1 m     L4    keep \n2     3      198679 18    2022… FERRY  old   C     0.796     1 m     L4    keep \n3     5      198679 18    2022… BUNNY  old   HB    0.899     1 m     L4    keep \n4     6      198679 18    2022… HAMMER old   C     0.932     1 m     L4    keep \n5     9      198679 18    2022… CLOTH  old   LB    1.02      1 m     L4    keep \n6    10      198679 18    2022… GLOVE  old   HB    2.39      1 m     L4    keep \n# ℹ 2 more variables: bad_2 &lt;chr&gt;, bad_3 &lt;chr&gt;\n\n\nCode\n# get trials and how many elminated \ndim(blur_acc)\n\n\n[1] 18144    14\n\n\nCode\ndim(blur_acc_new)\n\n\n[1] 17873    14\n\n\nThe analysis of accuracy is is based on 18144 data points. After removing fast and slow RTs we were left with 17873 data point points (0.015 %)"
  },
  {
    "objectID": "expt1a.html#contrast-code",
    "href": "expt1a.html#contrast-code",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.2 Contrast Code",
    "text": "5.2 Contrast Code\n\n\nCode\n## Contrasts\n#hypothesis\nblurC &lt;-hypr(HB~C, HB~LB, levels=c(\"C\", \"HB\", \"LB\"))\nblurC\n\n\nhypr object containing 2 null hypotheses:\nH0.1: 0 = HB - C\nH0.2: 0 = HB - LB\n\nCall:\nhypr(~HB - C, ~HB - LB, levels = c(\"C\", \"HB\", \"LB\"))\n\nHypothesis matrix (transposed):\n   [,1] [,2]\nC  -1    0  \nHB  1    1  \nLB  0   -1  \n\nContrast matrix:\n   [,1] [,2]\nC  -2/3  1/3\nHB  1/3  1/3\nLB  1/3 -2/3\n\n\nCode\n#set contrasts in df \nblur_acc_new$blur &lt;- as.factor(blur_acc_new$blur)\n\ncontrasts(blur_acc_new$blur) &lt;-contr.hypothesis(blurC)"
  },
  {
    "objectID": "expt1a.html#brms-accuracy-model",
    "href": "expt1a.html#brms-accuracy-model",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.3 BRMs: Accuracy Model",
    "text": "5.3 BRMs: Accuracy Model\n\n\nCode\n#weak prior\nprior_exp1 &lt;- c(set_prior(\"cauchy(0,.35)\", class = \"b\"))\n\n#fit model\nfit_acc_weak &lt;- brm(corr ~ blur + (1+blur|participant) + (1+blur|string), data=blur_acc_new, \nwarmup = 1000,\n                    iter = 5000,\n                    chains = 4, \n                    init=0, \n                    family = bernoulli(),\n     cores = 4,\nprior = prior_exp1, \ncontrol = list(adapt_delta = 0.9), \nbackend=\"cmdstanr\", \nsave_pars = save_pars(all=T),\nsample_prior = T, \nthreads = threading(4), \nfile=\"fit_acc_context\")"
  },
  {
    "objectID": "expt1a.html#contrast-code-1",
    "href": "expt1a.html#contrast-code-1",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.4 Contrast Code",
    "text": "5.4 Contrast Code\n\n\nCode\n## Contrasts\n#hypothesis\nblurC &lt;-hypr(LB~C, levels=c(\"C\", \"HB\", \"LB\"))\nblurC\n\n\nhypr object containing one (1) null hypothesis:\nH0.1: 0 = LB - C\n\nCall:\nhypr(~LB - C, levels = c(\"C\", \"HB\", \"LB\"))\n\nHypothesis matrix (transposed):\n   [,1]\nC  -1  \nHB  0  \nLB  1  \n\nContrast matrix:\n   [,1]\nC  -1/2\nHB    0\nLB  1/2\n\n\nCode\n#set contrasts in df \nblur_acc_new$blur &lt;- as.factor(blur_acc_new$blur)\n\ncontrasts(blur_acc_new$blur) &lt;-contr.hypothesis(blurC)"
  },
  {
    "objectID": "expt1a.html#brms-accuracy-model-1",
    "href": "expt1a.html#brms-accuracy-model-1",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.5 BRMs: Accuracy Model",
    "text": "5.5 BRMs: Accuracy Model\n\n\nCode\n#weak prior\nprior_exp1 &lt;- c(set_prior(\"cauchy(0,.35)\", class = \"b\"))\n\n#fit model\nfit_acc_weak_lb &lt;- brm(corr ~ blur + (1+blur|participant) + (1+blur|string), data=blur_acc_new, \nwarmup = 1000,\n                    iter = 5000,\n                    chains = 4, \n                    init=0, \n                    family = bernoulli(),\n     cores = 4,\nprior = prior_exp1, \ncontrol = list(adapt_delta = 0.9), \nbackend=\"cmdstanr\", \nsave_pars = save_pars(all=T),\nsample_prior = T, \nthreads = threading(4), \nfile=\"fit_acc_context_lbc\")\n\n\n\n\nCode\n# get file from osf\nacc_c&lt;-read_rds(\"https://osf.io/xwdzn/download\")\n\n# get lowblur-c comparison\nacc_lb &lt;- read_rds(\"https://osf.io/wt3ry/download\")"
  },
  {
    "objectID": "expt1a.html#model-summary",
    "href": "expt1a.html#model-summary",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.6 Model Summary",
    "text": "5.6 Model Summary\nUsing `brms::hypothesis to extract support for each comparison of interest.\n\n\nCode\nacc_means &lt;- emmeans(acc_c, specs=\"blur\", type=\"response\") %&gt;%\n  as.data.frame()\n\n\n\n\nCode\na &lt;- hypothesis(acc_c, \"blur1 &lt;  0\")\nb &lt;- hypothesis(acc_c, \"blur2 &lt;  0\")\n\nc&lt;- hypothesis(acc_lb, \"blur1 =  0\")\n\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis) %&gt;%\n    mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\n\ntab %&gt;% \n  mutate(Hypothesis = c(\"High Blur - Clear &lt; 0\", \"High Blur - Low Blur &lt; 0\", \"Low Blur - Clear =  0 \")) %&gt;% \n  gt(caption=md(\"Table: Accuracy Directional Hypotheses Experiment 1\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Accuracy Directional Hypotheses Experiment 1\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n    \n  \n  \n    High Blur - Clear &lt; 0\n-0.997\n0.182\n-1.304\n-0.701\nInf\n1.000\n    High Blur - Low Blur &lt; 0\n-1.036\n0.193\n-1.358\n-0.724\nInf\n1.000\n    Low Blur - Clear =  0 \n0.017\n0.103\n-0.186\n0.220\n1.23\n0.551\n  \n  \n  \n\n\n\n\n\n5.6.1 Accuracy Summary\nClear words were better identified (\\(M\\) = .985) compared to high blur words (\\(M\\) = .962), b = -0.997, 90% Cr.I[-1.304, -0.701], ER = . Low blurred words were better identified (\\(M\\) = .986) than high blurred words, \\(b\\) = -1.036, 90% Cr.I[-1.358, -0.724], ER = . However, the evidence was weak for there being no significant difference in the identification accuracy between clear and low blurred words, \\(b\\) = 0.017, 90% Cr.I[-0.186, 0.22], ER = 1.226.\n\n\n5.6.2 Figures\n\n\nCode\n#| fig-cap: \"Experiment 1: Accuracy\"\n\n\ntop_mean &lt;-blur_acc %&gt;%  #get means for each blur cond for plot\n  dplyr::filter(lex==\"m\")%&gt;%\n  group_by(blur)%&gt;%\n   dplyr::summarise(mean1=mean(corr)) %&gt;%\n  dplyr::ungroup()\n\n\np_mean &lt;-blur_acc %&gt;%  #get means participant x cond for  plottin\n  dplyr::filter(lex==\"m\")%&gt;%\n    dplyr::group_by(participant, blur)%&gt;%\n     dplyr::summarise(mean1=mean(corr))\n\n\np3 &lt;- ggplot(p_mean, aes(x = blur , y = mean1, fill = blur)) +\n    coord_cartesian(ylim = c(.5,1)) + \n  \n  ggdist::stat_halfeye(\n    aes(\n      y = mean1,\n      color = blur,\n      fill = after_scale(lighten(color, .5))\n    ),\n    shape = 18,\n    point_size = 3,\n    interval_size = 1.8,\n    adjust = .5,\n    .width = c(0, 1)\n  ) +\n    geom_point(aes(x = blur, y = mean1, colour = blur),position = position_jitter(width = .05), size = 1, shape = 20)+\n    geom_boxplot(aes(x = blur, y = mean1, fill = blur),outlier.shape = NA, alpha = .5, width = .1, colour = \"black\")+\n  labs(subtitle = \"Word Accuracy: Context Reinstatement\")+\n     scale_color_manual(values=met.brewer(\"Cassatt2\", 3))+\n scale_fill_manual(values=met.brewer(\"Cassatt2\", 3))+\n    stat_summary(fun=mean, geom=\"point\", colour=\"darkred\", size=3)+\n    labs(y = \"Accuracy\", x = \"Blur\") +\n    geom_label_repel(data=top_mean, aes(y=mean1, label=round(mean1, 2)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5) + \n    theme(axis.text=bold) + theme(legend.position = \"none\")\n  #  ggsave('place.png', width = 8, height = 6)\np3"
  },
  {
    "objectID": "expt1a.html#rts",
    "href": "expt1a.html#rts",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.7 RTs",
    "text": "5.7 RTs"
  },
  {
    "objectID": "expt1a.html#brms-rts",
    "href": "expt1a.html#brms-rts",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.8 BRMs: RTs",
    "text": "5.8 BRMs: RTs\n\n\nCode\n#load data from osf\nrts &lt;- read_csv(\"https://osf.io/xv5bd/download\")\n\n\n\n\nCode\nblur_rt&lt;- rts %&gt;%\n  group_by(participant) %&gt;%\n   dplyr::filter(corr==1, lex==\"m\")#only include nonwords\n\nblur_rt_new &lt;- blur_rt %&gt;% \n  dplyr::filter(rt &gt;= .2 & rt &lt;= 2.5)\n\n\ndim(blur_rt)\n\n\n[1] 17211    14\n\n\nCode\ndim(blur_rt_new)\n\n\n[1] 16980    14\n\n\nThe analysis of RTs (correct trials and words) is based on 16980 data points, after removing fast and slow RTs (0.013 %)\n\n\nCode\n## Contrasts\n#hypothesis\nblurC &lt;-hypr(HB~C, HB~LB, levels=c(\"C\", \"HB\", \"LB\"))\nblurC\n\n\nhypr object containing 2 null hypotheses:\nH0.1: 0 = HB - C\nH0.2: 0 = HB - LB\n\nCall:\nhypr(~HB - C, ~HB - LB, levels = c(\"C\", \"HB\", \"LB\"))\n\nHypothesis matrix (transposed):\n   [,1] [,2]\nC  -1    0  \nHB  1    1  \nLB  0   -1  \n\nContrast matrix:\n   [,1] [,2]\nC  -2/3  1/3\nHB  1/3  1/3\nLB  1/3 -2/3\n\n\nCode\n#set contrasts in df \nblur_rt$blur &lt;- as.factor(blur_rt$blur)\n\ncontrasts(blur_rt$blur) &lt;-contr.hypothesis(blurC)\n\n\n\n5.8.1 Ex-Gaussian\n\n5.8.1.1 Model Set-up\n\n\nCode\nlibrary(cmdstanr)\n\nbform_exg1 &lt;- bf(\nrt ~  0 + blur + (1 + blur |p| participant) + (1 + blur|i| string),\nsigma ~ 0 + blur + (1 + blur |p|participant) + (1 + blur |i| string),\nbeta ~ 0 + blur + (1 + blur |p|participant) + (1 + blur |i| string))\n\n\n\n\n5.8.1.2 Run Model\n\n\nCode\nprior_exp1 &lt;- c(set_prior(\"normal(0,100)\", class = \"b\", coef=\"\"))\n                \n\nfit_exg1 &lt;- brm(\nbform_exg1, data = blur_rt,\nwarmup = 1000,\n                    iter = 5000,\n                    chains = 4,\n                    prior = prior_exp1,\n                    family = exgaussian(),\n                    init = 0,\n                    cores = 4, \nsample_prior = T, \nsave_pars = save_pars(all=T),\ncontrol = list(adapt_delta = 0.8), \nbackend=\"cmdstanr\", \nthreads = threading(4))\n\nsetwd(here::here(\"Context\", \"RT_BRM_Model\"))\n\nsave(fit_exg1, file = \"blmm_rt_context_05-25-23.RData\")\n\n\n\n\nCode\n#load rdata for model \n#load_github_data(\"https://osf.io/uxc2f/download\")\n\n#setwd(here::here(\"Expt1\", \"BRM_ACC_RT\"))\n\n#here::here(\"Expt1\", \"BRM_ACC_RT\", #\"blmm_rt_context_05-25-23.RData\"))\n\n# no intercept model - easier to fit priors on all levels of factor \nfit_c &lt;- read_rds(\"https://osf.io/82nre/download\")"
  },
  {
    "objectID": "expt1a.html#model-summary-1",
    "href": "expt1a.html#model-summary-1",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.9 Model summary",
    "text": "5.9 Model summary\n\n5.9.1 Hypotheses\n\n\nCode\na &lt;- hypothesis(fit_c, \"blurHB - blurC &gt; 0\", dpar=\"mu\")\n\nb &lt;- hypothesis(fit_c, \"blurHB - blurLB &gt; 0\", dpar=\"mu\")\n\nc &lt;- hypothesis(fit_c, \"blurLB - blurC &gt; 0\", dpar=\"mu\")\n\nd &lt;- hypothesis(fit_c, \"sigma_blurHB - sigma_blurC &gt; 0\", dpar=\"sigma\")\n\ne &lt;- hypothesis(fit_c, \"sigma_blurHB - sigma_blurLB &gt; 0\", dpar=\"sigma\")\n\nf &lt;- hypothesis(fit_c, \"sigma_blurLB - sigma_blurC = 0\", dpar=\"sigma\")\n\ng &lt;- hypothesis(fit_c, \"beta_blurHB - beta_blurC &gt; 0\", dpar=\"beta\")\n\nh &lt;- hypothesis(fit_c, \"beta_blurHB - beta_blurLB &gt; 0\", dpar=\"beta\")\n\ni &lt;- hypothesis(fit_c, \"beta_blurLB - beta_blurC = 0\", dpar=\"c\")\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis, g$hypothesis, h$hypothesis, i$hypothesis) %&gt;% \n    mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\ntab %&gt;% \n  mutate(parameter=c(\"mu\",\"mu\", \"mu\",  \"sigma\", \"sigma\", \"sigma\", \"beta\", \"beta\", \"beta\"))%&gt;%\n  mutate(Hypothesis = c(\"High Blur - Clear &gt; 0\", \"High Blur - Low Blur &gt; 0\", \"Low Blur - Clear &gt;  0 \", \"High Blur - Clear &gt; 0\", \"High Blur - Low Blur &gt; 0\", \"Low Blur - Clear =  0\",\"High Blur - Clear &gt; 0\", \"High Blur - Low Blur &gt; 0\", \"Low Blur - Clear = 0  \"))%&gt;% \n  gt(caption=md(\"Table: Ex-Gaussian Model Results Experiment 1\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Ex-Gaussian Model Results Experiment 1\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n      parameter\n    \n  \n  \n    High Blur - Clear &gt; 0\n0.212\n0.008\n0.198\n0.225\nInf\n1.000\nmu\n    High Blur - Low Blur &gt; 0\n0.196\n0.008\n0.182\n0.209\nInf\n1.000\nmu\n    Low Blur - Clear &gt;  0 \n0.016\n0.003\n0.010\n0.022\nInf\n1.000\nmu\n    High Blur - Clear &gt; 0\n0.151\n0.077\n0.023\n0.275\n33.9\n0.971\nsigma\n    High Blur - Low Blur &gt; 0\n0.115\n0.079\n-0.016\n0.242\n12.1\n0.924\nsigma\n    Low Blur - Clear =  0\n0.037\n0.055\n-0.072\n0.144\n199.3\n0.995\nsigma\n    High Blur - Clear &gt; 0\n0.417\n0.032\n0.365\n0.470\nInf\n1.000\nbeta\n    High Blur - Low Blur &gt; 0\n0.420\n0.032\n0.367\n0.472\nInf\n1.000\nbeta\n    Low Blur - Clear = 0  \n-0.002\n0.024\n-0.049\n0.044\n573.8\n0.998\nbeta\n  \n  \n  \n\n\n\n\n\n\n5.9.2 Ex-Gaussian plots\n\n\nCode\nlibrary(patchwork)\n\np1&lt;-conditional_effects(fit_c, \"blur\", dpar = \"mu\")\np2&lt;-conditional_effects(fit_c, \"blur\", dpar = \"sigma\")\np3&lt;-conditional_effects(fit_c, \"blur\", dpar = \"beta\")\n\np1 &lt;- plot(p1, plot = FALSE)[[1]] +  labs(x = \"Blur\", y = \"Mu\", \n       color = \"blur\", fill = \"blur\") + scale_x_discrete(labels=c('Clear', 'High Blur', 'Low Blur')) + theme_minimal(base_size=20)\n\np2 &lt;- plot(p2, plot = FALSE)[[1]] +  labs(x = \"Blur\", y = \"Sigma\", \n       color = \"blur\", fill = \"blur\") + scale_x_discrete(labels=c('Clear', 'High Blur', 'Low Blur')) + theme_minimal(base_size=20)\n\np3 &lt;- plot(p3, plot = FALSE)[[1]] +  labs(x = \"Blur\", y = \"Beta/Tau\", \n       color = \"blur\", fill = \"blur\") + scale_x_discrete(labels=c('Clear', 'High Blur', 'Low Blur')) + theme_minimal(base_size=20)\n\np_all = p1+p2+p3\n\np_all + plot_annotation(\n  title = 'Ex-Gaussian Analysis: Experiment 1a'\n)\n\n\n\n\n\nCode\nggsave(\"p_all-ex.png\", width=12, height=4, dpi=300)\n\n\n\n5.9.2.1 Write-up\n\n5.9.2.1.1 Ex-Gaussian\nA visualization of how blurring affected processing during word recognition can be seen Fig. 3. Beginning with the μ parameter, there was greater shifting for high blurred words compared to clear words, b = 0.212, 90% Cr.I[0.198, 0.225], ER = , and low blur words, b = 0.196, 90% Cr.I[0.182, 0.209], ER = . Analyses of the σ and τ parameters yielded a similar pattern. Variance was higher for high blurred words compared to clear words, b = 0.151, 90% Cr.I[0.023, 0.275], ER = 33.934, and low blurred words, b = 0.115, 90% Cr.I[-0.016, 0.242], ER = 12.147. Finally, there was greater skewing for high blurred words compared to clear words , b = 0.417, 90% Cr.I[0.365, 0.47], ER = and low blurred words, b = 0.42, 90% Cr.I[0.367, 0.472], ER = . Low blurred words compared to clear words only differed on the μ parameter, \\(b\\) = 0.016, 90% Cr.I[0.01, 0.022], ER = , with greater shifting for low blurred words. For \\(\\tau\\) and \\(\\sigma\\), the 95 Cr.I crossed zero and ER for no difference was greater than 100."
  },
  {
    "objectID": "expt1a.html#diffusion-model-1",
    "href": "expt1a.html#diffusion-model-1",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.10 Diffusion model",
    "text": "5.10 Diffusion model\n\n\nCode\nblur_rt_diff&lt;- rts %&gt;%\n  group_by(participant) %&gt;%\n  dplyr::filter(rt &gt;= .2 & rt &lt;= 2.5)%&gt;%\n  dplyr::filter(lex==\"m\")\n\nhead(blur_rt_diff)\n\n\n# A tibble: 6 × 14\n# Groups:   participant [1]\n   ...1 participant age   date  string study blur     rt  corr lex   list  bad_1\n  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1     3      198679 18    2022… FERRY  old   C     0.796     1 m     L4    keep \n2     5      198679 18    2022… BUNNY  old   HB    0.899     1 m     L4    keep \n3     6      198679 18    2022… HAMMER old   C     0.932     1 m     L4    keep \n4     9      198679 18    2022… CLOTH  old   LB    1.02      1 m     L4    keep \n5    10      198679 18    2022… GLOVE  old   HB    2.39      1 m     L4    keep \n6    11      198679 18    2022… PLANET old   LB    1.50      1 m     L4    keep \n# ℹ 2 more variables: bad_2 &lt;chr&gt;, bad_3 &lt;chr&gt;\n\n\n\n\nCode\nformula &lt;- bf(rt | dec(corr) ~ 0 + blur + \n                (1 + blur|p|participant) + (1+blur|i|string),  \n              ndt ~ 0 + blur + (1 + blur|p|participant) + (1+blur|i|string),\n              bias =.5)\n\n\n\nbprior &lt;- prior(normal(0, 1), class = b) +\n  prior(normal(0, 1), class = b, dpar = ndt)+\n  prior(normal(0, 1), class = sd) +\n  prior(normal(0, 1), class = sd, dpar = ndt) + \n  prior(\"normal(0, 0.3)\", class = \"sd\", group = \"participant\")+ \n  prior(\"normal(0, 0.3)\", class = \"sd\", group = \"string\")\n\n\n\n\nCode\nmake_stancode(formula, \n              family = wiener(link_bs = \"identity\", \n                              link_ndt = \"identity\",\n                              link_bias = \"identity\"),\n              data = blur_rt_diff, \n              prior = bprior)\n\ntmp_dat &lt;- make_standata(formula, \n                         family = wiener(link_bs = \"identity\", \n                              link_ndt = \"identity\",\n                              link_bias = \"identity\"),\n                            data = blur_rt_diff, prior = bprior)\nstr(tmp_dat, 1, give.attr = FALSE)\n\ninitfun &lt;- function() {\n  list(\n    b = rnorm(tmp_dat$K),\n    bs=.5, \n    b_ndt = runif(tmp_dat$K_ndt, 0.1, 0.15),\n    sd_1 = runif(tmp_dat$M_1, 0.5, 1),\n    sd_2 = runif(tmp_dat$M_2, 0.5, 1),\n    z_1 = matrix(rnorm(tmp_dat$M_1*tmp_dat$N_1, 0, 0.01),\n                 tmp_dat$M_1, tmp_dat$N_1),\n    z_2 = matrix(rnorm(tmp_dat$M_2*tmp_dat$N_2, 0, 0.01),\n                 tmp_dat$M_2, tmp_dat$N_2),\n    L_1 = diag(tmp_dat$M_1),\n    L_2 = diag(tmp_dat$M_2)\n  )\n}\n\n\n\n\nCode\nfit_wiener1 &lt;- brm(formula, \n                  data = blur_rt_diff,\n                  family = wiener(link_bs = \"identity\", \n                                  link_ndt = \"identity\",\n                                  link_bias = \"identity\"),\n                  prior = bprior, init=initfun,\n                  iter = 2000, warmup = 500, \n                  chains = 4, cores = 4,\n                  file=\"weiner_diff_1\", \n                  backend = \"cmdstanr\", threads = threading(4), \n                  control = list(max_treedepth = 15))\n\n\n\n\nCode\n#diff object on osf\nfit_wiener &lt;- read_rds(\"https://osf.io/hqauz/download\")\n\n\n\n5.10.1 Diffusion Hypotheses\n\n\nCode\na &lt;- hypothesis(fit_wiener, \"blurHB - blurC &lt; 0\", dpar=\"mu\")\n\nb &lt;- hypothesis(fit_wiener, \"blurHB - blurLB &lt; 0\", dpar=\"mu\")\n\nc = hypothesis(fit_wiener, \"blurLB - blurC = 0\", dpar=\"mu\")\n\nd &lt;- hypothesis(fit_wiener, \"ndt_blurHB - ndt_blurC &gt; 0\", dpar=\"ndt\")\n\ne &lt;- hypothesis(fit_wiener, \"ndt_blurHB - ndt_blurLB &gt; 0\", dpar=\"ndt\")\n\nf &lt;- hypothesis(fit_wiener, \"ndt_blurLB - ndt_blurC &gt; 0\", dpar=\"ndt\")\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis) %&gt;% \n    mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\ntab %&gt;% \n  mutate(parameter=c(\"v\",\"v\", \"v\",  \"T_er\", \"T_er\", \"T_er\")) %&gt;%\n  mutate(Hypothesis = c(\"High Blur - Clear &gt; 0\", \"High Blur - Low Blur &gt; 0\", \"Low Blur - Clear =  0 \", \"High Blur - Clear &gt; 0\", \"High Blur - Low Blur &gt; 0\", \"Low Blur - Clear =  0\"))%&gt;% \n  gt(caption=md(\"Table: Diffusion Model Results for Experiment 1\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Diffusion Model Results for Experiment 1\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n      parameter\n    \n  \n  \n    High Blur - Clear &gt; 0\n-0.866\n0.067\n-0.976\n-0.758\nInf\n1.000\nv\n    High Blur - Low Blur &gt; 0\n-0.869\n0.070\n-0.986\n-0.755\nInf\n1.000\nv\n    Low Blur - Clear =  0 \n0.003\n0.048\n-0.091\n0.096\n28.8\n0.966\nv\n    High Blur - Clear &gt; 0\n0.106\n0.004\n0.099\n0.113\nInf\n1.000\nT_er\n    High Blur - Low Blur &gt; 0\n0.093\n0.005\n0.085\n0.101\nInf\n1.000\nT_er\n    Low Blur - Clear =  0\n0.013\n0.003\n0.008\n0.018\nInf\n1.000\nT_er\n  \n  \n  \n\n\n\n\n\n\nCode\nme_mu &lt;- conditional_effects(fit_wiener, \"blur\", dpar = \"mu\") \n\nme_mu_plot &lt;- plot(me_mu, plot = FALSE)[[1]] +  labs(x = \"Blur\", y = \"Drift Rate\", \n       color = \"blur\", fill = \"blur\") + scale_x_discrete(labels=c('Clear', 'High Blur', 'Low Blur')) + theme_minimal(base_size=28)\n\n\nme_mu_plot\n\n\n\n\n\n\n\nCode\nme_ndt &lt;- conditional_effects(fit_wiener, \"blur\", dpar = \"ndt\") \n\nme_ndt_plot &lt;- plot(me_ndt, plot = FALSE)[[1]] +  labs(x = \"Blur\", y = \"Non-Decision Time\", \n       color = \"blur\", fill = \"blur\") + scale_x_discrete(labels=c('Clear', 'High Blur', 'Low Blur')) + theme_minimal(base_size=28)\n\n\np_all = me_ndt_plot + me_mu_plot\n\np_all + plot_annotation(\n  title = 'Diffusion Analysis: Experiment 1a')\n\n\n\n\n\nCode\nggsave(\"p_all-diff.png\", width=14, height=4, dpi=300)\n\n\n\n\n5.10.2 Write-up\n\n5.10.2.1 Diffusion Model\nHigh blurred words had lower drift rate than clear words, b = -0.866, 90% Cr.I[-0.976, -0.758], ER = , and low blurred words, b = -0.869, 90% Cr.I[-0.986, -0.755], ER = . There was no difference in drift rate between low blurred words and cleared words, b = 0.003, 90% Cr.I[-0.091, 0.096], ER = 28.84. Non-decision time was higher for high blurred words compared to clear words, b = 0.106, 90% Cr.I[0.099, 0.113], ER = , and low blurred words, b = 0.093, 90% Cr.I[0.085, 0.101], ER = . Low blurred words had a higher non-decision time that clear words, b = 0.013, 90% Cr.I[0.008, 0.018], ER = ."
  },
  {
    "objectID": "expt1a.html#quantile-plotsvincentiles",
    "href": "expt1a.html#quantile-plotsvincentiles",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.11 Quantile Plots/Vincentiles",
    "text": "5.11 Quantile Plots/Vincentiles\n\nFigure 1Figure 2\n\n\n\n\nCode\n#Delta plots (one per subject) \nquibble &lt;- function(x, q = seq(.1, .9, .2)) {\n  tibble(x = quantile(x, q), q = q)\n}\n\ndata.quantiles &lt;- rts %&gt;%\n  dplyr::filter(rt &gt;= .2 | rt &lt;= 2.5) %&gt;% \n  dplyr::group_by(participant,blur,corr) %&gt;%\n  dplyr::filter(lex==\"m\")%&gt;%\n  dplyr::summarise(RT = list(quibble(rt, seq(.1, .9, .2)))) %&gt;% \n  tidyr::unnest(RT)\n\n\ndata.delta &lt;- data.quantiles %&gt;%\n  dplyr::filter(corr==1) %&gt;%\n  dplyr::select(-corr) %&gt;%\n  dplyr::group_by(participant, blur, q) %&gt;%\n  dplyr::summarize(RT=mean(x))\n\n\n\n\nCode\n#Delta plots (based on vincentiles)\nvincentiles &lt;- data.quantiles %&gt;%\n  dplyr::filter(corr==1) %&gt;%\n  dplyr::select(-corr) %&gt;%\n  dplyr::group_by(blur,q) %&gt;%\n  dplyr::summarize(RT=mean(x)) \n\nv=vincentiles %&gt;%\n  dplyr::group_by(blur,q) %&gt;%\n  dplyr::summarise(MRT=mean(RT))\n\nv &lt;- v %&gt;%\n  mutate(blur=ifelse(blur==\"HB\", \"High blur\", ifelse(blur==\"LB\", \"Low blur\", \"Clear\")))\n\n\nv$blur&lt;- factor(v$blur, level=c(\"High blur\", \"Low blur\", \"Clear\"))\n\n\n\np &lt;- ggplot(v, aes(x = q, y = MRT*1000, colour = blur, group=blur))+\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  scale_colour_manual(values=met.brewer(\"Cassatt2\", 3)) +\n  theme_minimal(base_size=28)  +\n  scale_y_continuous(breaks=seq(500,1600,100)) +\n  theme(legend.title=element_blank())+\n    coord_cartesian(ylim = c(500, 1600)) +\n  scale_x_continuous(breaks=seq(.1,.9, .2))+\n  geom_label_repel(data=v, aes(x=q, y=MRT*1000, label=round(MRT*1000,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5) + \n  labs(title = \"Quantile Analysis\", x = \"Quantiles\", y = \"Response latencies in ms\") \n\np\n\n\n\n\n\n\n\n\n\nCode\np2 &lt;- ggplot(data=v,aes(y=MRT, x=fct_relevel(blur, c(\"HB\", \"C\", \"LB\")), color=q)) +\n  geom_line()+\n  geom_point(size=4) + \n  labs(x=\"Blur\") + \n    scale_x_discrete(labels=c('High Blur', 'Low Blur', 'Clear'))\n\np2\n\n\n\n\n\n\n5.12 \n\n\n\n\n\n5.12.1 Delta Plots\n\n5.12.1.1 Clear vs. High Blur\n\n\nCode\n#| fig-cap: \"Delta plots depicting the magnitude of the effect over time in Experiment 1. Each dot represents the mean RT at the .1, .3, .5, .7 and .9 quantiles.\"\n#| \n\n v_chb &lt;- v %&gt;%\n    dplyr::filter(blur==\"Clear\" | blur==\"High blur\") %&gt;%\n    dplyr::group_by(q)%&gt;%\n     mutate(mean_rt = mean(MRT)*1000) %&gt;%\n     ungroup() %&gt;% select(-q) %&gt;%\n   tidyr::pivot_wider(names_from = \"blur\", values_from = \"MRT\") %&gt;%\n    mutate(diff=`High blur`*1000-Clear*1000)\n \n \np1 &lt;- ggplot(v_chb, aes(x = mean_rt, y = diff)) + \n  geom_abline(intercept = 0, slope = 0) +\n  geom_line(size = 1, colour = \"black\") +\n  geom_point(size = 3, colour = \"black\") +\n  theme_minimal(base_size=28) + \n  theme(legend.position = \"none\") +\nscale_y_continuous(breaks=seq(110,440,50)) +\n    coord_cartesian(ylim = c(110, 440)) +\n  scale_x_continuous(breaks=seq(600,1300, 200))+\n   geom_label_repel(data=v_chb, aes(y=diff, label=round(diff,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5)+\n  labs( title = \"Delta Plots: Clear - High Blur\", x = \"Mean RT per quantile\", y = \"Group differences\")\n\np1\n\n\n\n\n\n\n\n5.12.1.2 Clear vs. Low Blur\n\n\nCode\n v_clb &lt;- v %&gt;%\n    dplyr::filter(blur==\"Clear\" | blur==\"Low blur\") %&gt;%\n    dplyr::group_by(q)%&gt;%\n     mutate(mean_rt = mean(MRT)*1000) %&gt;%\n     ungroup() %&gt;% \n   select(-q) %&gt;%\n   tidyr::pivot_wider(names_from = \"blur\", values_from = \"MRT\") %&gt;%\n    mutate(diff=`Low blur`*1000-Clear*1000)\n \n\n\np2 &lt;- ggplot(v_clb, aes(x = mean_rt, y = diff)) + \n  geom_abline(intercept = 0, slope = 0) +\n  geom_line(size = 1, colour = \"black\") +\n  geom_point(size = 3, colour = \"black\") +\n  theme_minimal(base_size=28) + \n  theme(legend.position = \"none\") + \nscale_y_continuous(breaks=seq(-10, 70, 10)) +\n    coord_cartesian(ylim = c(-10, 70)) +\n  scale_x_continuous(breaks=seq(500,1150, 200))+\n    geom_label_repel(data=v_clb, aes(y=diff, label=round(diff,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5) + \n  labs( title = \"Delta Plots: Low Blur - Clear\", x = \"Mean RT per quantile\", y = \"Group differences\")\n\n\np2\n\n\n\n\n\nDelta plots depicting the magnitude of the effect over time in Experiment 1. Each dot represents the mean RT at the .1, .3, .5, .7 and .9 quantiles.\n\n\n\n\n\n\n5.12.1.3 High Blur vs. Low Blur\n\n\nCode\nv_hlb &lt;- v %&gt;%\n  dplyr::filter(blur==\"High blur\" | blur==\"Low blur\") %&gt;%\n  dplyr::group_by(q)%&gt;%\n  mutate(mean_rt = mean(MRT)*1000) %&gt;%\n     ungroup() %&gt;% \n   select(-q) %&gt;%\n  tidyr::pivot_wider(names_from = \"blur\", values_from = \"MRT\") %&gt;%\n  mutate(diff=`High blur`*1000-`Low blur`*1000)\n\n\np3 &lt;- ggplot(v_hlb, aes(x = mean_rt, y = diff)) + \n  geom_abline(intercept = 0, slope = 0) +\n  geom_line(size = 1, colour = \"black\") +\n  geom_point(size = 3, colour = \"black\") +\n  theme_minimal(base_size = 28) + \n  theme(legend.position = \"none\") + \n  scale_x_continuous(breaks=seq(600,1350, 200))+\n    geom_label_repel(data=v_hlb, aes(y=diff, label=round(diff,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5)+ \n  labs( title = \"Delta Plots High Blur - Low Blur\", x = \"Mean RT per quantile\", y = \"Group differences\")\n\n\np3\n\n\n\n\n\nDelta plots depicting the magnitude of the effect over time in Experiment 1. Each dot represents the mean RT at the .1, .3, .5, .7 and .9 quantiles.\n\n\n\n\n\n\nCode\np / (p1 + p2 + p3)\n\n\n\n\n\nCode\n# save figure\nggsave(filename='./figures/figure_kde.png',width=24,height=12)"
  },
  {
    "objectID": "expt1a.html#section-1",
    "href": "expt1a.html#section-1",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.12 ",
    "text": "5.12"
  },
  {
    "objectID": "expt1a.html#brm-conditionalized-memory",
    "href": "expt1a.html#brm-conditionalized-memory",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.13 BRM: Conditionalized Memory",
    "text": "5.13 BRM: Conditionalized Memory\n\n\\(D\\prime\\)\n\n\n\nCode\nmem_c &lt;- read_csv(\"https://osf.io/xjvc7/download\")\n\nhead(mem_c)\n\n\n# A tibble: 6 × 12\n   ...1 participant string blur  date  study    rt  corr lex   sayold condition1\n  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1     1      198679 ALLEY  HB    2022… m     1.24      1 y          1 High Blur \n2     2      198679 ARROW  C     2022… m     0.767     1 y          1 Clear     \n3     3      198679 ARTIST HB    2022… m     1.06      0 y          0 High Blur \n4     4      198679 BAKER  C     2022… m     0.997     1 y          1 Clear     \n5     5      198679 BAMBOO LB    2022… m     0.745     1 y          1 Low Blur  \n6     6      198679 BANANA HB    2022… m     0.697     1 y          1 High Blur \n# ℹ 1 more variable: isold &lt;dbl&gt;"
  },
  {
    "objectID": "expt1a.html#contrast-code-2",
    "href": "expt1a.html#contrast-code-2",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.14 Contrast code",
    "text": "5.14 Contrast code\n\n\nCode\n## Contrasts\n#hypothesis\nblurC &lt;-hypr(HB~C, HB~LB, levels=c(\"C\", \"HB\", \"LB\"))\nblurC\n\n\nhypr object containing 2 null hypotheses:\nH0.1: 0 = HB - C\nH0.2: 0 = HB - LB\n\nCall:\nhypr(~HB - C, ~HB - LB, levels = c(\"C\", \"HB\", \"LB\"))\n\nHypothesis matrix (transposed):\n   [,1] [,2]\nC  -1    0  \nHB  1    1  \nLB  0   -1  \n\nContrast matrix:\n   [,1] [,2]\nC  -2/3  1/3\nHB  1/3  1/3\nLB  1/3 -2/3\n\n\nCode\n#set contrasts in df \nmem_c$blur &lt;- as.factor(mem_c$blur)\n\ncontrasts(mem_c$blur) &lt;-contr.hypothesis(blurC)"
  },
  {
    "objectID": "expt1a.html#brm-model",
    "href": "expt1a.html#brm-model",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.15 BRM Model",
    "text": "5.15 BRM Model\n\n\nCode\nprior_exp2 &lt;- c(set_prior(\"cauchy(0,.35)\", class = \"b\"))\n\nfit_mem_c &lt;- brm(sayold ~ 0 + isold*blur + (1+isold*blur|participant) + (1+isold*blur|string), data=mem_c, \nwarmup = 1000,\n                    iter = 5000,\n                    chains = 4, \n                    init=0, \n                    family = bernoulli(link = \"probit\"),\n                    cores = 4, \ncontrol = list(adapt_delta = 0.9),\nprior=prior_exp2, \nsample_prior = T, \nfile=\"blmm_sdt_c_nointercept\"\nsave_pars = save_pars(all=T),\nbackend=\"cmdstanr\",\n\nthreads = threading(4))\n\n\n\n5.15.1 Marginal Means and Differences\n\n\nCode\nfit_mem1 &lt;- read_rds(\"https://osf.io/xsvgt/download\")\n\n\nsd_lb &lt;- read_rds(\"https://osf.io/4qp38/download\")"
  },
  {
    "objectID": "expt1a.html#test-hypotheses",
    "href": "expt1a.html#test-hypotheses",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.16 Test Hypotheses",
    "text": "5.16 Test Hypotheses\n\n5.16.1 High Blur &gt; Clear\n\n\nCode\n#HB &gt; C\na &lt;- hypothesis(fit_mem1 , \"isold1:blur1 &gt; 0\")\nb &lt;- hypothesis(fit_mem1 , \"isold1:blur2 &gt; 0\")\nc &lt;- hypothesis(sd_lb , \"isold1:blur1 = 0\")\n\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis) %&gt;%\n    mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\n\ntab %&gt;%\n  gt(caption=md(\"Table: Memory Directional Hypotheses Experiment 1\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Memory Directional Hypotheses Experiment 1\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n    \n  \n  \n    (isold1:blur1) &gt; 0\n0.128\n0.042\n0.059\n0.196\n694.65\n0.999\n    (isold1:blur2) &gt; 0\n0.124\n0.041\n0.057\n0.192\n665.67\n0.999\n    (isold1:blur1) = 0\n0.018\n0.041\n-0.061\n0.099\n2.02\n0.669"
  },
  {
    "objectID": "expt1a.html#write-up-2",
    "href": "expt1a.html#write-up-2",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.17 Write-up",
    "text": "5.17 Write-up\n\n5.17.1 Sensitivity\nHigh blur words were better remembered than clear words, \\(\\beta\\) = 0.128, 90% Cr.I[0.059, 0.196], ER = 694.652, and low blur words, \\(\\beta\\) = 0.124, 90% Cr.I[0.057, 0.192], ER = 665.667. There was no difference in sensitivity between clear words and low blur words, \\(\\beta\\) = 0.018, 90% Cr.I[-0.061, 0.099], ER = 665.667"
  },
  {
    "objectID": "expt1a.html#exploratory",
    "href": "expt1a.html#exploratory",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.18 Exploratory",
    "text": "5.18 Exploratory\n\n5.18.1 Bias\n\n\nCode\na &lt;- hypothesis(fit_mem1 , \"blur2 &lt; 0\")\nb &lt;- hypothesis(sd_lb , \"blur1 &lt; 0\")\nc&lt;-  hypothesis(fit_mem1 , \"blur1= 0\")\n\n\nLow blurred words had a bias towards more “old” responses compared to clear words, \\\\(\\beta\\) = 0.119, 90% Cr.I[0.061, 0.177], ER = 4.377^{-4}, and high blurred words, \\(\\beta\\) = -0.038, 90% Cr.I[-0.073, -0.003], ER = 26.444. There was no difference in bias between high blurred words and clear words, \\(\\beta\\) = 0.079, 90% Cr.I[0.014, 0.143], ER = 0.282.\n\n\n5.18.2 D’, C, and Differences\n\n\nCode\n# (Negative) criteria\nemm_m1_c1 &lt;- emmeans(fit_mem1, ~blur) %&gt;% \n  parameters::parameters(centrality = \"mean\")\n# Differences in (negative) criteria\nemm_m1_c2 &lt;- emmeans(fit_mem1, ~blur) %&gt;% \n  contrast(\"pairwise\") %&gt;% \n  parameters::parameters(centrality = \"mean\")\n\n# Dprimes for three groups\nemm_m1_d1 &lt;- emmeans(fit_mem1, ~isold + blur) %&gt;% \n  contrast(\"revpairwise\", by = \"blur\") %&gt;% \n  parameters::parameters(centrality = \"mean\")\n# Differences between groups\nemm_m1_d2 &lt;- emmeans(fit_mem1, ~isold + blur) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\")) %&gt;% \n  parameters::parameters(centrality = \"mean\")\n\n\n\n\nCode\nemm_m1_c1 &lt;- emmeans(fit_mem1, ~blur) \n\n  \nemm_m1_c2 &lt;- emmeans(fit_mem1, ~blur) %&gt;% \n  contrast(\"pairwise\")\n\n# Dprimes for three groups\nemm_m1_d1 &lt;- emmeans(fit_mem1, ~isold + blur) %&gt;% \n  contrast(\"revpairwise\", by = \"blur\")\n# Differences between groups\nemm_m1_d2 &lt;- emmeans(fit_mem1, ~isold + blur) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\")) \n\n\ntmp &lt;- bind_rows(\n  bind_rows(\n    gather_emmeans_draws(emm_m1_d1) %&gt;% \n      group_by(blur) %&gt;% \n      select(-contrast),\n    gather_emmeans_draws(emm_m1_d2) %&gt;% \n      rename(\n        blur = blur_pairwise\n      ) %&gt;% \n      group_by(blur) %&gt;% \n      select(-isold_revpairwise)\n  ),\n  bind_rows(\n    gather_emmeans_draws(emm_m1_c1),\n    gather_emmeans_draws(emm_m1_c2) %&gt;% \n      rename(\n        blur = contrast\n      )\n  ),\n  .id = \"Parameter\"\n) %&gt;% \n  mutate(Parameter = factor(Parameter, labels = c(\"d-prime\", \"Criterion\"))) %&gt;% \n  mutate(\n    t = if_else(str_detect(blur, \" - \"), \"Differences\", \"Group means\") %&gt;% \n      fct_inorder(),\n    blur = fct_inorder(blur)\n  )\n\ntmp %&gt;%   \n  ungroup() %&gt;% \n  mutate(.value = if_else(Parameter == \"Criterion\", .value * -1, .value)) %&gt;% \n  mutate(Parameter = fct_rev(Parameter)) %&gt;% \n  mutate(blur=case_when(\n  blur==\"C\"~ \"Clear\", \n  blur==\"C - HB\" ~ \"Clear - High blur\", \n  blur==\"HB\" ~ \"High blur\", \n  blur==\"LB\" ~ \"Low blur\", \n  blur==\"C - LB\" ~ \"Clear - Low blur\", \n  TRUE ~ \"High blur - Low blur\"\n  )) %&gt;% \n  ggplot(aes(blur, .value)) +\n  labs(\n    x = \"Blurring Level (or difference)\",\n    y = \"Parameter value\"\n  ) +\n  stat_halfeye(colour=\"blue\") + \n    facet_grid(Parameter~t, scales = \"free\") + \n    geom_hline(yintercept = 0, linewidth = .25) + \n  theme_minimal(base_size=26)\n\n\n\n\n\nPosterior distributions and 90%CIs of the criterion and dprime parameters, or differences therein, from the conditionalized model\n\n\n\n\nCode\nggsave(\"expt1a-dprime.png\", width=19, height=8, dpi=300)\n\n\n\n\nCode\nlibrary(ggdist)\n\n#classic SDT\nsdt &lt;- mem_c %&gt;%\ndplyr::mutate(type = \"hit\",\ntype = ifelse(isold==1 & sayold==0, \"miss\", type),\ntype = ifelse(isold==0 & sayold==0, \"cr\", type), # Correct rejection\ntype = ifelse(isold==0 & sayold==1, \"fa\", type))\n\nsdt &lt;- sdt %&gt;%\ndplyr::group_by(participant, blur, type) %&gt;%\ndplyr::summarise(count = n()) %&gt;%\ntidyr::spread(type, count) # Format data to one row per person\n\nsdt2 &lt;- sdt %&gt;%\ndplyr::group_by(participant, blur)%&gt;%\ndplyr::mutate(hr = hit / (hit+miss),\nfa = fa / (fa+cr)) %&gt;%\ndplyr::mutate(hr=case_when(\nis.na(hr) ~ 0.99,\nTRUE ~ hr),\nfa=case_when(\nis.na(fa) ~ 0.01,\nTRUE ~ fa),\nzhr=qnorm(hr),\nzfa=qnorm(fa),\ndprime = zhr-zfa, \ncrit = -0.5 * (zhr + zfa))%&gt;%\nungroup()\n\n\n\ntop_mean &lt;- sdt2  %&gt;%  #get means for plot\n   dplyr::group_by(blur)%&gt;%\n   dplyr::summarise(mean1=mean(dprime))\n\np_mean &lt;-sdt2 %&gt;%  #get means for plot\n    dplyr::group_by(participant, blur)%&gt;%\n     dplyr::summarise(mean1=mean(dprime))"
  },
  {
    "objectID": "expt1a.html#discussion",
    "href": "expt1a.html#discussion",
    "title": "3  Experiment 1a: Context Reinstatement",
    "section": "5.19 Discussion",
    "text": "5.19 Discussion\nExperiment 1a successfully replicated the pattern of results found in Rosner et al. (2015). Specifically, we found high blurred words had lower accuracy than clear and low blurred words and had better memory compared.\nAdding to this, we utilized cognitive and mathematical modeling to gain further insights into the mechanisms underlying the perceptual disfluency effect. Descriptively, high blurred words induced a more pronounced shift in the RT distribution (μ) and exhibited a higher degree of skew (τ) compared to clear and low blurred words. However, low blurred words did not differ compared to clear words on \\(\\mu\\) or \\(\\beta\\). These patterns can be clearly seen in our quantile plots and delta plots in Fig. 4.\nWe also fit the RTs and accuracy data to a diffusion model, which allowed us to make stronger inferences as it relates to stages of processing. High blurred words impacted both an early, non-decision, component evinced by higher \\(T_{er}\\) and a later more analytic, component evinced by a lower \\(v\\) than clear or low blurred words. On the other hand, low blurred words only affected \\(T_{er}\\).\nWe present evidence that different levels of disfluency can influence distinct stages of encoding, potentially contributing to the presence or absence of a mnemonic effect for perceptually blurred stimuli. Unlike most studies that commonly employ a single level of disfluency, our study incorporated two levels of disfluency. The results indicate that a subtle manipulation such as low blur primarily affects early processing stages, whereas a more pronounced perceptual manipulation (i.e., high blur) impacts both early and late processing stages. Regarding recognition memory, high blurred stimuli were better recognized compared to low blurred and clear words. This suggests that in order to observe a perceptual disfluency effect, the perceptual manipulation must be sufficiently disfluent to do so.\nGiven the important theoretical implications of these findings, Experiment 1B served as a conceptual replication. Due to the bias observed in the recognition memory test (i.e., low blurred words were responded to more liberally), we will not present old and new items as blurred, instead all of the words will be presented in a clear, different, font at test.\n\n\n\n\n\n\nBarthelme, S. (2023). Imager: Image processing library based on ’CImg’. https://CRAN.R-project.org/package=imager\n\n\nBürkner, P.-C. (2017a). Brms: An r package for bayesian multilevel models using stan. 80. https://doi.org/10.18637/jss.v080.i01\n\n\nBürkner, P.-C. (2017b). Brms: An r package for bayesian multilevel models using stan. 80. https://doi.org/10.18637/jss.v080.i01\n\n\nGeller, J., & Peterson, D. (2021). Is this going to be on the test? Test expectancy moderates the disfluency effect with sans forgetica. Journal of Experimental Psychology: Learning, Memory, and Cognition, 47(12), 1924–1938. https://doi.org/10.1037/xlm0001042\n\n\nGeller, J., Still, M. L., Dark, V. J., & Carpenter, S. K. (2018). Would disfluency by any other name still be disfluent? Examining the disfluency effect with cursive handwriting. Memory and Cognition, 46(7), 11091126. https://doi.org/10.3758/s13421-018-0824-6\n\n\nGrant, R. L., Carpenter, B., Furr, D. C., & Gelman, A. (2017). Introducing the StataStan Interface for Fast, Complex Bayesian Modeling Using Stan. The Stata Journal: Promoting Communications on Statistics and Stata, 17(2), 330–342. https://doi.org/10.1177/1536867x1701700205\n\n\nKinoshita, S., Amos, A., & Norris, D. (2023). Diacritic priming in novice readers of diacritics. Journal of Experimental Psychology: Human Perception and Performance, 49(3), 370–383. https://doi.org/10.1037/xhp0001084\n\n\nLenth, R. V. (2023). Emmeans: Estimated marginal means, aka least-squares means. https://CRAN.R-project.org/package=emmeans\n\n\nMakowski, D., Ben-Shachar, M. S., & Lüdecke, D. (2019). bayestestR: Describing effects and their uncertainty, existence and significance within the bayesian framework. 4, 1541. https://doi.org/10.21105/joss.01541\n\n\nMorey, R. D., & Rouder, J. N. (2022). BayesFactor: Computation of bayes factors for common designs. https://CRAN.R-project.org/package=BayesFactor\n\n\nPerea, M., Gil-López, C., Beléndez, V., & Carreiras, M. (2016). Do handwritten words magnify lexical effects in visual word recognition? Quarterly Journal of Experimental Psychology, 69(8), 1631–1647. https://doi.org/10.1080/17470218.2015.1091016\n\n\nRosner, T. M., Davis, H., & Milliken, B. (2015). Perceptual blurring and recognition memory: A desirable difficulty effect revealed. Acta Psychologica, 160, 11–22. https://doi.org/10.1016/j.actpsy.2015.06.006\n\n\nSchad, D. J., Vasishth, S., Hohenstein, S., & Kliegl, R. (2019). How to capitalize on a priori contrasts in linear (mixed) models: A tutorial. 110. https://doi.org/10.1016/j.jml.2019.104038\n\n\nVandekerckhove, J., Tuerlinckx, F., & Lee, M. D. (2011). Hierarchical diffusion models for two-choice response times. Psychological Methods, 16(1), 44–62. https://doi.org/10.1037/a0021765\n\n\nVergara-Martínez, M., Gutierrez-Sigut, E., Perea, M., Gil-López, C., & Carreiras, M. (2021). The time course of processing handwritten words: An ERP investigation. Neuropsychologia, 159, 107924. https://doi.org/10.1016/j.neuropsychologia.2021.107924\n\n\nZloteanu, M., & Vuorre, M. (2023). Bayesian generalized linear mixed effects models for deception detection analyses. http://dx.doi.org/10.31234/osf.io/fdh5b"
  },
  {
    "objectID": "Experiment2.html",
    "href": "Experiment2.html",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "",
    "text": "5 Method\nAll raw and summary data, materials, and R scripts for pre-processing, analysis, and plotting for Experiment 2 can be found at https://osf.io/6sy7k/"
  },
  {
    "objectID": "Experiment2.html#figure-theme",
    "href": "Experiment2.html#figure-theme",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "4.1 Figure Theme",
    "text": "4.1 Figure Theme\n\n\nCode\nbold &lt;- element_text(face = \"bold\", color = \"black\", size = 16) #axis bold\n\ntheme_set(theme_bw(base_size = 15, base_family = \"Arial\"))\n\ntheme_update(\n  panel.grid.major = element_line(color = \"grey92\", size = .4),\n  panel.grid.minor = element_blank(),\n  axis.title.x = element_text(color = \"grey30\", margin = margin(t = 7)),\n  axis.title.y = element_text(color = \"grey30\", margin = margin(r = 7)),\n  axis.text = element_text(color = \"grey50\"),\n  axis.ticks =  element_line(color = \"grey92\", size = .4),\n  axis.ticks.length = unit(.6, \"lines\"),\n  legend.position = \"top\",\n  plot.title = element_text(hjust = 0, color = \"black\", \n                            family = \"Arial\",\n                            size = 21, margin = margin(t = 10, b = 35)),\n  plot.subtitle = element_text(hjust = 0, face = \"bold\", color = \"grey30\",\n                               family = \"Arial\", \n                               size = 14, margin = margin(0, 0, 25, 0)),\n  plot.title.position = \"plot\",\n  plot.caption = element_text(color = \"grey50\", size = 10, hjust = 1,\n                              family = \"Arial\", \n                              lineheight = 1.05, margin = margin(30, 0, 0, 0)),\n  plot.caption.position = \"plot\", \n  plot.margin = margin(rep(20, 4))\n)\npal &lt;- c(met.brewer(\"Veronese\", 3))\n\n\n\n\nCode\n## flat violinplots\n### It relies largely on code previously written by David Robinson \n### (https://gist.github.com/dgrtwo/eb7750e74997891d7c20) and ggplot2 by H Wickham\n#check if required packages are installed\n#Load packages\n# Defining the geom_flat_violin function. Note: the below code modifies the \n# existing github page by removing a parenthesis in line 50\n\ngeom_flat_violin &lt;- function(mapping = NULL, data = NULL, stat = \"ydensity\",\n                             position = \"dodge\", trim = TRUE, scale = \"area\",\n                             show.legend = NA, inherit.aes = TRUE, ...) {\n  layer(\n    data = data,\n    mapping = mapping,\n    stat = stat,\n    geom = GeomFlatViolin,\n    position = position,\n    show.legend = show.legend,\n    inherit.aes = inherit.aes,\n    params = list(\n      trim = trim,\n      scale = scale,\n      ...\n    )\n  )\n}\n# horizontal nudge position adjustment\n# copied from https://github.com/tidyverse/ggplot2/issues/2733\nposition_hnudge &lt;- function(x = 0) {\n  ggproto(NULL, PositionHNudge, x = x)\n}\nPositionHNudge &lt;- ggproto(\"PositionHNudge\", Position,\n                          x = 0,\n                          required_aes = \"x\",\n                          setup_params = function(self, data) {\n                            list(x = self$x)\n                          },\n                          compute_layer = function(data, params, panel) {\n                            transform_position(data, function(x) x + params$x)\n                          }\n)"
  },
  {
    "objectID": "Experiment2.html#participants",
    "href": "Experiment2.html#participants",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "5.1 Participants",
    "text": "5.1 Participants\nWe used the same sample size as Experiment 1a (N = 216). All participants were recruited through the university subject pool at Rutgers University (SONA). We used a similar exclusion criteria to Experiment 1a. Because of this, we oversampled we randomly chose 36 participants from each list to reach our target sample size."
  },
  {
    "objectID": "Experiment2.html#apparatus-stimuli-design-procedure-and-analysis",
    "href": "Experiment2.html#apparatus-stimuli-design-procedure-and-analysis",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "5.2 Apparatus, stimuli, design, procedure, and analysis",
    "text": "5.2 Apparatus, stimuli, design, procedure, and analysis\nSimilar to Experiment 1a, the experiment was run using PsychoPy (Peirce et al., 2019) and hosted on Pavlovia (www.pavlovia.org). You can see an example of the experiment by navigating to this website: https://run.pavlovia.org/Jgeller112/ldt_dd_l1_jol. You can also download the source code for the experiment at this site.\nWe used the same stimuli from Experiment 1a. The main difference between Experiment 1a and 1b was all items were presented in a clear, Arial font. To make it more similar to Experiment 1a each set of words presented as clear, low blur, and high blur at study were yoked to a set of new words that were counterbalanced across lists. Therefore, instead of there being one false alarm rate there were 3, one for each blurring level. This ensured each word was compared to studied clear, studied high blurred, and studied low blurred words.\nWe fit the same statistical models as Experiment 1a."
  },
  {
    "objectID": "Experiment2.html#accuracy",
    "href": "Experiment2.html#accuracy",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "6.1 Accuracy",
    "text": "6.1 Accuracy\nThe data file is cleaned (participants &gt;=.8, no duplicate participants, no participants &lt; 17. )\n\n\nCode\n# get data from osf\nblur_acc &lt;- read_csv(\"https://osf.io/excgd/download\") %&gt;%\n    dplyr::filter(lex==\"m\")\n\n\nblur_acc_new&lt;- blur_acc %&gt;%\n  dplyr::filter(rt &gt;= .2 & rt &lt;= 2.5)\n\nhead(blur_acc)\n\n\n# A tibble: 6 × 14\n   ...1 participant age   date  string study blur     rt  corr lex   list  bad_1\n  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1     1      198670 18    2022… POODLE old   LB    0.702     1 m     L3    keep \n2     4      198670 18    2022… CRUST  old   C     0.773     1 m     L3    keep \n3    10      198670 18    2022… GREASE old   LB    0.617     1 m     L3    keep \n4    11      198670 18    2022… SCREW  old   LB    0.647     1 m     L3    keep \n5    14      198670 18    2022… CRADLE old   HB    1.35      1 m     L3    keep \n6    16      198670 18    2022… THROAT old   LB    0.664     1 m     L3    keep \n# ℹ 2 more variables: bad_2 &lt;chr&gt;, bad_3 &lt;chr&gt;\n\n\nCode\ndim(blur_acc)\n\n\n[1] 18144    14\n\n\nCode\ndim(blur_acc_new)\n\n\n[1] 17809    14\n\n\nThe analysis of accuracy is is based on 18144 data points. After removing fast and slow RTs we were left with 17809 data point (0.018 %)"
  },
  {
    "objectID": "Experiment2.html#contrast-code",
    "href": "Experiment2.html#contrast-code",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "6.2 Contrast Code",
    "text": "6.2 Contrast Code\n\n\nCode\n## Contrasts\n#hypothesis\nblurC &lt;-hypr(HB~C, HB~LB, levels=c(\"C\", \"HB\", \"LB\"))\nblurC\n\n\nhypr object containing 2 null hypotheses:\nH0.1: 0 = HB - C\nH0.2: 0 = HB - LB\n\nCall:\nhypr(~HB - C, ~HB - LB, levels = c(\"C\", \"HB\", \"LB\"))\n\nHypothesis matrix (transposed):\n   [,1] [,2]\nC  -1    0  \nHB  1    1  \nLB  0   -1  \n\nContrast matrix:\n   [,1] [,2]\nC  -2/3  1/3\nHB  1/3  1/3\nLB  1/3 -2/3\n\n\nCode\n#set contrasts in df \nblur_acc$blur &lt;- as.factor(blur_acc$blur)\n\ncontrasts(blur_acc$blur) &lt;-contr.hypothesis(blurC)"
  },
  {
    "objectID": "Experiment2.html#brms-accuracy-model",
    "href": "Experiment2.html#brms-accuracy-model",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "6.3 BRMs: Accuracy Model",
    "text": "6.3 BRMs: Accuracy Model\n\n\nCode\n#weak prior\nprior_exp1 &lt;- c(set_prior(\"cauchy(0,.35)\", class = \"b\"))\n\n#fit model\nfit_acc_weak &lt;- brm(corr ~ blur + (1+blur|participant) + (1+blur|string), data=blur_acc_new, \nwarmup = 1000,\n                    iter = 5000,\n                    chains = 4, \n                    init=0, \n                    family = bernoulli(),\n     cores = 4,\nprior = prior_exp1, \ncontrol = list(adapt_delta = 0.9), \nbackend=\"cmdstanr\", \nsave_pars = save_pars(all=T),\nsample_prior = T, \nthreads = threading(4), \nfile=\"fit_acc_weak_nocontext\")\n\n\n\n\nCode\n# get file from osf\ntmp &lt;- tempdir()\ndownload.file(\"https://osf.io/ne36z/download\", \n              file.path(tmp, \"acc_blmm_expnocontext.RData\"))\nload(file.path(tmp, \"acc_blmm_expnocontext.RData\"))\n\nfit_acc_lbc &lt;- read_rds(\"https://osf.io/yhz4c/download\")"
  },
  {
    "objectID": "Experiment2.html#model-summary",
    "href": "Experiment2.html#model-summary",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "6.4 Model Summary",
    "text": "6.4 Model Summary\n\n6.4.1 Hypotheses\n\n\nCode\nacc_means &lt;- emmeans(fit_acc_noc, specs=\"blur\", type=\"response\") %&gt;%\n  as.data.frame()\n\n\n\n\nCode\na = hypothesis(fit_acc_noc , \"blur1 &lt; 0\")\nb= hypothesis(fit_acc_noc , \"blur2 &lt; 0\")\nc= hypothesis(fit_acc_lbc, \"blur1 =  0\")\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis)%&gt;% \n    mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\ntab %&gt;% \n   mutate(Hypothesis = c(\"High Blur - Clear &lt; 0\", \"High Blur - Low Blur &lt; 0\", \"Low Blur - Clear = 0 \")) %&gt;% \n  gt(caption=md(\"Table: Experiment 1b Accuracy\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Experiment 1b Accuracy\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n    \n  \n  \n    High Blur - Clear &lt; 0\n-1.111\n0.206\n-1.454\n-0.783\nInf\n1.000\n    High Blur - Low Blur &lt; 0\n-1.072\n0.187\n-1.384\n-0.769\nInf\n1.000\n    Low Blur - Clear = 0 \n0.069\n0.111\n-0.149\n0.285\n0.905\n0.475\n  \n  \n  \n\n\n\n\n\n\n6.4.2 Accuracy Figures\n\n\nCode\ntop_mean &lt;-blur_acc%&gt;%  #get means for each blur cond for plot\n  dplyr::filter(lex==\"m\")%&gt;%\n  group_by(blur)%&gt;%\n   dplyr::summarise(mean1=mean(corr)) %&gt;%\n  dplyr::ungroup()\n\n\np_mean &lt;-blur_acc %&gt;%  #get means participant x cond for  plottin\n  dplyr::filter(lex==\"m\")%&gt;%\n    dplyr::group_by(participant, blur)%&gt;%\n     dplyr::summarise(mean1=mean(corr))\n\n\np3 &lt;- ggplot(p_mean, aes(x = blur , y = mean1, fill = blur)) +\n    coord_cartesian(ylim = c(.5,1)) + \n  \n  ggdist::stat_halfeye(\n    aes(\n      y = mean1,\n      color = blur,\n      fill = after_scale(lighten(color, .5))\n    ),\n    shape = 18,\n    point_size = 3,\n    interval_size = 1.8,\n    adjust = .5,\n    .width = c(0, 1)\n  ) +\n    geom_point(aes(x = blur, y = mean1, colour = blur),position = position_jitter(width = .05), size = 1, shape = 20)+\n    geom_boxplot(aes(x = blur, y = mean1, fill = blur),outlier.shape = NA, alpha = .5, width = .1, colour = \"black\")+\n  labs(subtitle = \"Word Accuracy: No Context Reinstatement\")+\n     scale_color_manual(values=met.brewer(\"Cassatt2\", 3))+\n scale_fill_manual(values=met.brewer(\"Cassatt2\", 3))+\n    stat_summary(fun=mean, geom=\"point\", colour=\"darkred\", size=3)+\n    labs(y = \"Accuracy\", x = \"Blur\") +\n    geom_label_repel(data=top_mean, aes(y=mean1, label=round(mean1, 2)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5) + \n    theme(axis.text=bold) + theme(legend.position = \"none\") \n  #  ggsave('place.png', width = 8, height = 6)\np3\n\n\n\n\n\n\n\n6.4.3 Accuracy\nClear words were better identified (\\(M\\) = .987) compared to high blur words (\\(M\\) = .962), \\(b\\) = -1.111, 95% Cr.I[-1.454, -0.783], ER = . Low blurred words were better identified (\\(M\\) = .\\(M\\) = .987) than high blurred words, \\(b\\) = -1.072, 95% Cr.I[-1.384, -0.769], ER = . However, the evidence was weak for there being no significant difference in the identification accuracy between clear and low blurred words, b = 0.069, 95% Cr.I[-0.149, 0.285], ER = 0.905."
  },
  {
    "objectID": "Experiment2.html#rts",
    "href": "Experiment2.html#rts",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "6.5 RTs",
    "text": "6.5 RTs"
  },
  {
    "objectID": "Experiment2.html#brms-ex-gaussian",
    "href": "Experiment2.html#brms-ex-gaussian",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "6.6 BRMs: Ex-Gaussian",
    "text": "6.6 BRMs: Ex-Gaussian\n\n\nCode\n#load data from osf\nrts &lt;- read_csv(\"https://osf.io/excgd/download\")\n\n\n\n\nCode\nblur_rt&lt;- rts %&gt;%\n  group_by(participant) %&gt;%\n   dplyr::filter(corr==1, lex==\"m\")#only include nonwords\n\nblur_rt_new &lt;- blur_rt %&gt;% \n  dplyr::filter(rt &gt;= .2 & rt &lt;= 2.5) %&gt;%\n  mutate(rt_ms=rt*1000)\n\n\ndim(blur_rt)\n\n\n[1] 17222    14\n\n\nCode\ndim(blur_rt_new)\n\n\n[1] 16939    15\n\n\nThe analysis of RTs (correct trials and words) is is based on 16939 data points, after removing fast and slow RTs (0.016 %)\n\n6.6.1 Density Plots\n\n\nCode\np &lt;- ggplot(blur_rt_new, aes(rt_ms, group = blur, fill = blur)) +\n  geom_density(colour = \"black\", size = 0.75, alpha = 0.5) +\n  scale_fill_manual(values=c(\"grey40\", \"orange1\", \"red\")) +\n  theme(axis.title = element_text(size = 16, face = \"bold\", colour = \"black\"), \n        axis.text = element_text(size = 16, colour = \"black\"), \n        plot.title = element_text(face = \"bold\", size = 20)) +\n  coord_cartesian(xlim=c(600, 1100)) +\n  scale_x_continuous(breaks=seq(600,1100,100)) +\n  labs(title = \"Density Plot By Blur\", y = \"Density\", x = \"Response latencies in ms\") + \n    theme_bw() \n\np"
  },
  {
    "objectID": "Experiment2.html#contrasts",
    "href": "Experiment2.html#contrasts",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "6.7 Contrasts",
    "text": "6.7 Contrasts\n\n\nCode\n#hypothesis\nblurC &lt;-hypr(HB~C, HB~LB, levels=c(\"C\", \"HB\", \"LB\"))\nblurC\n\n\nhypr object containing 2 null hypotheses:\nH0.1: 0 = HB - C\nH0.2: 0 = HB - LB\n\nCall:\nhypr(~HB - C, ~HB - LB, levels = c(\"C\", \"HB\", \"LB\"))\n\nHypothesis matrix (transposed):\n   [,1] [,2]\nC  -1    0  \nHB  1    1  \nLB  0   -1  \n\nContrast matrix:\n   [,1] [,2]\nC  -2/3  1/3\nHB  1/3  1/3\nLB  1/3 -2/3\n\n\nCode\n#set contrasts in df \nblur_rt$blur &lt;- as.factor(blur_rt$blur)\n\ncontrasts(blur_rt$blur) &lt;-contr.hypothesis(blurC)\n\n\n\n6.7.1 Ex-Gaussian\n\n6.7.1.1 Model Set-up\n\n\nCode\nbform_exg1 &lt;- bf(\nrt ~ 0+ blur + (1 + blur |p| participant) + (1 + blur|i| string),\nsigma ~ 0+ blur + (1 + blur |p|participant) + (1 + blur |i| string),\nbeta ~ 0 + blur + (1 + blur |p|participant) + (1 + blur |i| string))\n\n\n\n\n6.7.1.2 Run Model\n\n\nCode\nprior_exp1 &lt;- c(set_prior(\"normal(0,10)\", class = \"b\"), \n                 set_prior(\"normal(0,10)\", class = \"b\", dpar=\"sigma\"), \n                 set_prior(\"normal(0,10)\", class = \"b\", dpar=\"beta\")\n                \n\nfit_exg1 &lt;- brm(\nbform_exg1, data = blur_rt,\nwarmup = 1000,\n                    iter = 5000,\n                    chains = 4,\n                    prior = prior_exp1,\n                    family = exgaussian(),\n                    init = 0,\n                    cores = 4, \nsample_prior = T, \nsave_pars = save_pars(all=T),\ncontrol = list(adapt_delta = 0.8), \nbackend=\"cmdstanr\", \nthreads = threading(4))\n\n\n\n\nCode\n#load rdata for model \n#load_github_data(\"https://osf.io/uxc2f/download\")\n\n\nfit_exg1 &lt;- read_rds(\"https://osf.io/egqyt/download\")\n\n\n\n\n\n6.7.2 Model summary\n\n6.7.2.1 Hypotheses\n\n\nCode\na &lt;- hypothesis(fit_exg1, \"blurHB - blurC &gt; 0\", dpar=\"mu\")\n\nb &lt;- hypothesis(fit_exg1, \"blurHB - blurLB &gt; 0\", dpar=\"mu\")\n\nc &lt;- hypothesis(fit_exg1, \"blurLB - blurC &gt; 0\", dpar=\"mu\")\n\nd &lt;- hypothesis(fit_exg1, \"sigma_blurHB - sigma_blurC &gt; 0\", dpar=\"sigma\")\n\ne &lt;- hypothesis(fit_exg1, \"sigma_blurHB - sigma_blurLB &gt; 0\", dpar=\"sigma\")\n\nf &lt;- hypothesis(fit_exg1, \"sigma_blurLB - sigma_blurC = 0\", dpar=\"sigma\")\n\ng &lt;- hypothesis(fit_exg1, \"beta_blurHB - beta_blurC &gt; 0\", dpar=\"beta\")\n\nh &lt;- hypothesis(fit_exg1, \"beta_blurHB - beta_blurLB &gt; 0\", dpar=\"beta\")\n\ni &lt;- hypothesis(fit_exg1, \"beta_blurLB - beta_blurC = 0\", dpar=\"c\")\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis, g$hypothesis, h$hypothesis, i$hypothesis) %&gt;% \n    mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\ntab %&gt;% \n  mutate(parameter=c(\"mu\",\"mu\", \"mu\",  \"sigma\", \"sigma\", \"sigma\", \"beta\", \"beta\", \"beta\"))%&gt;%\n  mutate(Hypothesis = c(\"High Blur - Clear &gt; 0\", \"High Blur - Low Blur &gt; 0\", \"Low Blur - Clear &gt;  0 \", \"High Blur - Clear &gt; 0\", \"High Blur - Low Blur &gt; 0\", \"Low Blur - Clear =  0\",\"High Blur - Clear &gt; 0\", \"High Blur - Low Blur &gt; 0\", \"Low Blur - Clear = 0  \")) %&gt;%\n  gt(caption=md(\"Table: Ex-Gaussian Model Results Experiment 1\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Ex-Gaussian Model Results Experiment 1\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n      parameter\n    \n  \n  \n    High Blur - Clear &gt; 0\n0.208\n0.009\n0.194\n0.222\nInf\n1.000\nmu\n    High Blur - Low Blur &gt; 0\n0.194\n0.009\n0.180\n0.209\nInf\n1.000\nmu\n    Low Blur - Clear &gt;  0 \n0.014\n0.003\n0.008\n0.019\nInf\n1.000\nmu\n    High Blur - Clear &gt; 0\n0.273\n0.075\n0.149\n0.393\n3199.0\n1.000\nsigma\n    High Blur - Low Blur &gt; 0\n0.399\n0.083\n0.263\n0.535\nInf\n1.000\nsigma\n    Low Blur - Clear =  0\n-0.126\n0.061\n-0.250\n-0.009\n27.9\n0.965\nsigma\n    High Blur - Clear &gt; 0\n0.368\n0.031\n0.317\n0.419\nInf\n1.000\nbeta\n    High Blur - Low Blur &gt; 0\n0.349\n0.032\n0.296\n0.401\nInf\n1.000\nbeta\n    Low Blur - Clear = 0  \n0.019\n0.024\n-0.028\n0.066\n444.0\n0.998\nbeta\n  \n  \n  \n\n\n\n\n\n\n6.7.2.2 Ex-Gaussian plots\n\n\nCode\np1&lt;-conditional_effects(fit_exg1, \"blur\", dpar = \"mu\")\np2&lt;-conditional_effects(fit_exg1, \"blur\", dpar = \"sigma\")\np3&lt;-conditional_effects(fit_exg1, \"blur\", dpar = \"beta\")\n\np1\n\n\n\n\n\nCode\np2\n\n\n\n\n\nCode\np3\n\n\n\n\n\n\n\n\n6.7.3 Write-up\n\n6.7.3.1 Ex-Gaussian\nA visualization of how blurring affected processing can be seen Fig. 5. Beginning with the μ parameter, there was greater shifting for high blurred words (vs. clear words), \\(b\\) = 0.208, 95% Cr.I[0.194, 0.222], ER = , and low blur words, \\(b\\) = 0.194, 95% Cr.I[0.18, 0.209], ER = . Analyses of the σ and τ parameters yielded a similar pattern.High blurred word had greater variance than clear words, \\(b\\) = 0.273, 95% Cr.I[0.149, 0.393], ER = 3199, and low blurred words, \\(b\\) = 0.399, 95% Cr.I[0.263, 0.535], ER = . Finally, there was greater skewing for high blurred words (vs. clear words), \\(b\\) = 0.368, 95% Cr.I[0.317, 0.419], ER = and for high blur (vs. clear) words, \\(b\\) = 0.349, 95% Cr.I[0.296, 0.401], ER = . Low blurred words (vs. clear words) only differed on the μ parameter, \\(b\\) = 0.014, 95% Cr.I[0.008, 0.019], ER = , with greater shifting for low blurred words. For \\(\\tau\\) and \\(\\sigma\\), the 95 Cr.I crossed zero and ER for no difference was greater than 100.\n\n\n\n6.7.4 Diffusion modeling\n\n\nCode\nblur_rt_diff&lt;- rts %&gt;%\n  group_by(participant) %&gt;%\n  dplyr::filter(rt &gt;= .2 & rt &lt;= 2.5)%&gt;%\n  dplyr::filter(lex==\"m\")\n\nhead(blur_rt_diff)\n\n\n# A tibble: 6 × 14\n# Groups:   participant [1]\n   ...1 participant age   date  string study blur     rt  corr lex   list  bad_1\n  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1     1      198670 18    2022… POODLE old   LB    0.702     1 m     L3    keep \n2     4      198670 18    2022… CRUST  old   C     0.773     1 m     L3    keep \n3    10      198670 18    2022… GREASE old   LB    0.617     1 m     L3    keep \n4    11      198670 18    2022… SCREW  old   LB    0.647     1 m     L3    keep \n5    14      198670 18    2022… CRADLE old   HB    1.35      1 m     L3    keep \n6    16      198670 18    2022… THROAT old   LB    0.664     1 m     L3    keep \n# ℹ 2 more variables: bad_2 &lt;chr&gt;, bad_3 &lt;chr&gt;\n\n\n\n\nCode\nformula &lt;- bf(rt | dec(corr) ~ 0 + blur + \n                (1 + blur|p|participant) + (1+blur|i|string),  \n              ndt ~ 0 + blur + (1 + blur|p|participant) + (1+blur|i|string),\n              bias =.5)\n\nbprior &lt;- prior(normal(0, 1), class = b) +\n  prior(normal(0, 1), class = b, dpar = ndt)+\n  prior(normal(0, 1), class = sd) +\n  prior(normal(0, 1), class = sd, dpar = ndt) + \n  prior(\"normal(0, 0.3)\", class = \"sd\", group = \"participant\")+ \n  prior(\"normal(0, 0.3)\", class = \"sd\", group = \"string\")\n\n\n\n\nCode\nmake_stancode(formula, \n              family = wiener(link_bs = \"identity\", \n                              link_ndt = \"identity\",\n                              link_bias = \"identity\"),\n              data = blur_rt_diff, \n              prior = bprior)\n\n\n// generated with brms 2.19.0\nfunctions {\n /* compute correlated group-level effects\n  * Args:\n  *   z: matrix of unscaled group-level effects\n  *   SD: vector of standard deviation parameters\n  *   L: cholesky factor correlation matrix\n  * Returns:\n  *   matrix of scaled group-level effects\n  */\n  matrix scale_r_cor(matrix z, vector SD, matrix L) {\n    // r is stored in another dimension order than z\n    return transpose(diag_pre_multiply(SD, L) * z);\n  }\n  /* Wiener diffusion log-PDF for a single response\n   * Args:\n   *   y: reaction time data\n   *   dec: decision data (0 or 1)\n   *   alpha: boundary separation parameter &gt; 0\n   *   tau: non-decision time parameter &gt; 0\n   *   beta: initial bias parameter in [0, 1]\n   *   delta: drift rate parameter\n   * Returns:\n   *   a scalar to be added to the log posterior\n   */\n   real wiener_diffusion_lpdf(real y, int dec, real alpha,\n                              real tau, real beta, real delta) {\n     if (dec == 1) {\n       return wiener_lpdf(y | alpha, tau, beta, delta);\n     } else {\n       return wiener_lpdf(y | alpha, tau, 1 - beta, - delta);\n     }\n   }\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  int&lt;lower=0,upper=1&gt; dec[N];  // decisions\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int&lt;lower=1&gt; K_ndt;  // number of population-level effects\n  matrix[N, K_ndt] X_ndt;  // population-level design matrix\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  int&lt;lower=1&gt; J_1[N];  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  vector[N] Z_1_2;\n  vector[N] Z_1_3;\n  vector[N] Z_1_ndt_4;\n  vector[N] Z_1_ndt_5;\n  vector[N] Z_1_ndt_6;\n  int&lt;lower=1&gt; NC_1;  // number of group-level correlations\n  // data for group-level effects of ID 2\n  int&lt;lower=1&gt; N_2;  // number of grouping levels\n  int&lt;lower=1&gt; M_2;  // number of coefficients per level\n  int&lt;lower=1&gt; J_2[N];  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_2_1;\n  vector[N] Z_2_2;\n  vector[N] Z_2_3;\n  vector[N] Z_2_ndt_4;\n  vector[N] Z_2_ndt_5;\n  vector[N] Z_2_ndt_6;\n  int&lt;lower=1&gt; NC_2;  // number of group-level correlations\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  real min_Y = min(Y);\n}\nparameters {\n  vector[K] b;  // population-level effects\n  real&lt;lower=0&gt; bs;  // boundary separation parameter\n  vector[K_ndt] b_ndt;  // population-level effects\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  matrix[M_1, N_1] z_1;  // standardized group-level effects\n  cholesky_factor_corr[M_1] L_1;  // cholesky factor of correlation matrix\n  vector&lt;lower=0&gt;[M_2] sd_2;  // group-level standard deviations\n  matrix[M_2, N_2] z_2;  // standardized group-level effects\n  cholesky_factor_corr[M_2] L_2;  // cholesky factor of correlation matrix\n}\ntransformed parameters {\n  real bias = 0.5;  // initial bias parameter\n  matrix[N_1, M_1] r_1;  // actual group-level effects\n  // using vectors speeds up indexing in loops\n  vector[N_1] r_1_1;\n  vector[N_1] r_1_2;\n  vector[N_1] r_1_3;\n  vector[N_1] r_1_ndt_4;\n  vector[N_1] r_1_ndt_5;\n  vector[N_1] r_1_ndt_6;\n  matrix[N_2, M_2] r_2;  // actual group-level effects\n  // using vectors speeds up indexing in loops\n  vector[N_2] r_2_1;\n  vector[N_2] r_2_2;\n  vector[N_2] r_2_3;\n  vector[N_2] r_2_ndt_4;\n  vector[N_2] r_2_ndt_5;\n  vector[N_2] r_2_ndt_6;\n  real lprior = 0;  // prior contributions to the log posterior\n  // compute actual group-level effects\n  r_1 = scale_r_cor(z_1, sd_1, L_1);\n  r_1_1 = r_1[, 1];\n  r_1_2 = r_1[, 2];\n  r_1_3 = r_1[, 3];\n  r_1_ndt_4 = r_1[, 4];\n  r_1_ndt_5 = r_1[, 5];\n  r_1_ndt_6 = r_1[, 6];\n  // compute actual group-level effects\n  r_2 = scale_r_cor(z_2, sd_2, L_2);\n  r_2_1 = r_2[, 1];\n  r_2_2 = r_2[, 2];\n  r_2_3 = r_2[, 3];\n  r_2_ndt_4 = r_2[, 4];\n  r_2_ndt_5 = r_2[, 5];\n  r_2_ndt_6 = r_2[, 6];\n  lprior += normal_lpdf(b | 0, 1);\n  lprior += gamma_lpdf(bs | 1, 1);\n  lprior += normal_lpdf(b_ndt | 0, 1);\n  lprior += normal_lpdf(sd_1 | 0, 0.3)\n    - 6 * normal_lccdf(0 | 0, 0.3);\n  lprior += lkj_corr_cholesky_lpdf(L_1 | 1);\n  lprior += normal_lpdf(sd_2 | 0, 0.3)\n    - 6 * normal_lccdf(0 | 0, 0.3);\n  lprior += lkj_corr_cholesky_lpdf(L_2 | 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] ndt = rep_vector(0.0, N);\n    mu += X * b;\n    ndt += X_ndt * b_ndt;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_1_2[J_1[n]] * Z_1_2[n] + r_1_3[J_1[n]] * Z_1_3[n] + r_2_1[J_2[n]] * Z_2_1[n] + r_2_2[J_2[n]] * Z_2_2[n] + r_2_3[J_2[n]] * Z_2_3[n];\n    }\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      ndt[n] += r_1_ndt_4[J_1[n]] * Z_1_ndt_4[n] + r_1_ndt_5[J_1[n]] * Z_1_ndt_5[n] + r_1_ndt_6[J_1[n]] * Z_1_ndt_6[n] + r_2_ndt_4[J_2[n]] * Z_2_ndt_4[n] + r_2_ndt_5[J_2[n]] * Z_2_ndt_5[n] + r_2_ndt_6[J_2[n]] * Z_2_ndt_6[n];\n    }\n    for (n in 1:N) {\n      target += wiener_diffusion_lpdf(Y[n] | dec[n], bs, ndt[n], bias, mu[n]);\n    }\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(to_vector(z_1));\n  target += std_normal_lpdf(to_vector(z_2));\n}\ngenerated quantities {\n  // compute group-level correlations\n  corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1);\n  vector&lt;lower=-1,upper=1&gt;[NC_1] cor_1;\n  // compute group-level correlations\n  corr_matrix[M_2] Cor_2 = multiply_lower_tri_self_transpose(L_2);\n  vector&lt;lower=-1,upper=1&gt;[NC_2] cor_2;\n  // extract upper diagonal of correlation matrix\n  for (k in 1:M_1) {\n    for (j in 1:(k - 1)) {\n      cor_1[choose(k - 1, 2) + j] = Cor_1[j, k];\n    }\n  }\n  // extract upper diagonal of correlation matrix\n  for (k in 1:M_2) {\n    for (j in 1:(k - 1)) {\n      cor_2[choose(k - 1, 2) + j] = Cor_2[j, k];\n    }\n  }\n}\n\n\nCode\ntmp_dat &lt;- make_standata(formula, \n                         family = wiener(link_bs = \"identity\", \n                              link_ndt = \"identity\",\n                              link_bias = \"identity\"),\n                            data = blur_rt_diff, prior = bprior)\nstr(tmp_dat, 1, give.attr = FALSE)\n\n\nList of 29\n $ N         : int 17809\n $ Y         : num [1:17809(1d)] 0.702 0.773 0.617 0.647 1.346 ...\n $ dec       : num [1:17809(1d)] 1 1 1 1 1 1 1 1 1 1 ...\n $ K         : int 3\n $ X         : num [1:17809, 1:3] 0 1 0 0 0 0 1 0 0 0 ...\n $ Z_1_1     : num [1:17809(1d)] 1 1 1 1 1 1 1 1 1 1 ...\n $ Z_1_2     : num [1:17809(1d)] 0 0 0 0 1 0 0 0 1 0 ...\n $ Z_1_3     : num [1:17809(1d)] 1 0 1 1 0 1 0 1 0 1 ...\n $ Z_2_1     : num [1:17809(1d)] 1 1 1 1 1 1 1 1 1 1 ...\n $ Z_2_2     : num [1:17809(1d)] 0 0 0 0 1 0 0 0 1 0 ...\n $ Z_2_3     : num [1:17809(1d)] 1 0 1 1 0 1 0 1 0 1 ...\n $ K_ndt     : int 3\n $ X_ndt     : num [1:17809, 1:3] 0 1 0 0 0 0 1 0 0 0 ...\n $ Z_1_ndt_4 : num [1:17809(1d)] 1 1 1 1 1 1 1 1 1 1 ...\n $ Z_1_ndt_5 : num [1:17809(1d)] 0 0 0 0 1 0 0 0 1 0 ...\n $ Z_1_ndt_6 : num [1:17809(1d)] 1 0 1 1 0 1 0 1 0 1 ...\n $ Z_2_ndt_4 : num [1:17809(1d)] 1 1 1 1 1 1 1 1 1 1 ...\n $ Z_2_ndt_5 : num [1:17809(1d)] 0 0 0 0 1 0 0 0 1 0 ...\n $ Z_2_ndt_6 : num [1:17809(1d)] 1 0 1 1 0 1 0 1 0 1 ...\n $ bias      : num 0.5\n $ J_1       : int [1:17809(1d)] 1 1 1 1 1 1 1 1 1 1 ...\n $ J_2       : int [1:17809(1d)] 119 48 71 129 45 152 109 84 148 154 ...\n $ N_1       : int 216\n $ M_1       : int 6\n $ NC_1      : int 15\n $ N_2       : int 168\n $ M_2       : int 6\n $ NC_2      : int 15\n $ prior_only: int 0\n\n\nCode\ninitfun &lt;- function() {\n  list(\n    b = rnorm(tmp_dat$K),\n    bs=.5, \n    b_ndt = runif(tmp_dat$K_ndt, 0.1, 0.15),\n    sd_1 = runif(tmp_dat$M_1, 0.5, 1),\n    sd_2 = runif(tmp_dat$M_2, 0.5, 1),\n    z_1 = matrix(rnorm(tmp_dat$M_1*tmp_dat$N_1, 0, 0.01),\n                 tmp_dat$M_1, tmp_dat$N_1),\n    z_2 = matrix(rnorm(tmp_dat$M_2*tmp_dat$N_2, 0, 0.01),\n                 tmp_dat$M_2, tmp_dat$N_2),\n    L_1 = diag(tmp_dat$M_1),\n    L_2 = diag(tmp_dat$M_2)\n  )\n}\n\n\n\n\nCode\nfit_wiener1 &lt;- brm(formula, \n                  data = blur_rt_diff,\n                  family = wiener(link_bs = \"identity\", \n                                  link_ndt = \"identity\",\n                                  link_bias = \"identity\"),\n                  prior = bprior, init=initfun,\n                  iter = 2000, warmup = 500, \n                  chains = 4, cores = 4,\n                  file=\"weiner_diff_1\", \n                  backend = \"cmdstanr\", threads = threading(4), \n                  control = list(max_treedepth = 15))\n\n\n\n\nCode\nfit_wiener &lt;- read_rds(\"https://osf.io/3j98t/download\")\n\n\n\n6.7.4.1 Hypotheses\n\n\nCode\na &lt;- hypothesis(fit_wiener, \"blurHB - blurC &lt; 0\", dpar=\"mu\")\nb &lt;- hypothesis(fit_wiener, \"blurHB - blurLB &lt; 0\", dpar=\"mu\")\nc &lt;- hypothesis(fit_wiener, \"blurLB - blurC = 0\", dpar=\"mu\")\n\nd &lt;- hypothesis(fit_wiener, \"ndt_blurHB - ndt_blurC &gt; 0\", dpar=\"ndt\")\ne &lt;- hypothesis(fit_wiener, \"ndt_blurHB - ndt_blurLB &gt; 0\", dpar=\"ndt\")\nf &lt;- hypothesis(fit_wiener, \"ndt_blurLB - ndt_blurC &gt; 0\", dpar=\"ndt\")\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis) %&gt;% \n    mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\ntab %&gt;% \n  mutate(parameter=c(\"v\",\"v\", \"v\", \"T_er\", \"T_er\", \"T_er\"))%&gt;%\n   mutate(Hypothesis = c(\"High Blur - Clear &lt; 0\", \"High Blur - Low Blur &lt; 0\", \"Low Blur - Clear =  0 \", \"High Blur - Clear &lt; 0\", \"High Blur - Low Blur &lt; 0\", \"Low Blur - Clear &gt;  0 \")) %&gt;% \n  gt(caption=md(\"Table: Diffusion Model Experiment 1b\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Diffusion Model Experiment 1b\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n      parameter\n    \n  \n  \n    High Blur - Clear &lt; 0\n-0.908\n0.065\n-1.016\n-0.799\nInf\n1.000\nv\n    High Blur - Low Blur &lt; 0\n-0.884\n0.072\n-1.005\n-0.766\nInf\n1.000\nv\n    Low Blur - Clear =  0 \n-0.023\n0.055\n-0.130\n0.086\n24.3\n0.961\nv\n    High Blur - Clear &lt; 0\n0.107\n0.005\n0.100\n0.115\nInf\n1.000\nT_er\n    High Blur - Low Blur &lt; 0\n0.093\n0.005\n0.085\n0.102\nInf\n1.000\nT_er\n    Low Blur - Clear &gt;  0 \n0.014\n0.003\n0.009\n0.020\nInf\n1.000\nT_er\n  \n  \n  \n\n\n\n\n\n\nCode\nme_mu &lt;- conditional_effects(fit_wiener, \"blur\", dpar = \"mu\") \n\nplot(me_mu, plot = FALSE)[[1]] +  labs(x = \"Blur\", y = \"Drift Rate\", \n       color = \"blur\", fill = \"blur\") +  scale_x_discrete(labels=c('Clear', 'High Blur', 'Low Blur'))\n\n\n\n\n\n\n\nCode\nme_mu &lt;- conditional_effects(fit_wiener, \"blur\", dpar = \"ndt\") \n\nplot(me_mu, plot = FALSE)[[1]] +  labs(x = \"Blur\", y = \"Non-Decision Time\", \n       color = \"blur\", fill = \"blur\") +  scale_x_discrete(labels=c('Clear', 'High Blur', 'Low Blur'))\n\n\n\n\n\n\n\n\n6.7.5 Write-up\n\n6.7.5.1 Diffusion Model\nLooking at drift rate, high blurred words had lower drift rate than clear words, \\(b\\) = -0.908, 95% Cr.I[-1.016, -0.799], ER = , and low blurred words, \\(b\\) = -0.884, 95% Cr.I[-1.005, -0.766], ER = . There was no difference in drift rate between Low blurred words and cleared words, \\(b\\) = -0.023, 95% Cr.I[-0.13, 0.086], ER = 24.322. Non-decision time was higher for high blurred words compared to clear words, \\(b\\) = 0.107, 95% Cr.I[0.1, 0.115], ER = , and low blurred words, \\(b\\) = 0.093, 95% Cr.I[0.085, 0.102], ER = . Low blurred words had a higher non-decision time than clear words, \\(b\\) = 0.014, 95% Cr.I[0.009, 0.02], ER = .\n\n\n\n6.7.6 Quantile Plots/Vincentiles\n\nFigure 1Figure 2\n\n\n\n\nCode\n#Delta plots (one per subject) \nquibble &lt;- function(x, q = seq(.1, .9, .2)) {\n  tibble(x = quantile(x, q), q = q)\n}\n\ndata.quantiles &lt;- rts %&gt;%\n  dplyr::filter(rt &gt;= .2 | rt &lt;= 2.5) %&gt;% \n  dplyr::group_by(participant,blur,corr) %&gt;%\n  dplyr::filter(lex==\"m\")%&gt;%\n  dplyr::summarise(RT = list(quibble(rt, seq(.1, .9, .2)))) %&gt;% \n  tidyr::unnest(RT)\n\n\ndata.delta &lt;- data.quantiles %&gt;%\n  dplyr::filter(corr==1) %&gt;%\n  dplyr::select(-corr) %&gt;%\n  dplyr::group_by(participant, blur, q) %&gt;%\n  dplyr::summarize(RT=mean(x))\n\n\n\n\nCode\n#Delta plots (based on vincentiles)\nvincentiles &lt;- data.quantiles %&gt;%\n  dplyr::filter(corr==1) %&gt;%\n  dplyr::select(-corr) %&gt;%\n  dplyr::group_by(blur,q) %&gt;%\n  dplyr::summarize(RT=mean(x)) \n\nv=vincentiles %&gt;%\n  dplyr::group_by(blur,q) %&gt;%\n  dplyr::summarise(MRT=mean(RT))\n\nv$blur&lt;- factor(v$blur, level=c(\"HB\", \"LB\", \"C\"))\n\n\np &lt;- ggplot(v, aes(x = q, y = MRT*1000, colour = blur, group=blur))+\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  scale_colour_manual(values=met.brewer(\"Cassatt2\", 3)) +\n  theme_bw() + \n  theme(axis.title = element_text(size = 16, face = \"bold\"), \n        axis.text = element_text(size = 16),\n        plot.title = element_text(face = \"bold\", size = 20)) +\n  scale_y_continuous(breaks=seq(500,1600,100)) +\n  theme(legend.title=element_blank())+\n    coord_cartesian(ylim = c(500, 1600)) +\n  scale_x_continuous(breaks=seq(.1,.9, .2))+\n  geom_label_repel(data=v, aes(x=q, y=MRT*1000, label=round(MRT*1000,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5)+\n  labs(title = \"Quantile Analysis\", x = \"Quantiles\", y = \"Response latencies in ms\")\n\np\n\n\n\n\n\n\n\n\n\nCode\np2 &lt;- ggplot(data=v,aes(y=MRT, x=fct_relevel(blur, c(\"HB\", \"C\", \"LB\")), color=q)) +\n  geom_line()+\n  geom_point(size=4) + \n  labs(x=\"blur\", y=\"Reaction Time (ms)\")\n\np2\n\n\n\n\n\n\n\n\n\n\n6.7.7 Delta Plots\n\n6.7.7.1 Clear vs. High Blur\n\n\nCode\n#diff\n\n v_chb &lt;- v %&gt;%\n    dplyr::filter(blur==\"C\" | blur==\"HB\") %&gt;%\n    dplyr::group_by(q)%&gt;%\n     mutate(mean_rt = mean(MRT)*1000) %&gt;%\n     ungroup() %&gt;% select(-q) %&gt;%\n   tidyr::pivot_wider(names_from = \"blur\", values_from = \"MRT\") %&gt;%\n    mutate(diff=HB*1000-C*1000)\n \n \n   \n\n\np3 &lt;- ggplot(v_chb, aes(x = mean_rt, y = diff)) + \n  geom_abline(intercept = 0, slope = 0) +\n  geom_line(size = 1, colour = \"black\") +\n  geom_point(size = 3, colour = \"black\") +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  theme(axis.title = element_text(size = 16, face = \"bold\"), \n        axis.text = element_text(size = 16),\n        plot.title = element_text(face = \"bold\", size = 20)) +\nscale_y_continuous(breaks=seq(110,440,50)) +\n    coord_cartesian(ylim = c(110, 440)) +\n  scale_x_continuous(breaks=seq(600,1300, 200))+\n   geom_label_repel(data=v_chb, aes(y=diff, label=round(diff,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5)+\n  labs( title = \"Clear - High Blur\", x = \"Mean RT per quantile\", y = \"Group differences\")\n\np3\n\n\n\n\n\n\n\n6.7.7.2 Clear vs. Low Blur\n\n\nCode\n v_clb &lt;- v %&gt;%\n    dplyr::filter(blur==\"C\" | blur==\"LB\") %&gt;%\n    dplyr::group_by(q)%&gt;%\n     mutate(mean_rt = mean(MRT)*1000) %&gt;%\n     ungroup() %&gt;% \n   select(-q) %&gt;%\n   tidyr::pivot_wider(names_from = \"blur\", values_from = \"MRT\") %&gt;%\n    mutate(diff=LB*1000-C*1000)\n \n\n\np4 &lt;- ggplot(v_clb, aes(x = mean_rt, y = diff)) + \n  geom_abline(intercept = 0, slope = 0) +\n  geom_line(size = 1, colour = \"black\") +\n  geom_point(size = 3, colour = \"black\") +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  theme(axis.title = element_text(size = 16, face = \"bold\"), \n        axis.text = element_text(size = 16),\n        plot.title = element_text(face = \"bold\", size = 20)) +\nscale_y_continuous(breaks=seq(10, 70, 10)) +\n    coord_cartesian(ylim = c(10, 70)) +\n  scale_x_continuous(breaks=seq(500,1150, 200))+\n    geom_label_repel(data=v_clb, aes(y=diff, label=round(diff,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5) + \n  labs( title = \"Low Blur - Clear\", x = \"Mean RT per quantile\", y = \"Group differences\")\n\n\np4\n\n\n\n\n\n\n\n6.7.7.3 High Blur vs. Low Blur\n\n\nCode\nv_hlb &lt;- v %&gt;%\n  dplyr::filter(blur==\"HB\" | blur==\"LB\") %&gt;%\n  dplyr::group_by(q)%&gt;%\n  mutate(mean_rt = mean(MRT)*1000) %&gt;%\n     ungroup() %&gt;% \n   select(-q) %&gt;%\n  tidyr::pivot_wider(names_from = \"blur\", values_from = \"MRT\") %&gt;%\n  mutate(diff=HB*1000-LB*1000)\n\n\np5 &lt;- ggplot(v_hlb, aes(x = mean_rt, y = diff)) + \n  geom_abline(intercept = 0, slope = 0) +\n  geom_line(size = 1, colour = \"black\") +\n  geom_point(size = 3, colour = \"black\") +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  theme(axis.title = element_text(size = 16, face = \"bold\"), \n        axis.text = element_text(size = 16),\n        plot.title = element_text(face = \"bold\", size = 20)) +\n  scale_x_continuous(breaks=seq(600,1350, 200))+\n    geom_label_repel(data=v_hlb, aes(y=diff, label=round(diff,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5)+ \n  labs( title = \"High Blur - Low Blur\", x = \"Mean RT per quantile\", y = \"Group differences\")\n\n\np5\n\n\n\n\n\n\n\n\n6.7.8 Quantile/delta summary plots\n\n\nCode\nbottom &lt;- cowplot::plot_grid(p3, p4,p5, \n                   ncol = 3, \n                   nrow = 1,\n                   label_size = 14, \n                   hjust = -0.8, \n                   scale=.95,\n                   align = \"v\")\n\ncowplot::plot_grid(p, bottom, \n                   ncol=1, nrow=2)"
  },
  {
    "objectID": "Experiment2.html#brm-conditionalized-memory",
    "href": "Experiment2.html#brm-conditionalized-memory",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "6.8 BRM: Conditionalized Memory",
    "text": "6.8 BRM: Conditionalized Memory\n\n\\(D\\prime\\)\n\n\n\nCode\nmem_nc &lt;- read_csv(\"https://osf.io/jw2gx/download\")\n\nhead(mem_nc)\n\n\n# A tibble: 6 × 12\n   ...1 participant string blur  date            study     rt  corr lex   sayold\n  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1     1      198754 ALLEY  C     2022-03-02_16h… m     0.0606     1 y          1\n2     2      198754 ARTIST C     2022-03-02_16h… m     0.151      1 y          1\n3     3      198754 BAKER  LB    2022-03-02_16h… m     0.209      1 y          1\n4     4      198754 BAMBOO HB    2022-03-02_16h… m     0.247      1 y          1\n5     5      198754 BANANA C     2022-03-02_16h… m     0.290      1 y          1\n6     6      198754 BASKET HB    2022-03-02_16h… m     0.527      1 y          1\n# ℹ 2 more variables: condition1 &lt;chr&gt;, isold &lt;dbl&gt;"
  },
  {
    "objectID": "Experiment2.html#contrast-code-1",
    "href": "Experiment2.html#contrast-code-1",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "6.9 Contrast code",
    "text": "6.9 Contrast code\n\n\nCode\n## Contrasts\n#hypothesis\nblurC &lt;-hypr(HB~C, HB~LB, levels=c(\"C\", \"HB\", \"LB\"))\nblurC\n\n#set contrasts in df \nmem_nc$blur &lt;- as.factor(mem_nc$blur)\n\ncontrasts(mem_nc$blur) &lt;-contr.hypothesis(blurC)\n\nmem_nc$isold &lt;- ifelse(mem_nc$isold==\"0\", \"new\", \"old\")\n\nisold &lt;- hypr(new~old, levels=c(\"new\", \"old\"))\n\nmem_nc$isold &lt;- as.factor(mem_nc$isold)\n\ncontrasts(mem_nc$isold) &lt;- contr.hypothesis(mem_nc$isold)"
  },
  {
    "objectID": "Experiment2.html#brm-model",
    "href": "Experiment2.html#brm-model",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "6.10 BRM Model",
    "text": "6.10 BRM Model\n\n\nCode\nprior_exp2 &lt;- c(set_prior(\"cauchy(0,.35)\", class = \"b\"))\n\nfit_mem_noc &lt;- brm(sayold ~ isold*blur + (1+isold*blur|participant) + (1+isold*blur|string), data=mem_nc, \nwarmup = 1000,\n                    iter = 5000,\n                    chains = 4, \n                    init=0, \n                    family = bernoulli(link = \"probit\"),\n                    cores = 4, \ncontrol = list(adapt_delta = 0.9),\nprior=prior_exp2, \nsample_prior = T, \nsave_pars = save_pars(all=T),\nbackend=\"cmdstanr\", \nthreads = threading(4))\n\n\n\n6.10.1 D’, C, and Differences\n\n\nCode\nfit_mem_noc &lt;- read_rds(\"https://osf.io/2pgnm/download\")\n\n#get the lowblur vs. c conrtast\nfit_mem_lbc &lt;- read_rds(\"https://osf.io/tucn9/download\")\n\n\n\n\nCode\n# (Negative) criteria\nemm_m1_c1 &lt;- emmeans(fit_mem_noc, ~blur) %&gt;%\n    parameters::parameters(centrality = \"mean\")\n\n  \nemm_m1_c2 &lt;- emmeans(fit_mem_noc, ~blur) %&gt;% \n  contrast(\"pairwise\") %&gt;%\n    parameters::parameters(centrality = \"mean\")\n\n# Dprimes for three groups\nemm_m1_d1 &lt;- emmeans(fit_mem_noc, ~isold + blur) %&gt;% \n  contrast(\"revpairwise\", by = \"blur\") %&gt;%\n    parameters::parameters(centrality = \"mean\")\n\n# Differences between groups\nemm_m1_d2 &lt;- emmeans(fit_mem_noc, ~isold + blur) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\")) %&gt;%\n    parameters::parameters(centrality = \"mean\")\n\nreduce(list(emm_m1_c1, emm_m1_c2, emm_m1_d1, emm_m1_d2), bind_rows) %&gt;% \n  select(c(1:2, 4:5)) %&gt;%\n  gt()\n\n\n\n\n\n\n  \n    \n    \n      Parameter\n      Mean\n      CI_low\n      CI_high\n    \n  \n  \n    C\n0.0410\n-0.03885\n0.11969\n    HB\n0.1054\n0.02843\n0.17981\n    LB\n0.0592\n-0.01939\n0.13462\n    C - HB\n-0.0644\n-0.10654\n-0.02390\n    C - LB\n-0.0182\n-0.06085\n0.02392\n    HB - LB\n0.0462\n0.00705\n0.08605\n    old - new, C\n1.3470\n1.22228\n1.47288\n    old - new, HB\n1.4210\n1.29320\n1.54758\n    old - new, LB\n1.3030\n1.18136\n1.42704\n    old - new, C - HB\n-0.0740\n-0.15070\n0.00316\n    old - new, C - LB\n0.0439\n-0.03559\n0.12338\n    old - new, HB - LB\n0.1180\n0.04178\n0.19658\n  \n  \n  \n\n\n\n\n\n\nCode\nemm_m1_c1 &lt;- emmeans(fit_mem_noc, ~blur) \n\n  \nemm_m1_c2 &lt;- emmeans(fit_mem_noc, ~blur) %&gt;% \n  contrast(\"pairwise\")\n\n# Dprimes for three groups\nemm_m1_d1 &lt;- emmeans(fit_mem_noc, ~isold + blur) %&gt;% \n  contrast(\"revpairwise\", by = \"blur\")\n# Differences between groups\nemm_m1_d2 &lt;- emmeans(fit_mem_noc, ~isold + blur) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\")) \n\n\ntmp &lt;- bind_rows(\n  bind_rows(\n    gather_emmeans_draws(emm_m1_d1) %&gt;% \n      group_by(blur) %&gt;% \n      select(-contrast),\n    gather_emmeans_draws(emm_m1_d2) %&gt;% \n      rename(\n        blur = blur_pairwise\n      ) %&gt;% \n      group_by(blur) %&gt;% \n      select(-isold_revpairwise)\n  ),\n  bind_rows(\n    gather_emmeans_draws(emm_m1_c1),\n    gather_emmeans_draws(emm_m1_c2) %&gt;% \n      rename(\n        blur = contrast\n      )\n  ),\n  .id = \"Parameter\"\n) %&gt;% \n  mutate(Parameter = factor(Parameter, labels = c(\"dprime\", \"Criterion\"))) %&gt;% \n  mutate(\n    t = if_else(str_detect(blur, \" - \"), \"Differences\", \"Group means\") %&gt;% \n      fct_inorder(),\n    blur = fct_inorder(blur)\n  )\n\ntmp %&gt;%   \n  ungroup() %&gt;% \n  mutate(.value = if_else(Parameter == \"Criterion\", .value * -1, .value)) %&gt;% \n  mutate(Parameter = fct_rev(Parameter)) %&gt;% \n  ggplot(aes(blur, .value)) +\n  labs(\n    x = \"Blurring Level (or difference)\",\n    y = \"Parameter value\"\n  ) +\n   stat_halfeye(colour=\"blue\") + \n    facet_grid(Parameter~t, scales = \"free\") + \n  \n  geom_hline(yintercept = 0, linewidth = .25) + \n  theme_bw(base_size = 16)\n\n\n\n\n\nPosterior distributions and 95%CIs of the criterion and dprime parameters, or differences therein, from the conditionalized model\n\n\n\n\n\n\nCode\na = hypothesis(fit_mem_noc , \"isold1:blur1 &gt; 0\")\nb= hypothesis(fit_mem_noc , \"isold1:blur2 &gt; 0\")\nc= hypothesis(fit_mem_lbc, \"isold1:blur1 = 0\")\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis) %&gt;%\n    mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\n\ntab %&gt;% \n  gt(caption=md(\"Table: Memory Sensitvity Directional Hypotheses Experiment 2\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Memory Sensitvity Directional Hypotheses Experiment 2\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n    \n  \n  \n    (isold1:blur1) &gt; 0\n0.074\n0.039\n0.009\n0.139\n32.68\n0.970\n    (isold1:blur2) &gt; 0\n0.118\n0.040\n0.053\n0.183\n550.72\n0.998\n    (isold1:blur1) = 0\n-0.040\n0.039\n-0.116\n0.038\n1.79\n0.642\n  \n  \n  \n\n\n\n\n\n\n6.10.2 Write-up\n\n\n6.10.3 Sensitivity\nHigh blur words were better remembered than clear words, $\\beta$ = 0.074, 95% Cr.I[0.009, 0.139], 32.684, and low blur words, \\(\\beta\\) = 0.118, 95% Cr.I[0.053, 0.183], (isold1:blur2) &gt; 0, 0.118, 0.04, 0.053, 0.183, 550.724, 0.998, *$\\beta$esis$Evid.Ratio. There was weak evidence for no difference between clear and low blurred words, \\(\\beta\\) = -0.04, 95% Cr.I[-0.116, 0.038], ER = 1.793"
  },
  {
    "objectID": "Experiment2.html#discussion",
    "href": "Experiment2.html#discussion",
    "title": "4  Experiment 1b: No Context Reinstatement",
    "section": "6.11 Discussion",
    "text": "6.11 Discussion\nOur results replicate Experiment 1a with context not reinstated during test. Specifically, during encoding, high blurred words shifted the RT distribution, produced greater skewing, had lower drift rate \\(v\\), and higher non-decision time \\(T_{er}\\). For low blurred words, one difference worth mentioning is that there seems to be increasing differences (although much smaller) compared to clear words. Looking at the quantile plots we do see a small increase at the trailing edge of the distribution that could explain this.\nCritically, during the test phase, high blurred words better recognition performance than clear and low blurred words.\n\n\n\n\n\n\nPeirce, J., Gray, J. R., Simpson, S., MacAskill, M., Höchenberger, R., Sogo, H., Kastman, E., & Lindeløv, J. K. (2019). PsychoPy2: Experiments in behavior made easy. Behavior Research Methods, 51(1), 195–203. https://doi.org/10.3758/s13428-018-01193-y"
  },
  {
    "objectID": "Experiment3.html",
    "href": "Experiment3.html",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "",
    "text": "6 WF x Blur: LDT\nExperiment 2 explored the source of the late stage processing underlying the disfluency effect. Using a word frequency manipulation coupled with a semantic categorization task, we discovered non-additive effects of frequency and blurring on response time distributions. Specifically, the word frequency effect was magnified for high blurred words(compared to clear and low blurred words). We observed this on \\(\\mu\\) and $b$, indicating that when stimuli are degraded, word frequency influences early and late stages of processing during word recognition. This pattern has also been found with other disfluent stimuli, such as hard-to-read handwritten cursive words (Barnhart & Goldinger, 2010; Perea et al., 2016; Vergara-Martínez et al., 2021).\nLooking at quantile and delta plots, we see there was a robust word-frequency advantage that increased in the higher quantiles for clear and low-blurred words at the 0.1, 0.3, 0.5, 0.7, and 0.9 quantiles, respectively). Critically, for the high blurred condition, the word-frequency effect not only changed across quantiles with a steeper slope, but it was also larger in the first quantiles. This finding suggests that word-frequency already taps onto an encoding stage of processing when the stimuli appear in hard-to-read format like high blurred words.This replicates earlier research with easy-to-read and hard-to-read handwriting (Perea et al., 2016; Vergara-Martínez et al., 2021).\nCritical here is how non-additivity observed here impacts memory. Replicating Experiments 1a and 1b, we observed an overall memory benfit for high blurred words. Additionally, we observed better recognition memory for low frequency words compared to high frequency words. Examining the interaction between blurring and frequency revealed a distinct pattern. We observed a disfluency effect for high frequency-high blurred words. However, low frequency words, regardless of blurring level, appeared to have similar memory scores (Cr.I included 0 for each comparison). This pattern of findings helps us shed some light on the potential source of late stage processing in the disfluency effect.\nThe compensatory processing account (Mulligan, 1996) posits that memory performance depends on the depth of stimulus encoding, with items undergoing the most top-down processing yielding the biggest memory benefit. Contrary to this, we did not observe superior memory for low frequency-high blurred words. In fact, low frequency words did not seem to show a strong disfluency effect for any of the comparisons. Interestingly, there seemed to be a reversal in the effect, with clear words having higher sensitivity than low blurred words. Despite this, we did observe a disfluency effect for high frequency words. The theoretical implications of this is discussed in the General Discussion."
  },
  {
    "objectID": "Experiment3.html#set-up",
    "href": "Experiment3.html#set-up",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.1 Set-up",
    "text": "5.1 Set-up\nBelow are the packages you should install to ensure this document runs properly.\n\n\nCode\n#load packages\nlibrary(plyr)\nlibrary(easystats)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(ggeffects)\nlibrary(here)\nlibrary(data.table)\nlibrary(ggrepel)\nlibrary(gt)\nlibrary(brms)\nlibrary(ggdist)\nlibrary(emmeans)\nlibrary(tidylog)\nlibrary(tidybayes)\nlibrary(hypr)\nlibrary(cowplot)\nlibrary(tidyverse)\nlibrary(colorspace)\nlibrary(ragg)\nlibrary(cowplot)\nlibrary(ggtext)\nlibrary(MetBrewer)\nlibrary(ggdist)\nlibrary(modelbased)\nlibrary(flextable)\nlibrary(cmdstanr)\nlibrary(brms)\nlibrary(Rfssa)\nlibrary(easystats)\nlibrary(knitr)\n\noptions(digits = 3)\noptions(timeout=200)\n\noptions(set.seed(666))"
  },
  {
    "objectID": "Experiment3.html#figure-theme",
    "href": "Experiment3.html#figure-theme",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.2 Figure Theme",
    "text": "5.2 Figure Theme\n\n\nCode\nbold &lt;- element_text(face = \"bold\", color = \"black\", size = 16) #axis bold\ntheme_set(theme_minimal(base_size = 15, base_family = \"Arial\"))\ntheme_update(\n  panel.grid.major = element_line(color = \"grey92\", size = .4),\n  panel.grid.minor = element_blank(),\n  axis.title.x = element_text(color = \"grey30\", margin = margin(t = 7)),\n  axis.title.y = element_text(color = \"grey30\", margin = margin(r = 7)),\n  axis.text = element_text(color = \"grey50\"),\n  axis.ticks =  element_line(color = \"grey92\", size = .4),\n  axis.ticks.length = unit(.6, \"lines\"),\n  legend.position = \"top\",\n  plot.title = element_text(hjust = 0, color = \"black\", \n                            family = \"Arial\",\n                            size = 21, margin = margin(t = 10, b = 35)),\n  plot.subtitle = element_text(hjust = 0, face = \"bold\", color = \"grey30\",\n                               family = \"Arial\", \n                               size = 14, margin = margin(0, 0, 25, 0)),\n  plot.title.position = \"plot\",\n  plot.caption = element_text(color = \"grey50\", size = 10, hjust = 1,\n                              family = \"Arial\", \n                              lineheight = 1.05, margin = margin(30, 0, 0, 0)),\n  plot.caption.position = \"plot\", \n  plot.margin = margin(rep(20, 4))\n)\npal &lt;- c(met.brewer(\"Veronese\", 3))\n\n\n\n\nCode\n## flat violinplots\n### It relies largely on code previously written by David Robinson \n### (https://gist.github.com/dgrtwo/eb7750e74997891d7c20) and ggplot2 by H Wickham\n#check if required packages are installed\n#Load packages\n# Defining the geom_flat_violin function. Note: the below code modifies the \n# existing github page by removing a parenthesis in line 50\n\ngeom_flat_violin &lt;- function(mapping = NULL, data = NULL, stat = \"ydensity\",\n                             position = \"dodge\", trim = TRUE, scale = \"area\",\n                             show.legend = NA, inherit.aes = TRUE, ...) {\n  layer(\n    data = data,\n    mapping = mapping,\n    stat = stat,\n    geom = GeomFlatViolin,\n    position = position,\n    show.legend = show.legend,\n    inherit.aes = inherit.aes,\n    params = list(\n      trim = trim,\n      scale = scale,\n      ...\n    )\n  )\n}\n# horizontal nudge position adjustment\n# copied from https://github.com/tidyverse/ggplot2/issues/2733\nposition_hnudge &lt;- function(x = 0) {\n  ggproto(NULL, PositionHNudge, x = x)\n}\nPositionHNudge &lt;- ggproto(\"PositionHNudge\", Position,\n                          x = 0,\n                          required_aes = \"x\",\n                          setup_params = function(self, data) {\n                            list(x = self$x)\n                          },\n                          compute_layer = function(data, params, panel) {\n                            transform_position(data, function(x) x + params$x)\n                          }\n)"
  },
  {
    "objectID": "Experiment3.html#method",
    "href": "Experiment3.html#method",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.3 Method",
    "text": "5.3 Method\nThis study was preregistered https://osf.io/kjq3t. All raw and summary data, materials, and R scripts for pre-processing, analysis, and plotting for Experiment 3 can be found at our OSF page: https://osf.io/6sy7k/."
  },
  {
    "objectID": "Experiment3.html#participants",
    "href": "Experiment3.html#participants",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.4 Participants",
    "text": "5.4 Participants"
  },
  {
    "objectID": "Experiment3.html#materials",
    "href": "Experiment3.html#materials",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.5 Materials",
    "text": "5.5 Materials\nNon-animal and animal words were adapted from Fernández-López et al. (2022). To make the experiment more feasible for online participants and to evenly split our conditions, we winnowed their non-animal words and presented 90 (1/2 HF and 1/2 LF) non-animal words and 45 animal words during study. This kept the 2:1 ratio used in previous experiments (e.g. (Fernández-López et al., 2022; Perea et al., 2018). At test, 90 non-animal words we did not use during the semantic categorization task were used as new words for the recognition test. We created six counterbalanced lists to ensure that each non-animal word was presented as both old and new and as clear, high blurred, and low blurred across participants. Similar to non-words from Experiments 1a and 1b, we excluded animal words from analysis.\nThe number of letters of the animal words (M = 5.3; range: 3-9) was similar to that of the non-animal words (high-frequency words: M = 5.3, range: 3-8; low-frequency words: M = 5.3, range: 3-9). The animal words had an ample range of word-frequency in the SUBTLEX database (M = 11.84 per million; range: 0.61-192.84)."
  },
  {
    "objectID": "Experiment3.html#procedure",
    "href": "Experiment3.html#procedure",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.6 Procedure",
    "text": "5.6 Procedure\nWe used the same procedure as Experiments 1b. The main difference is that instead of making a word/non-word decision, participants made a semantic categorization judgement (i.e., animal/not animal)."
  },
  {
    "objectID": "Experiment3.html#results",
    "href": "Experiment3.html#results",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.7 Results",
    "text": "5.7 Results\n\n5.7.1 Accuracy\n\n\nCode\nrts_wf &lt;- read_csv(\"https://osf.io/29hnd/download\")\n\nhead(rts_wf)\n\n\n# A tibble: 6 × 15\n   ...1 participant      date    age target study blur  frequency category    rt\n  &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;\n1     1 54847f1cfdf99b0… 2023…    50 SWEEP  old   C     LOW       NONAN     3.29\n2     2 54847f1cfdf99b0… 2023…    50 STATUE old   HB    LOW       NONAN     4.84\n3     3 54847f1cfdf99b0… 2023…    50 WAITR… old   LB    LOW       NONAN     2.44\n4     4 54847f1cfdf99b0… 2023…    50 TRUE   old   HB    HIGH      NONAN     2.43\n5     5 54847f1cfdf99b0… 2023…    50 START  old   HB    HIGH      NONAN     2.43\n6     6 54847f1cfdf99b0… 2023…    50 PLEAD  old   LB    LOW       NONAN     1.74\n# ℹ 5 more variables: corr &lt;dbl&gt;, List &lt;dbl&gt;, bad_1 &lt;chr&gt;, bad_2 &lt;chr&gt;,\n#   bad_3 &lt;chr&gt;"
  },
  {
    "objectID": "Experiment3.html#brms",
    "href": "Experiment3.html#brms",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.8 BRMs",
    "text": "5.8 BRMs\n\n5.8.1 Accuracy\n\n\nCode\nrts_dim &lt;- rts_wf %&gt;%\n  filter(category==\"NONAN\")\n\nblur_acc_wf&lt;- rts_wf %&gt;%\n  group_by(participant) %&gt;%\n  dplyr::filter(rt &gt;= .2 & rt &lt;= 2.5)%&gt;%\n  dplyr::filter(category==\"NONAN\")\n\nhead(blur_acc_wf)\n\n\n# A tibble: 6 × 15\n# Groups:   participant [1]\n   ...1 participant      date    age target study blur  frequency category    rt\n  &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;\n1     3 54847f1cfdf99b0… 2023…    50 WAITR… old   LB    LOW       NONAN     2.44\n2     4 54847f1cfdf99b0… 2023…    50 TRUE   old   HB    HIGH      NONAN     2.43\n3     5 54847f1cfdf99b0… 2023…    50 START  old   HB    HIGH      NONAN     2.43\n4     6 54847f1cfdf99b0… 2023…    50 PLEAD  old   LB    LOW       NONAN     1.74\n5     7 54847f1cfdf99b0… 2023…    50 FOREV… old   C     HIGH      NONAN     1.64\n6     9 54847f1cfdf99b0… 2023…    50 SNEEZE old   C     LOW       NONAN     1.63\n# ℹ 5 more variables: corr &lt;dbl&gt;, List &lt;dbl&gt;, bad_1 &lt;chr&gt;, bad_2 &lt;chr&gt;,\n#   bad_3 &lt;chr&gt;\n\n\nCode\ndim(blur_acc_wf)\n\n\n[1] 38526    15\n\n\nCode\ndim(rts_dim)\n\n\n[1] 38880    15\n\n\nWe started with 38880. After we removed RTs below .2 and above 2.5 (0.009)we were left with 38526 data points.\n\n\nCode\n## Contrasts\n#hypothesis\nblurC &lt;-hypr(HB~C, HB~LB, levels=c(\"C\", \"HB\", \"LB\"))\nblurC\n\n\nhypr object containing 2 null hypotheses:\nH0.1: 0 = HB - C\nH0.2: 0 = HB - LB\n\nCall:\nhypr(~HB - C, ~HB - LB, levels = c(\"C\", \"HB\", \"LB\"))\n\nHypothesis matrix (transposed):\n   [,1] [,2]\nC  -1    0  \nHB  1    1  \nLB  0   -1  \n\nContrast matrix:\n   [,1] [,2]\nC  -2/3  1/3\nHB  1/3  1/3\nLB  1/3 -2/3\n\n\nCode\n#set contrasts in df \nblur_acc_wf$blur &lt;- as.factor(blur_acc_wf$blur)\n\ncontrasts(blur_acc_wf$blur) &lt;-contr.hypothesis(blurC)\n\n\nfreqc &lt;- hypr(HIGH~LOW,levels=c(\"HIGH\", \"LOW\"))\nfreqc\n\n\nhypr object containing one (1) null hypothesis:\nH0.1: 0 = HIGH - LOW\n\nCall:\nhypr(~HIGH - LOW, levels = c(\"HIGH\", \"LOW\"))\n\nHypothesis matrix (transposed):\n     [,1]\nHIGH  1  \nLOW  -1  \n\nContrast matrix:\n     [,1]\nHIGH  1/2\nLOW  -1/2\n\n\nCode\nblur_acc_wf$frequency&lt;- as.factor(blur_acc_wf$frequency)\n\ncontrasts(blur_acc_wf$frequency) &lt;-contr.hypothesis(freqc)\n\n\n\n\n5.8.2 Model\n\n\nCode\nprior_expsc &lt;- c(set_prior(\"cauchy(0,.35)\", class = \"b\"))\n\nfit_acc_wf &lt;- brm(corr ~ blur*frequency + (1+blur*frequency|participant) + (1+blur*frequency|target), data=blur_acc_wf, \nwarmup = 1000,\n                    iter = 5000,\n                    chains = 4, \n                    init=0, \n                    family = bernoulli(),\n     cores = 4, \n\nprior=prior_expsc,\nsample_prior = T, \nsave_pars = save_pars(all=T),\ncontrol = list(adapt_delta = 0.9), \nfile=\"acc_blmm_sc\", \nbackend=\"cmdstanr\", \nthreads = threading(4))\n\n\n\n\nCode\n# get file from osf\ntmp &lt;- tempdir()\ndownload.file(\"https://osf.io/5u7p8/download\", file.path(tmp, \"acc_blmm_sc.RData\"))\nload(file.path(tmp, \"acc_blmm_sc.RData\"))\n\nfit_acc_sc_lb &lt;- read_rds(\"https://osf.io/ehjxq/download\")"
  },
  {
    "objectID": "Experiment3.html#model-summary",
    "href": "Experiment3.html#model-summary",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.9 Model Summary",
    "text": "5.9 Model Summary\n\n5.9.1 Hypotheses\n\n\nCode\n# Dprimes for three groups\nemm_acc &lt;- emmeans(fit_acc_sc, ~frequency + blur, type=\"response\") %&gt;% \n  parameters::parameters(centrality = \"mean\")\n\n\nemm_acc\n\n\nParameter | Mean | Mean.1 |       95% CI |   pd\n-----------------------------------------------\nHIGH, C   | 1.00 |   1.00 | [1.00, 1.00] | 100%\nLOW, C    | 1.00 |   1.00 | [1.00, 1.00] | 100%\nHIGH, HB  | 0.99 |   0.99 | [0.99, 0.99] | 100%\nLOW, HB   | 0.99 |   0.99 | [0.99, 0.99] | 100%\nHIGH, LB  | 1.00 |   1.00 | [0.99, 1.00] | 100%\nLOW, LB   | 1.00 |   1.00 | [0.99, 1.00] | 100%\n\n\n\n\nCode\na = hypothesis(fit_acc_sc , \"blur1 &lt; 0\")\n\nb = hypothesis(fit_acc_sc , \"blur2 &lt; 0\")\n\nc = hypothesis(fit_acc_sc_lb, \"blur1 &lt; 0\")\n\nd= hypothesis(fit_acc_sc, \"frequency1 = 0\")\n\ne = hypothesis(fit_acc_sc, \"blur1:frequency1 = 0\")\n\nf = hypothesis(fit_acc_sc , \"blur2:frequency1 = 0\")\n\ng = hypothesis(fit_acc_sc_lb, \"blur1:frequency1 = 0\")\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis, g$hypothesis) %&gt;%\n  mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\ntab %&gt;% \n   mutate(Hypothesis = c(\"High Blur - Clear &lt; 0\", \"High Blur-Low Blur &lt; 0\", \"Low Blur - Clear = 0\",\"Low Frequency - High Frequency\",  \"(High Blur-Clear) - (Low Frequency-High Frequency) &lt; 0\", \"(High Blur-Low Blur) - (Low Frequency-High Frequency) &lt; 0\", \"(Low Blur-Clear) - (Low Frequency-High Frequency) =  0\")) %&gt;% \n  gt(caption=md(\"Table: Experiment 3 Accuracy\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Experiment 3 Accuracy\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n    \n  \n  \n    High Blur - Clear &lt; 0\n-1.128\n0.271\n-1.590\n-0.689\nInf\n1.000\n    High Blur-Low Blur &lt; 0\n-0.764\n0.238\n-1.154\n-0.374\n1999.000\n1.000\n    Low Blur - Clear = 0\n-0.072\n0.123\n-0.273\n0.130\n2.638\n0.725\n    Low Frequency - High Frequency\n0.132\n0.195\n-0.235\n0.528\n1.166\n0.538\n    (High Blur-Clear) - (Low Frequency-High Frequency) &lt; 0\n0.017\n0.242\n-0.462\n0.514\n1.156\n0.536\n    (High Blur-Low Blur) - (Low Frequency-High Frequency) &lt; 0\n-0.002\n0.234\n-0.466\n0.463\n1.236\n0.553\n    (Low Blur-Clear) - (Low Frequency-High Frequency) =  0\n0.088\n0.215\n-0.337\n0.508\n0.597\n0.374\n  \n  \n  \n\n\n\n\n\n\n5.9.2 Accuracy Summary\nThere was no frequency effect, $b$ = 0.132, 95% Cr.I[-0.235, 0.528], ER = , although the evidence for a lack of a difference is ambiguous.\nTurning to blurring, clear words were better identified than high blur words (\\(M\\) = .963), $b$ = -1.128, 95% Cr.I[-1.59, -0.689], ER = . Low blurred words were better identified than high blurred words, $b$ = -0.764, 95% Cr.I[-1.154, -0.374], ER = 1999. There was weak evidence for a difference between clear and low blurred words, $b$ = -0.072, 95% Cr.I[-0.273, 0.13], ER = 2.638.\nThere were no interactions between blurring and word frequency–all 95% Cr.I included 0; however, evidence for this lack of difference was ambiguous (ER = ~1)."
  },
  {
    "objectID": "Experiment3.html#rts",
    "href": "Experiment3.html#rts",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.10 RTs",
    "text": "5.10 RTs\n\n\nCode\np_rt_filter &lt;- rts_wf %&gt;%\n  filter(corr==1, category==\"NONAN\")\n\n\np_rt_out &lt;- p_rt_filter %&gt;% \n filter(rt &gt;= .2 & rt &lt;= 2.5) %&gt;%\n  ungroup()\n\np_rt &lt;- p_rt_filter %&gt;%\n   filter(rt &gt;= .2 & rt &lt;= 2.5) %&gt;%\n  group_by(frequency, blur) %&gt;%\n  dplyr::summarise(rt=mean(rt)) %&gt;%\n  dplyr::mutate(rt_ms=rt*1000) %&gt;%\n  select(-rt)\n\n# table for the effect\np_rt %&gt;% group_by(blur) %&gt;%\n  pivot_wider(names_from=frequency, values_from=rt_ms) %&gt;%\n  mutate(Freq_Effect=round(LOW-HIGH)) %&gt;%\n  flextable()  \n\n\n\nblurHIGHLOWFreq_EffectC70572318HB9611,01655LB71873013\n\n\nWe had 38152 correct RT trials for non-animal responses. After removing RTs below .2 and above 2.5 we were left with 37823 trials.\n\n\nCode\n## Contrasts\n#hypothesis\n#set contrasts in df \np_rt_filter$blur &lt;- as.factor(p_rt_filter$blur)\n\ncontrasts(p_rt_filter$blur) &lt;-contr.hypothesis(blurC)\n\n\nfreqc &lt;- hypr(HIGH~LOW,levels=c(\"HIGH\", \"LOW\"))\nfreqc\n\n\nhypr object containing one (1) null hypothesis:\nH0.1: 0 = HIGH - LOW\n\nCall:\nhypr(~HIGH - LOW, levels = c(\"HIGH\", \"LOW\"))\n\nHypothesis matrix (transposed):\n     [,1]\nHIGH  1  \nLOW  -1  \n\nContrast matrix:\n     [,1]\nHIGH  1/2\nLOW  -1/2\n\n\nCode\np_rt_filter$frequency&lt;- as.factor(p_rt_filter$frequency)\n\ncontrasts(p_rt_filter$frequency) &lt;-contr.hypothesis(freqc)\n\n\n\n5.10.1 Ex-Gaussian\n\n5.10.1.1 Model Set-up\n\n\nCode\nlibrary(cmdstanr)\n#max model\nbform_exg1 &lt;- bf(\nrt ~ blur*frequency + (1 + blur*frequency |p| participant) + (1 + blur|i| Target),\nsigma ~ blur*frequency + (1 + blur*frequency |p|participant) + (1 + blur |i| Target),\nbeta ~ blur*frequency  + (1 + blur*frequency |p|participant) + (1 + blur |i| Target))\n\n\n\n\n5.10.1.2 Run model\n\n\nCode\n#|\nprior_exp1 &lt;- c(set_prior(\"normal(0,100)\", class = \"b\", coef=\"\"))\n\nfit_exg1 &lt;- brm(\nbform_exg1, data = blur_rt_sc,\nwarmup = 1000,\n                    iter = 5000,\n                    chains = 4,\n                    family = exgaussian(),\n                    init = 0,\n                    cores = 4,\ncontrol = list(adapt_delta = 0.8), \nbackend=\"cmdstanr\", \nfile = \"blmm_sc_wf\",\nthreads = threading(4))\n\n\n\n\nCode\nfit_sc &lt;- read_rds(\"https://osf.io/kdv38/download\")\nfit_sc_lc &lt;- read_rds(\"https://osf.io/49bgx/download\")"
  },
  {
    "objectID": "Experiment3.html#model-summary-1",
    "href": "Experiment3.html#model-summary-1",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.11 Model summary",
    "text": "5.11 Model summary\n\n\nCode\na = hypothesis(fit_sc , \"blur1 &gt; 0\", dpar=\"mu\")\n\nb = hypothesis(fit_sc , \"blur2 &gt;  0\", dpar=\"mu\")\n\nc = hypothesis(fit_sc_lc, \"blur1 = 0\", dpar=\"mu\")\n\nd= hypothesis(fit_sc, \"frequency1 &gt; 0\", dpar=\"mu\")\n\ne = hypothesis(fit_sc, \"blur1:frequency1 &lt; 0 \", dpar=\"mu\")\n\nf = hypothesis(fit_sc , \"blur2:frequency1 &lt; 0 \", dpar=\"mu\")\n\ng = hypothesis(fit_sc_lc, \"blur1:frequency1 = 0\", dpar=\"mu\")\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis, g$hypothesis) %&gt;%\n  mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\ntab %&gt;% \n   mutate(Hypothesis = c(\"High Blur - Clear &gt; 0\", \"High Blur-Low Blur &gt; 0\", \"Low Blur - Clear = 0\",\"Low Frequency - High Frequency\",  \"(High Blur-Clear) - (Low Frequency-High Frequency) &lt; 0\", \"(High Blur-Low Blur) - (Low Frequency-High Frequency) &lt; 0\", \"(Low Blur-Clear) - (Low Frequency-High Frequency) =  0\")) %&gt;% \n  gt(caption=md(\"Table: Experiment 3 Memory Mu\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Experiment 3 Memory Mu\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n    \n  \n  \n    High Blur - Clear &gt; 0\n0.274\n0.007\n0.262\n0.286\nInf\n1.000\n    High Blur-Low Blur &gt; 0\n0.265\n0.007\n0.253\n0.277\nInf\n1.000\n    Low Blur - Clear = 0\n0.002\n0.006\n-0.010\n0.015\n1500\n0.999\n    Low Frequency - High Frequency\n-0.026\n0.006\n-0.035\n-0.016\n0\n0.000\n    (High Blur-Clear) - (Low Frequency-High Frequency) &lt; 0\n-0.029\n0.012\n-0.050\n-0.009\n96\n0.990\n    (High Blur-Low Blur) - (Low Frequency-High Frequency) &lt; 0\n-0.032\n0.012\n-0.052\n-0.012\n181\n0.995\n    (Low Blur-Clear) - (Low Frequency-High Frequency) =  0\n-0.017\n0.009\n-0.036\n0.000\n170\n0.994\n  \n  \n  \n\n\n\n\n\n5.11.1 Mu\nHigh blurred words had greater shifting than clear words, $b$ = 0.274, 95% Cr.I[0.262, 0.286], ER = , and low blurred words, $b$ = 0.265, 95% Cr.I[0.253, 0.277], ER = . There was no difference in amount of shifting between low blurred words and clear words, $b$ = 0.002, 95% Cr.I[-0.01, 0.015], ER = 1500.042. For word frequency, there was greater shifting for low frequency compared to high frequency words, $b$ = -0.026, 95% Cr.I[-0.035, -0.016], ER = 0. In terms of the interaction between frequency and blurring, there was an amplified word frequency effect for high blurred words compared to clear words, $b$ = -0.029, 95% Cr.I[-0.05, -0.009], ER = 95.97 and low blurred words, $b$ = -0.032, 95% Cr.I[-0.052, -0.012], ER = 180.818. There was strong evidence that there was no amplification of the word frequency effect for the low blurred vs. clear comparison, $b$ = -0.017, 95% Cr.I[-0.036, -2.118^{-4}], ER = 170.388.\n\n\nCode\na = hypothesis(fit_sc , \"sigma_blur1 &gt; 0\")\n\nb = hypothesis(fit_sc , \"sigma_blur2 &gt; 0\")\n\nc = hypothesis(fit_sc_lc, \"sigma_blur1 &gt; 0\")\n\nd= hypothesis(fit_sc, \"sigma_frequency1 &lt; 0\")\n\ne = hypothesis(fit_sc, \"sigma_blur1:frequency1 &lt; 0\")\n\nf = hypothesis(fit_sc , \"sigma_blur2:frequency1 &gt; 0\")\n\ng = hypothesis(fit_sc_lc, \"sigma_blur1:frequency1 &gt; 0\")\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis, g$hypothesis) %&gt;%\n  mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\ntab %&gt;% \n   mutate(Hypothesis = c(\"High Blur - Clear &gt; 0\", \"High Blur-Low Blur &gt; 0\", \"Low Blur - Clear = 0\",\"Low Frequency - High Frequency\",  \"(High Blur-Clear) - (Low Frequency-High Frequency) &lt; 0\", \"(High Blur-Low Blur) - (Low Frequency-High Frequency) &lt; 0\", \"(Low Blur-Clear) - (Low Frequency-High Frequency) =  0\")) %&gt;% \n  gt(caption=md(\"Table: Experiment 3 Ex-Gaussian Sigma\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Experiment 3 Ex-Gaussian Sigma\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n    \n  \n  \n    High Blur - Clear &gt; 0\n0.688\n0.049\n0.608\n0.769\nInf\n1.000\n    High Blur-Low Blur &gt; 0\n0.707\n0.051\n0.623\n0.791\nInf\n1.000\n    Low Blur - Clear = 0\n0.012\n0.035\n-0.045\n0.070\n1.700\n0.630\n    Low Frequency - High Frequency\n-0.076\n0.037\n-0.136\n-0.016\n50.282\n0.981\n    (High Blur-Clear) - (Low Frequency-High Frequency) &lt; 0\n0.074\n0.082\n-0.060\n0.210\n0.222\n0.182\n    (High Blur-Low Blur) - (Low Frequency-High Frequency) &lt; 0\n0.119\n0.083\n-0.018\n0.256\n12.051\n0.923\n    (Low Blur-Clear) - (Low Frequency-High Frequency) =  0\n0.015\n0.064\n-0.089\n0.121\n1.462\n0.594\n  \n  \n  \n\n\n\n\n\n\n5.11.2 Sigma\nLow frequency words showed greater variance than high frequency words, $b$ = -0.076, 95% Cr.I[-0.136, -0.016], ER = 50.282.\nHigh blur words had higher \\(\\sigma\\) compared to clear, $b$ = 0.688, 95% Cr.I[0.608, 0.769], ER = , and low blurred words, $b$ = 0.707, 95% Cr.I[0.623, 0.791], ER = . There was weak evidence that low blurred words having greater variance than clear words, $b$ = 0.012, 95% Cr.I[-0.136, 0.07], ER = 1.7. There were no significant interactions–all Cr.I included 0.\n\n\nCode\na = hypothesis(fit_sc , \"beta_blur1 &gt; 0\", dpar=\"beta\")\n\nb = hypothesis(fit_sc , \"beta_blur2 &gt; 0\", dpar=\"beta\")\n\nc = hypothesis(fit_sc_lc, \"beta_blur1 &lt; 0\", dpar=\"beta\")\n\nd= hypothesis(fit_sc, \"beta_frequency1 &lt; 0\", dpar=\"beta\")\n\ne = hypothesis(fit_sc, \"beta_blur1:frequency1 &lt; 0\", dpar=\"beta\")\n\nf = hypothesis(fit_sc , \"beta_blur2:frequency1 &lt; 0\", dpar=\"beta\")\n\ng = hypothesis(fit_sc_lc, \"beta_blur1:frequency1 &lt; 0\", dpar=\"beta\")\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis, g$hypothesis) %&gt;% \n    mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\n\ntab %&gt;% \n  mutate(Hypothesis = c(\"High Blur - Clear &gt; 0\", \"High Blur-Low Blur &gt; 0\", \"Low Blur - Clear = 0\",\"Low Frequency - High Frequency\",  \"(High Blur-Clear) - (Low Frequency-High Frequency) &lt; 0\", \"(High Blur-Low Blur) - (Low Frequency-High Frequency) &lt; 0\", \"(Low Blur-Clear) - (Low Frequency-High Frequency) &lt;  0\")) %&gt;% \n  gt(caption=md(\"Table: Experiment 3 Ex-Gaussian Beta\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Experiment 3 Ex-Gaussian Beta\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n    \n  \n  \n    High Blur - Clear &gt; 0\n0.531\n0.027\n0.487\n0.575\nInf\n1.000\n    High Blur-Low Blur &gt; 0\n0.549\n0.027\n0.504\n0.593\nInf\n1.000\n    Low Blur - Clear = 0\n-0.038\n0.036\n-0.097\n0.020\n6.4\n0.865\n    Low Frequency - High Frequency\n-0.029\n0.021\n-0.062\n0.005\n11.0\n0.917\n    (High Blur-Clear) - (Low Frequency-High Frequency) &lt; 0\n-0.077\n0.042\n-0.146\n-0.008\n30.8\n0.969\n    (High Blur-Low Blur) - (Low Frequency-High Frequency) &lt; 0\n-0.143\n0.043\n-0.213\n-0.073\n2665.7\n1.000\n    (Low Blur-Clear) - (Low Frequency-High Frequency) &lt;  0\n-0.098\n0.050\n-0.184\n-0.022\n57.8\n0.983\n  \n  \n  \n\n\n\n\n\n\n5.11.3 Beta\nLow frequency words showed greater skewing than high frequency words, $b$ = -0.029, 95% Cr.I[-0.062, 0.005], ER = 11.012.\nHigh blurred words showed greater skewing than clear words, $b$ = 0.531, 95% Cr.I[0.487, 0.575], ER = , and low blurred words, $b$ = 0.549, 95% Cr.I[0.504, 0.593], ER = . There was strong evidence for no skewing difference between low blurred words and clear words, b = -0.038, 95% Cr.I[-0.097, 0.02], ER = 6.401.\nThe word frequency effect was magnified for high blurred words compared to clear, $b$ = -0.077, 95% Cr.I[-0.146, -0.008], ER = 30.809, and low blur words, $b$ = -0.143, 95% Cr.I[-0.213, -0.073], ER = 2665.667 There was also an interaction for the low blurred vs. clear words comparison, $b$ = -0.098, 95% Cr.I[-0.184, -0.022], ER = 57.824. However, the word frequency was reversed here with low blurred-high frequency words having greater skewing than low blurred-low frequency words.\n\n\n5.11.4 Ex-Gaussian conditional plots\n\n\nCode\np1&lt;-conditional_effects(fit_sc, terms=c(\"blur\",\"freq\"),  dpar = \"mu\")\np2&lt;-conditional_effects(fit_sc, \"blur\", dpar = \"sigma\")\np3&lt;-conditional_effects(fit_sc, terms=c(\"blur\",\"freq\"), dpar = \"beta\")\n\np1\n\n\n\n\n\n\n\n\n\n\n\nCode\np2\n\n\n\n\n\nCode\np3"
  },
  {
    "objectID": "Experiment3.html#quantile-plotsvincentiles",
    "href": "Experiment3.html#quantile-plotsvincentiles",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.12 Quantile Plots/Vincentiles",
    "text": "5.12 Quantile Plots/Vincentiles\n::: {.panel-tabset}\n\n5.12.1 Figure 1\n\n\nCode\n#Delta plots (one per subject)\nquibble &lt;- function(x, q = seq(.1, .9, .2)) {\n  tibble(x = quantile(x, q), q = q)\n}\n\ndata.quantiles &lt;- p_rt_out %&gt;%\n  dplyr::group_by(participant,blur,frequency, corr) %&gt;%\n  dplyr::mutate(rt_ms = rt*1000) %&gt;% \n  dplyr::summarise(RT = list(quibble(rt_ms, seq(.1, .9, .2)))) %&gt;% \n  tidyr::unnest(RT) %&gt;%\n  ungroup()\n\n\ndata.delta &lt;- data.quantiles %&gt;%\n  dplyr::group_by(participant, blur,frequency,  q) %&gt;%\n  dplyr::summarize(RT=mean(x)) %&gt;%\n  ungroup()\n\n\n\n\nCode\n#Delta plots (based on vincentiles)\nvincentiles &lt;- data.quantiles %&gt;%\n  dplyr::group_by(blur,frequency, q) %&gt;%\n  dplyr::summarize(RT=mean(x)) %&gt;%\n  ungroup()\n\nv=vincentiles %&gt;%\n  dplyr::group_by(blur,frequency, q) %&gt;%\n  dplyr::summarise(MRT=mean(RT)) %&gt;%\n  ungroup()\n\n\np &lt;- ggplot(v, aes(x = q, y = MRT, colour=blur)) +\n  facet_grid(~frequency) + \n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  scale_colour_manual(values=met.brewer(\"Cassatt2\", 3)) +\n  theme_bw() + \n  theme(axis.title = element_text(size = 16, face = \"bold\"), \n        axis.text = element_text(size = 16),\n        plot.title = element_text(face = \"bold\", size = 20)) +\n  scale_y_continuous(breaks=seq(500,1600,100)) +\n  theme(legend.title=element_blank())+\n    coord_cartesian(ylim = c(500, 1600)) +\n  scale_x_continuous(breaks=seq(.1,.9, .2))+\n    geom_label_repel(data=v, aes(x=q, y=MRT, label=round(MRT,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5)+\n  labs(title = \"Quantile Analysis\", x = \"Quantiles\", y = \"Response latencies in ms\")\n\np\n\n\n\n\n\nCode\np1 &lt;- ggplot(v, aes(x = q, y = MRT, colour=frequency)) +\n  facet_grid(~blur) + \n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  scale_colour_manual(values=met.brewer(\"Cassatt2\", 3)) +\n  theme_bw() + \n  theme(axis.title = element_text(size = 16, face = \"bold\"), \n        axis.text = element_text(size = 16),\n        plot.title = element_text(face = \"bold\", size = 20)) +\n  scale_y_continuous(breaks=seq(600,1600,100)) +\n  theme(legend.title=element_blank())+\n    coord_cartesian(ylim = c(600, 1600)) +\n  scale_x_continuous(breaks=seq(.1,.9, .2))+\n   geom_label_repel(data=v, aes(y=MRT, label=round(MRT,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5)+\n  labs(title = \"Quantile Analysis\", x = \"Quantiles\", y = \"Response latencies in ms\")\n\np1\n\n\n\n\n\n\n\n5.12.2 Figure 2\n\n\nCode\np2 &lt;- ggplot(data=v,aes(y=MRT, x=frequency, color=q)) + facet_grid(~blur)+\n  geom_line()+\n  geom_point(size=4) + \n  ggeasy::easy_add_legend_title(\"Quantiles\")\n\np2\n\n\n\n\n\n\n\n5.12.3 Delta Plots\n\n\nCode\n#diff\nv_wf &lt;- v %&gt;%\n  dplyr::group_by(blur, q)%&gt;%\n  tidyr::pivot_wider(names_from = \"frequency\", values_from = \"MRT\") %&gt;%\n  mutate(diff=LOW-HIGH) %&gt;%\n  ungroup()\n\nv_wf %&gt;% select(blur, q, diff) %&gt;% pivot_wider(names_from=\"q\", values_from=\"diff\") %&gt;% flextable()\n\n\n\nblur0.10.30.50.70.9C12.1017.218.619.225.1HB31.4641.848.767.298.4LB9.9212.112.013.617.7\n\n\n\n\nCode\nv_chb &lt;- v %&gt;%\n  dplyr::filter(blur==\"C\" | blur==\"HB\") %&gt;%\n  dplyr::group_by(frequency, q)%&gt;%\n  mutate(mean_rt = mean(MRT)) %&gt;% \n  ungroup()%&gt;% \n  tidyr::pivot_wider(names_from = \"blur\", values_from = \"MRT\") %&gt;%\n  mutate(diff=HB-C)\nv_chb\n\n\n# A tibble: 10 × 6\n   frequency     q mean_rt     C    HB  diff\n   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 HIGH        0.1    653.  575.  731.  157.\n 2 HIGH        0.3    722.  623.  821.  198.\n 3 HIGH        0.5    792.  669.  915.  247.\n 4 HIGH        0.7    881.  731. 1031.  300.\n 5 HIGH        0.9   1060.  868. 1253.  386.\n 6 LOW         0.1    675.  587.  763.  176.\n 7 LOW         0.3    751.  640.  863.  223.\n 8 LOW         0.5    826.  687.  964.  277.\n 9 LOW         0.7    925.  751. 1099.  348.\n10 LOW         0.9   1122.  893. 1352.  459.\n\n\nCode\nv_chb_freq &lt;- v %&gt;%\n  dplyr::group_by(blur, q)%&gt;%\n  mutate(mean_rt = mean(MRT)) %&gt;% \n  ungroup()%&gt;% \n  tidyr::pivot_wider(names_from = \"frequency\", values_from = \"MRT\") %&gt;%\n  mutate(diff=LOW-HIGH)\nv_chb_freq\n\n\n# A tibble: 15 × 6\n   blur      q mean_rt  HIGH   LOW  diff\n   &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 C       0.1    581.  575.  587. 12.1 \n 2 C       0.3    631.  623.  640. 17.2 \n 3 C       0.5    678.  669.  687. 18.6 \n 4 C       0.7    741.  731.  751. 19.2 \n 5 C       0.9    880.  868.  893. 25.1 \n 6 HB      0.1    747.  731.  763. 31.5 \n 7 HB      0.3    842.  821.  863. 41.8 \n 8 HB      0.5    940.  915.  964. 48.7 \n 9 HB      0.7   1065. 1031. 1099. 67.2 \n10 HB      0.9   1302. 1253. 1352. 98.4 \n11 LB      0.1    593.  588.  598.  9.92\n12 LB      0.3    641.  635.  647. 12.1 \n13 LB      0.5    688.  682.  694. 12.0 \n14 LB      0.7    750.  744.  757. 13.6 \n15 LB      0.9    896.  888.  905. 17.7 \n\n\nCode\np1 &lt;- ggplot(v_chb, aes(x = mean_rt, y = diff)) + facet_grid(~frequency) + \n  geom_abline(intercept = 0, slope = 0) +\n  geom_line(size = 1, colour = \"black\") +\n  geom_point(size = 3, colour = \"black\") +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  theme(axis.title = element_text(size = 16, face = \"bold\"), \n        axis.text = element_text(size = 16),\n        plot.title = element_text(face = \"bold\", size = 20)) +\nscale_y_continuous(breaks=seq(100,500,50)) +\n    coord_cartesian(ylim = c(100, 500)) +\n  scale_x_continuous(breaks=seq(600,1150, 100)) +\ngeom_label_repel(data=v_chb, aes(y=diff, label=round(diff,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5)  +\n  labs( title = \"Delta Plots: Freq x Blur\", x = \"Mean RTs per quantile\", y = \"High Blur - Clear Effect\")\n\np1\n\n\n\n\n\nCode\np2 &lt;- ggplot(v_chb_freq, aes(x = mean_rt, y = diff)) + facet_grid(~blur) + \n  geom_line(size = 1, colour = \"black\") +\n  geom_point(size = 3, colour = \"black\") +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  theme(axis.title = element_text(size = 16, face = \"bold\"), \n        axis.text = element_text(size = 16),\n        plot.title = element_text(face = \"bold\", size = 20)) +\nscale_y_continuous(breaks=seq(0,100,10)) +\n    coord_cartesian(ylim = c(0, 100)) +\n  scale_x_continuous(breaks=seq(600,1100, 100))+\n  geom_label_repel(data=v_chb_freq, aes(y=diff, label=round(diff,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5)+\n  labs( title = \"Delta Plots: Freq x Blur\", x = \"Mean RT per quantile\", y = \"Frequency Effect\")\n\np2\n\n\n\n\n\n\n\n5.12.4 Clear vs. Low Blur\n\n\nCode\nv_clb &lt;- v %&gt;%\n  dplyr::group_by(frequency,q)%&gt;%\n   mutate(mean_rt = mean(MRT)) %&gt;% \n  ungroup() %&gt;% \n  tidyr::pivot_wider(names_from = \"blur\", values_from = \"MRT\") %&gt;%\n  mutate(diff=LB-C)\n\n\np2 &lt;- ggplot(v_clb, aes(x = mean_rt, y = diff)) + facet_grid(~frequency) + \n  geom_abline(intercept = 0, slope = 0) +\n  geom_line(size = 1, colour = \"black\") +\n  geom_point(size = 3, colour = \"black\") +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  theme(axis.title = element_text(size = 16, face = \"bold\"), \n        axis.text = element_text(size = 16),\n        plot.title = element_text(face = \"bold\", size = 20)) +\nscale_y_continuous(breaks=seq(0, 100, 20)) +\n    coord_cartesian(ylim = c(0, 100)) +\n  scale_x_continuous(breaks=seq(600,1100, 100))+\n  geom_label_repel(data=v_clb, aes(y=diff, label=round(diff,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5) + \n  labs( title = \"Clear - Low Blur\", x = \"Mean RT per quantile\", y = \"Differences\")\n\n\np2\n\n\n\n\n\n\n\n5.12.5 High Blur vs. Low Blur\n\n\nCode\nv_hlb &lt;- v %&gt;%\n  dplyr::filter(blur==\"HB\" | blur==\"LB\") %&gt;%\n  dplyr::group_by(frequency,q)%&gt;%\n   mutate(mean_rt = mean(MRT)) %&gt;% \n  ungroup() %&gt;% \n  tidyr::pivot_wider(names_from = \"blur\", values_from = \"MRT\") %&gt;%\n  mutate(diff=HB-LB)\n\n\np3 &lt;- ggplot(v_hlb, aes(x = mean_rt, y = diff)) + \n  facet_grid(~frequency) + \n  geom_abline(intercept = 0, slope = 0) +\n  geom_line(size = 1, colour = \"black\") +\n  geom_point(size = 3, colour = \"black\") +\n  theme_bw() + \n  theme(legend.position = \"none\") + \n  theme(axis.title = element_text(size = 16, face = \"bold\"), \n        axis.text = element_text(size = 16),\n        plot.title = element_text(face = \"bold\", size = 20)) +\n  scale_x_continuous(breaks=seq(600,1100, 100))+\n  geom_label_repel(data=v_hlb, aes(y=diff, label=round(diff,0)), color=\"black\", min.segment.length = 0, seed = 42, box.padding = 0.5) + \n  labs( title = \"High Blur - Low Blur\", x = \"Mean RT per quantile\", y = \"Group differences\")\n\n\np3\n\n\n\n\n\n\n\n5.12.6 Quantile/delta summary plot\n\n\nCode\nbottom &lt;- cowplot::plot_grid(p1, p2,p3, \n                   ncol = 3, \n                   nrow = 1,\n                   label_size = 14, \n                   hjust = -0.8, \n                   scale=.95,\n                   align = \"v\")\n\ncowplot::plot_grid(p, bottom, \n                   ncol=1, nrow=2)\n\n\n\n\n\nGroup RT distributions in the blurring and word frequency manipulations in word stimuli. Top: Each point represents the average RT quantiles (.1, .3, 0.5, 0.7,and 0.9) in each condition. Bottom: These values were obtained by computing the quantiles for each participant and subsequently averaging the obtained valuesfor each quantile over the participants"
  },
  {
    "objectID": "Experiment3.html#brm-conditionalized-memory",
    "href": "Experiment3.html#brm-conditionalized-memory",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.13 BRM: Conditionalized Memory",
    "text": "5.13 BRM: Conditionalized Memory\n\n\\(D\\prime\\)\n\n\n\nCode\nmem_sc &lt;- read_csv(\"https://osf.io/eapu5/download\")\n\nhead(mem_sc)\n\n\n# A tibble: 6 × 11\n   ...1 participant   target frequency blur  study    rt  corr sayold condition1\n  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1     1 54847f1cfdf9… AWAKE  LOW       HB    old    1.46     1      1 High Blur \n2     2 54847f1cfdf9… BOTHER HIGH      LB    old    1.45     1      1 Low Blur  \n3     3 54847f1cfdf9… BOW    LOW       C     old    1.45     0      0 Clear     \n4     4 54847f1cfdf9… BUCKLE LOW       C     old    1.45     1      1 Clear     \n5     5 54847f1cfdf9… DEAD   HIGH      LB    old    1.46     1      1 Low Blur  \n6     6 54847f1cfdf9… DIRTY  HIGH      LB    old    1.44     1      1 Low Blur  \n# ℹ 1 more variable: isold &lt;dbl&gt;\n\n\n\n5.13.1 Contrasts\n\n\nCode\n#hypothesis\nblurC &lt;-hypr(HB~C, HB~LB,levels=c(\"C\", \"HB\", \"LB\"))\nblurC\n\n\nhypr object containing 2 null hypotheses:\nH0.1: 0 = HB - C\nH0.2: 0 = HB - LB\n\nCall:\nhypr(~HB - C, ~HB - LB, levels = c(\"C\", \"HB\", \"LB\"))\n\nHypothesis matrix (transposed):\n   [,1] [,2]\nC  -1    0  \nHB  1    1  \nLB  0   -1  \n\nContrast matrix:\n   [,1] [,2]\nC  -2/3  1/3\nHB  1/3  1/3\nLB  1/3 -2/3\n\n\nCode\nHF_cont &lt;- hypr(HIGH~LOW,levels=c(\"HIGH\", \"LOW\"))\nHF_cont\n\n\nhypr object containing one (1) null hypothesis:\nH0.1: 0 = HIGH - LOW\n\nCall:\nhypr(~HIGH - LOW, levels = c(\"HIGH\", \"LOW\"))\n\nHypothesis matrix (transposed):\n     [,1]\nHIGH  1  \nLOW  -1  \n\nContrast matrix:\n     [,1]\nHIGH  1/2\nLOW  -1/2\n\n\n\n\nCode\n#set contrasts in df \nmem_sc$blur&lt;-as.factor(mem_sc$blur)\n\ncontrasts(mem_sc$blur) &lt;-contr.hypothesis(blurC)\n\nmem_sc$frequency&lt;-as.factor(mem_sc$frequency)\n\ncontrasts(mem_sc$frequency) &lt;-contr.hypothesis(HF_cont)"
  },
  {
    "objectID": "Experiment3.html#brm-model-memory-conditionalzied",
    "href": "Experiment3.html#brm-model-memory-conditionalzied",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.14 BRM Model: Memory Conditionalzied",
    "text": "5.14 BRM Model: Memory Conditionalzied\n\n\nCode\nprior_exp &lt;- c(set_prior(\"cauchy(0,.35)\", class = \"b\"))\n\nfit_sc_mem &lt;- brm(sayold ~ isold*blur*frequency + (1+isold*blur*frequency|participant) + (1+isold*blur*frequency|target), data=mem_sc, \nwarmup = 1000,\n                    iter = 5000,\n                    chains = 4, \n                    init=0, \n                    family = bernoulli(link = \"probit\"),\n                    cores = 4, \ncontrol = list(adapt_delta = 0.9),\nprior=prior_exp, \nsample_prior = T, \nsave_pars = save_pars(all=T),\nbackend=\"cmdstanr\", \nthreads = threading(4))\n\n\n\n5.14.1 Marginal Means and Differences\n\n\nCode\nfit_sc_mem &lt;- read_rds(\"https://osf.io/wn79f/download\")\n\nfit_sc_mem_lb &lt;- read_rds(\"https://osf.io/c8bqh/download\")\n\n\n\n\nCode\n#| code-fold: show\nemm_m2_d1 &lt;- emmeans(fit_sc_mem, ~isold | blur * frequency) %&gt;% \n  contrast(\"revpairwise\")\n\nemm_m2_d2 &lt;- emmeans(fit_sc_mem, ~isold + blur * frequency) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\"), by = \"frequency\")\n\n# (Negative) criteria\nemm_m2_c1 &lt;- emmeans(fit_sc_mem, ~blur * frequency)\nemm_m2_c2 &lt;- emmeans(fit_sc_mem, ~blur | frequency) %&gt;% \n  contrast(\"pairwise\")\n\n\n\n\nCode\ntmp &lt;- bind_rows(\n  bind_rows(\n    gather_emmeans_draws(emm_m2_d1) %&gt;% \n      group_by(blur, frequency) %&gt;% \n      select(-contrast),\n    gather_emmeans_draws(emm_m2_d2) %&gt;% \n      rename(\n        blur = blur_pairwise\n      ) %&gt;% \n      group_by(blur, frequency) %&gt;% \n      select(-isold_revpairwise)\n  ),\n  bind_rows(\n    gather_emmeans_draws(emm_m2_c1),\n    gather_emmeans_draws(emm_m2_c2) %&gt;% \n      rename(\n        blur = contrast\n      )\n  ),\n  .id = \"Parameter\"\n) %&gt;% \n  ungroup() %&gt;% \n  mutate(Parameter = factor(Parameter, labels = c(\"dprime\", \"Criterion\"))) %&gt;% \n    mutate(\n    t = if_else(str_detect(blur, \" - \"), \"Differences\", \"Group means\") %&gt;% \n      fct_inorder(),\n    blur = fct_inorder(blur)\n  ) \n  \ntmp %&gt;%   \n  mutate(.value = if_else(Parameter == \"Criterion\", .value * -1, .value)) %&gt;% \n  mutate(Parameter = fct_rev(Parameter)) %&gt;% \n  ggplot(aes(blur, .value, slab_fill = frequency)) +\n  labs(\n    x = \"Blurring Level (or difference)\",\n    y = \"Parameter value\"\n  ) +\n  geom_hline(yintercept = 0, linewidth = .25) +\n  scale_x_continuous(\n    breaks = 1:6,\n    labels = unique(tmp$blur)\n  ) +\n  scale_slab_alpha_discrete(range = c(1, .5)) +\n  stat_halfeye(\n    normalize = \"xy\",\n    width = 0.33,\n    slab_color = \"black\",\n    interval_size_range = c(0.2, 1),\n    .width = c(0.66, 0.95), \n    aes(\n      side = ifelse(frequency==\"HIGH\", \"left\", \"right\"),\n      x = ifelse(frequency == \"HIGH\", as.numeric(blur)-.08, as.numeric(blur)+.08)\n      )\n  ) +\n  guides(slab_alpha = \"none\") +\n  facet_grid(Parameter~t, scales = \"free\")\n\n\n\n\n\nFigure 5.1: Posterior distributions and 95%CIs of the criterion and dprime parameters, or differences therein, from Model 2.\n\n\n\n\n\n\nCode\na = hypothesis(fit_sc_mem , \"isold1:blur1 &gt; 0\")\n\nb = hypothesis(fit_sc_mem , \"isold1:blur2 &gt; 0\")\n\nc = hypothesis(fit_sc_mem_lb, \"isold1:blur1 = 0\")\n\nd= hypothesis(fit_sc_mem, \"frequency1 &gt; 0\")\n\ne = hypothesis(fit_sc_mem, \"isold1:blur1:frequency1 &gt; 0\")\n\nf = hypothesis(fit_sc_mem, \"isold1:blur2:frequency1 &gt; 0\")\n\ng = hypothesis(fit_sc_mem_lb, \"isold1:blur1:frequency1 = 0\")\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis, g$hypothesis) %&gt;%\n    mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\n\ntab %&gt;% \n   mutate(Hypothesis = c(\"High Blur - Clear &gt; 0\", \"High Blur-Low Blur &gt; 0\", \"Low Blur - Clear = 0\",\"Low Frequency - High Frequency\",  \"(High Blur-Clear) - (Low Frequency-High Frequency) &gt; 0\", \"(High Blur-Low Blur) - (Low Frequency-High Frequency) &gt; 0\", \"(Low Blur-Clear) - (Low Frequency-High Frequency) &gt; 0\")) %&gt;% \n  gt(caption=md(\"Table: Experiment 3 Memory Sensitivity D-prime\")) %&gt;%\n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Experiment 3 Memory Sensitivity D-prime\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n    \n  \n  \n    High Blur - Clear &gt; 0\n0.070\n0.025\n0.028\n0.112\n409.256\n0.998\n    High Blur-Low Blur &gt; 0\n0.087\n0.027\n0.044\n0.131\n1999.000\n1.000\n    Low Blur - Clear = 0\n-0.017\n0.026\n-0.068\n0.034\n4.434\n0.816\n    Low Frequency - High Frequency\n0.075\n0.039\n0.011\n0.138\n35.281\n0.972\n    (High Blur-Clear) - (Low Frequency-High Frequency) &gt; 0\n0.142\n0.051\n0.061\n0.226\n483.848\n0.998\n    (High Blur-Low Blur) - (Low Frequency-High Frequency) &gt; 0\n0.041\n0.051\n-0.044\n0.126\n3.651\n0.785\n    (Low Blur-Clear) - (Low Frequency-High Frequency) &gt; 0\n0.099\n0.052\n-0.003\n0.202\n0.451\n0.311\n  \n  \n  \n\n\n\n\n\n5.14.1.1 Frequency\n\n\nCode\nlibrary(emmeans)\n# Dprimes for three groups\nemm_freq &lt;- emmeans(fit_sc_mem, ~isold + frequency) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\")) %&gt;% \n  parameters::parameters(centrality = \"mean\")\n\n\n\n5.14.1.1.1 Blur\n\n\nCode\n# Dprimes for three groups\nemm_blur &lt;- emmeans(fit_sc_mem, ~isold + blur) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\")) %&gt;% \n  parameters::parameters(centrality = \"mean\") %&gt;%\n  flextable()\n\n\n\n\n5.14.1.1.2 Blur * Frequency\n\n\nCode\nemm_m2_d2 &lt;- emmeans(fit_sc_mem, ~isold + blur * frequency) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\"), by = \"frequency\") %&gt;%\n    parameters::parameters(centrality = \"mean\")\n\nemm_m2_d2\n\n\nParameter                |     Mean |   Mean.1 |         95% CI |     pd\n------------------------------------------------------------------------\nold - new, C - HB, HIGH  |    -0.14 |    -0.14 | [-0.21, -0.07] |   100%\nold - new, C - LB, HIGH  |    -0.03 |    -0.03 | [-0.11,  0.04] | 81.74%\nold - new, HB - LB, HIGH |     0.11 |     0.11 | [ 0.03,  0.18] | 99.78%\nold - new, C - HB, LOW   | 8.93e-04 | 8.93e-04 | [-0.07,  0.07] | 51.14%\nold - new, C - LB, LOW   |     0.07 |     0.07 | [-0.01,  0.14] | 95.45%\nold - new, HB - LB, LOW  |     0.07 |     0.07 | [-0.01,  0.14] | 96.21%\n\n\n\n\n\n\n5.14.2 Write-up\n\n\n5.14.3 Recognition memory (conditionalized)\nLow frequency words were better recognized than high frequency words, $\\beta$ = 0.075, 95% Cr.I[0.011, 0.138], ER = 35.281. Similar to Experiments 1a and 1a, there was better memory recognition for high blurred words compared to clear words, $\\beta$ = 0.07, 95% Cr.I[0.028, 0.112], ER = 409.256, and low blurred words, $b$ = 0.087, 95% Cr.I[0.044, 0.131], ER = 1999. There was no recognition memory difference between clear and low blur words, $\\beta$ = -0.017, 95% Cr.I[-0.068, 0.034], ER = 4.434. There was strong evidence for an interaction between high blurred words (vs. clear) and frequency, $\\beta$ = 0.142, 95% Cr.I[0.061, 0.226], ER = 483.848, with better memory for high frequency-high blurred words, $\\beta$ = -0.14, 95% Cr.I[-0.21, -0.07]. There was some evidence of an interaction between blurring and frequency for high blurred words vs. low blurred words, $\\beta$ = 0.041, 95% Cr.I[-0.044, 0.126], ER = 3.651. High frequency-high blurred words were better recognized than high-frequency-low blurred words, $b$ = 0.11, 95% Cr.I[0.04, 0.18]. There was also an interaction between frequency and low blurred vs. clear words, $\\beta$= 0.099, 95% Cr.I[-0.003, 0.202], ER = 4.434. For low frequency words, clear words were remembered better than low blurred words."
  },
  {
    "objectID": "Experiment3.html#brm-model-memory-unconditionalzied",
    "href": "Experiment3.html#brm-model-memory-unconditionalzied",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "5.15 BRM Model: Memory Unconditionalzied",
    "text": "5.15 BRM Model: Memory Unconditionalzied\n\n\nCode\nprior_exp &lt;- c(set_prior(\"cauchy(0,.35)\", class = \"b\"))\n\nfit_sc_mem &lt;- brm(sayold ~ isold*blur*frequency + (1+isold*blur*frequency|participant) + (1+isold*blur*frequency|target), data=mem_sc, \nwarmup = 1000,\n                    iter = 5000,\n                    chains = 4, \n                    init=0, \n                    family = bernoulli(link = \"probit\"),\n                    cores = 4, \ncontrol = list(adapt_delta = 0.9),\nprior=prior_exp, \nsample_prior = T, \nsave_pars = save_pars(all=T),\nbackend=\"cmdstanr\", \nthreads = threading(4))\n\n\n\n5.15.1 Marginal Means and Differences\n\n\nCode\nfit_sc_mem_uc &lt;- read_rds(\"https://osf.io/ghv2s/download\")\n\n\n\n\nCode\n#| code-fold: show\nemm_m2_d1 &lt;- emmeans(fit_sc_mem_uc, ~isold | blur * frequency) %&gt;% \n  contrast(\"revpairwise\")\n\nemm_m2_d2 &lt;- emmeans(fit_sc_mem_uc, ~isold + blur * frequency) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\"), by = \"frequency\")\n\n# (Negative) criteria\nemm_m2_c1 &lt;- emmeans(fit_sc_mem_uc, ~blur * frequency)\nemm_m2_c2 &lt;- emmeans(fit_sc_mem_uc, ~blur | frequency) %&gt;% \n  contrast(\"pairwise\")\n\n\n\n\nCode\ntmp &lt;- bind_rows(\n  bind_rows(\n    gather_emmeans_draws(emm_m2_d1) %&gt;% \n      group_by(blur, frequency) %&gt;% \n      select(-contrast),\n    gather_emmeans_draws(emm_m2_d2) %&gt;% \n      rename(\n        blur = blur_pairwise\n      ) %&gt;% \n      group_by(blur, frequency) %&gt;% \n      select(-isold_revpairwise)\n  ),\n  bind_rows(\n    gather_emmeans_draws(emm_m2_c1),\n    gather_emmeans_draws(emm_m2_c2) %&gt;% \n      rename(\n        blur = contrast\n      )\n  ),\n  .id = \"Parameter\"\n) %&gt;% \n  ungroup() %&gt;% \n  mutate(Parameter = factor(Parameter, labels = c(\"dprime\", \"Criterion\"))) %&gt;% \n   mutate(\n    t = if_else(str_detect(blur, \" - \"), \"Differences\", \"Group means\") %&gt;% \n      fct_inorder(),\n    blur = fct_inorder(blur)\n  ) \ntmp %&gt;%   \n  mutate(.value = if_else(Parameter == \"Criterion\", .value * -1, .value)) %&gt;% \n  mutate(Parameter = fct_rev(Parameter)) %&gt;% \n  ggplot(aes(blur, .value, slab_fill = frequency)) +\n  labs(\n    x = \"Blurring Level (or difference)\",\n    y = \"Parameter value\"\n  ) +\n  geom_hline(yintercept = 0, linewidth = .25) +\n  scale_x_continuous(\n    breaks = 1:6,\n    labels = unique(tmp$blur)\n  ) +\n  scale_slab_alpha_discrete(range = c(1, .5)) +\n  stat_halfeye(\n    normalize = \"xy\",\n    width = 0.44,\n    slab_color = \"black\",\n    interval_size_range = c(0.2, 1),\n    .width = c(0.66, 0.95),\n    aes(\n      side = ifelse(frequency==\"HIGH\", \"left\", \"right\"),\n      x = ifelse(frequency == \"HIGH\", as.numeric(blur)-.08, as.numeric(blur)+.08)\n      )\n  ) +\n  guides(slab_alpha = \"none\") +\n  facet_grid(Parameter~t, scales = \"free\")\n\n\n\n\n\nFigure 5.2: Posterior distributions and 95%CIs of the criterion and dprime parameters, or differences therein, from unconditonalized model.\n\n\n\n\n\n\nCode\na = hypothesis(fit_sc_mem_uc , \"isold:blur1 &gt; 0\")\n\nb = hypothesis(fit_sc_mem_uc , \"isold:blur2 &gt; 0\")\n\nc = hypothesis(fit_sc_mem_uc, \"isold:blur1 &gt; 0\")\n\nd= hypothesis(fit_sc_mem_uc, \"frequency1 &gt; 0\")\n\ne = hypothesis(fit_sc_mem_uc, \"isold:blur1:frequency1 &gt; 0\")\n\nf = hypothesis(fit_sc_mem_uc , \"isold:blur2:frequency1 &gt; 0\")\n\ng = hypothesis(fit_sc_mem_uc, \"isold:blur1:frequency1 &gt; 0\")\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis, g$hypothesis) %&gt;%\n    mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\n\ntab %&gt;% \n   mutate(Hypothesis = c(\"High Blur - Clear &gt; 0\", \"High Blur-Low Blur &gt; 0\", \"Low Blur - Clear = 0\",\"Low Frequency - High Frequency\",  \"(High Blur-Clear) - (Low Frequency-High Frequency) &gt; 0\", \"(High Blur-Low Blur) - (Low Frequency-High Frequency) &gt; 0\", \"(Low Blur-Clear) - (Low Frequency-High Frequency) &gt; 0\")) %&gt;% \n  gt(caption=md(\"Table: Experiment 3 Memory Sensitivity D'\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  )\n\n\n\n\n\n\n  Table: Experiment 3 Memory Sensitivity D’\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n    \n  \n  \n    High Blur - Clear &gt; 0\n0.058\n0.025\n0.017\n0.100\n91.49\n0.989\n    High Blur-Low Blur &gt; 0\n0.072\n0.026\n0.031\n0.115\n409.26\n0.998\n    Low Blur - Clear = 0\n0.058\n0.025\n0.017\n0.100\n91.49\n0.989\n    Low Frequency - High Frequency\n0.222\n0.054\n0.132\n0.312\n5332.33\n1.000\n    (High Blur-Clear) - (Low Frequency-High Frequency) &gt; 0\n0.145\n0.049\n0.066\n0.225\n726.27\n0.999\n    (High Blur-Low Blur) - (Low Frequency-High Frequency) &gt; 0\n0.045\n0.050\n-0.038\n0.128\n4.49\n0.818\n    (Low Blur-Clear) - (Low Frequency-High Frequency) &gt; 0\n0.145\n0.049\n0.066\n0.225\n726.27\n0.999\n  \n  \n  \n\n\n\n\n\n5.15.1.1 Frequency\n\n\nCode\nlibrary(emmeans)\n# Dprimes for three groups\nemm_freq &lt;- emmeans(fit_sc_mem, ~isold + frequency) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\")) %&gt;% \n  parameters::parameters(centrality = \"mean\")\n\n\n\n5.15.1.1.1 Blur\n\n\nCode\n# Dprimes for three groups\nemm_blur &lt;- emmeans(fit_sc_mem, ~isold + blur) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\")) %&gt;% \n  parameters::parameters(centrality = \"mean\") %&gt;%\n  flextable()\n\n\n\n\n5.15.1.1.2 Blur * Frequency\n\n\nCode\nemm_m2_d2 &lt;- emmeans(fit_sc_mem_uc, ~isold + blur * frequency) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\"), by = \"frequency\") %&gt;%\n    parameters::parameters(centrality = \"mean\")\n\nemm_m2_d2\n\n\nParameter            |  Mean | Mean.1 |         95% CI |     pd\n---------------------------------------------------------------\n1 - 0, C - HB, HIGH  | -0.13 |  -0.13 | [-0.20, -0.06] | 99.99%\n1 - 0, C - LB, HIGH  | -0.04 |  -0.04 | [-0.11,  0.03] | 83.93%\n1 - 0, HB - LB, HIGH |  0.09 |   0.09 | [ 0.03,  0.16] | 99.59%\n1 - 0, C - HB, LOW   |  0.01 |   0.01 | [-0.05,  0.08] | 65.49%\n1 - 0, C - LB, LOW   |  0.06 |   0.06 | [-0.01,  0.14] | 95.88%\n1 - 0, HB - LB, LOW  |  0.05 |   0.05 | [-0.02,  0.12] | 91.49%\n\n\n\n\n\n\n5.15.2 Write-up\n\n5.15.2.1 Recognition Memory (Unconditionalized)\nHigh frequency words were better recognized compared to high frequency words, $b$ = 0.222, 95% Cr.I[0.132, 0.312], ER = 5332.333. Similar to Experiments 1A and 1B, there was better memory recognition for high blurred words compared to clear words, $b$= 0.058, 95% Cr.I[0.017, 0.1], ER = 91.486, and low blurred words, $b$ = 0.072, 95% Cr.I[0.031, 0.115], ER = 409.256. There was no recognition memory difference between clear and low blur words,b = 0.058, 95% Cr.I[0.017, 0.1], ER = 91.486. There was strong evidence for an interaction between high blurred words (vs. clear) and frequency on sensitivity, $b$ = 0.145, 95% Cr.I[0.066, 0.225], ER = 726.273, with better memory for high frequency =high blurred words compared to clear words, $b$ = -0.13, 95% Cr.I[-0.2, -0.06]. There was some evidence of an interaction between blurring and frequency for high blurred words vs. low blurred words, $b$ = 0.045, 95% Cr.I[-0.038, 0.128], ER = 4.485. Specifically, high frequency-high blurred words were better recognized than high-frequency-low blurred words, $b$ = 0.09, 95% Cr.I[0.03, 0.16]. There was no interaction between frequency and low blurred vs. clear word comparison, $b$ = 0.145, 95% Cr.I[0.066, 0.225], ER = 91.486."
  },
  {
    "objectID": "Experiment3.html#marginal-means-and-differences-2",
    "href": "Experiment3.html#marginal-means-and-differences-2",
    "title": "5  Experiment 3: Semantic Categorization Word Frequency and Disfluency",
    "section": "6.1 Marginal Means and Differences",
    "text": "6.1 Marginal Means and Differences\n\n\nCode\nwf_mem_ldt &lt;- read_csv(\"https://osf.io/cu6y9/download\")\n\nhead(wf_mem_ldt)\n\n\n# A tibble: 6 × 13\n   ...1 participant Target freq  blur  date  study    rt  corr sayold condition1\n  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1     1 2023-05-02… BELOW  Low   HB    2023… old   0.607     0      0 High Blur \n2     2 2023-05-02… BOW    Low   C     2023… old   0.656     1      1 Clear     \n3     3 2023-05-02… BREAT… High  LB    2023… old   0.626     1      1 Low Blur  \n4     4 2023-05-02… BUCKLE Low   C     2023… old   0.836     1      1 Clear     \n5     5 2023-05-02… COW    Low   LB    2023… old   0.900     1      1 Low Blur  \n6     6 2023-05-02… CROCO… Low   LB    2023… old   0.566     1      1 Low Blur  \n# ℹ 2 more variables: frequency &lt;chr&gt;, isold &lt;dbl&gt;\n\n\n\n\nCode\ntmp &lt;- tempdir()\ndownload.file(\"https://osf.io/3avcy/download\", \n              file.path(tmp, \"wf_blmm_sdt_cond.rda\"))\nload(file.path(tmp, \"wf_blmm_sdt_cond.rda\"))\n\nfit_wf_mem_ldt_lb &lt;- read_rds(\"https://osf.io/yv9qd/download\")\n\n\n\n\nCode\n#| code-fold: show\nemm_m2_d1 &lt;- emmeans(fit_wf_mem, ~isold | blur * freq) %&gt;% \n  contrast(\"revpairwise\")\n\nemm_m2_d2 &lt;- emmeans(fit_wf_mem, ~isold + blur * freq) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\"), by = \"freq\")\n\n# (Negative) criteria\nemm_m2_c1 &lt;- emmeans(fit_wf_mem, ~blur * freq)\nemm_m2_c2 &lt;- emmeans(fit_wf_mem, ~blur | freq) %&gt;% \n  contrast(\"pairwise\")\n\n\n\n\nCode\ntmp &lt;- bind_rows(\n  bind_rows(\n    gather_emmeans_draws(emm_m2_d1) %&gt;% \n      group_by(blur, freq) %&gt;% \n      select(-contrast),\n    gather_emmeans_draws(emm_m2_d2) %&gt;% \n      rename(\n        blur = blur_pairwise\n      ) %&gt;% \n      group_by(blur, freq) %&gt;% \n      select(-isold_revpairwise)\n  ),\n  bind_rows(\n    gather_emmeans_draws(emm_m2_c1),\n    gather_emmeans_draws(emm_m2_c2) %&gt;% \n      rename(\n        blur = contrast\n      )\n  ),\n  .id = \"Parameter\"\n) %&gt;% \n  ungroup() %&gt;%\n  mutate(Parameter = factor(Parameter, labels = c(\"dprime\", \"Criterion\"))) %&gt;% \n  mutate(\n    t = if_else(str_detect(blur, \" - \"), \"Differences\", \"Group means\") %&gt;% \n      fct_inorder(),\n    blur = fct_inorder(blur)\n  ) \ntmp %&gt;%   \n  mutate(.value = if_else(Parameter == \"Criterion\", .value * -1, .value)) %&gt;% \n  mutate(Parameter = fct_rev(Parameter)) %&gt;% \n  ggplot(aes(blur, .value, slab_fill = freq)) +\n  labs(\n    x = \"Blurring Level (or difference)\",\n    y = \"Parameter value\"\n  ) +\n  geom_hline(yintercept = 0, linewidth = .25) +\n  scale_x_continuous(\n    breaks = 1:6,\n    labels = unique(tmp$blur)\n  ) +\n  scale_slab_alpha_discrete(range = c(1, .5)) +\n  stat_halfeye(\n    normalize = \"xy\",\n    width = 0.33,\n    slab_color = \"black\",\n    linewidth = 0.4,\n    interval_size_range = c(0.2, 1),\n    .width = c(0.66, 0.95), \n    aes(\n      side = ifelse(freq == \"High\", \"left\", \"right\"),\n      x = ifelse(freq == \"High\", as.numeric(blur)-.1, as.numeric(blur)+.1)\n      )\n  ) +\n  guides(slab_alpha = \"none\") +\n  facet_grid(Parameter~t, scales = \"free\")\n\n\n\n\n\nFigure 6.1: Posterior distributions and 95%CIs of the criterion and dprime parameters, or differences therein, from unconditonalized model.\n\n\n\n\n\n\nCode\na = hypothesis(fit_wf_mem , \"isold:blur1 &gt; 0\")\n\nb = hypothesis(fit_wf_mem , \"isold:blur2 &gt; 0\")\n\nc = hypothesis(fit_wf_mem_ldt_lb, \"isold1:blur1 = 0\")\n\nd= hypothesis(fit_wf_mem, \"freq1 &gt; 0\")\n\ne = hypothesis(fit_wf_mem, \"isold:blur1:freq1 &gt; 0\")\n\nf = hypothesis(fit_wf_mem , \"isold:blur2:freq1 &gt; 0\")\n\ng = hypothesis(fit_wf_mem_ldt_lb, \"isold1:blur1:freq1 &gt; 0\")\n\ntab &lt;- bind_rows(a$hypothesis, b$hypothesis, c$hypothesis, d$hypothesis, e$hypothesis, f$hypothesis, g$hypothesis) %&gt;%\n    mutate(Evid.Ratio=as.numeric(Evid.Ratio))%&gt;%\n  select(-Star)\n\ntab[, -1] &lt;- t(apply(tab[, -1], 1, round, digits = 3))\n\n\ntab %&gt;% \n   mutate(Hypothesis = c(\"High Blur - Clear &gt; 0\", \"High Blur-Low Blur &gt; 0\", \"Low Blur - Clear = 0\",\"Low Frequency - High Frequency\",  \"(High Blur-Clear) - (Low Frequency-High Frequency) &gt; 0\", \"(High Blur-Low Blur) - (Low Frequency-High Frequency) &gt; 0\", \"(Low Blur-Clear) - (Low Frequency-High Frequency) &gt; 0\")) %&gt;% \n  gt(caption=md(\"Table: Experiment 3 Memory Sensitivity D'\")) %&gt;% \n  cols_align(\n    columns=-1,\n    align=\"right\"\n  ) \n\n\n\n\n\n\n  Table: Experiment 3 Memory Sensitivity D’\n  \n    \n    \n      Hypothesis\n      Estimate\n      Est.Error\n      CI.Lower\n      CI.Upper\n      Evid.Ratio\n      Post.Prob\n    \n  \n  \n    High Blur - Clear &gt; 0\n0.060\n0.023\n0.023\n0.097\n252.968\n0.996\n    High Blur-Low Blur &gt; 0\n0.096\n0.022\n0.060\n0.133\nInf\n1.000\n    Low Blur - Clear = 0\n-0.037\n0.022\n-0.079\n0.006\n2.049\n0.672\n    Low Frequency - High Frequency\n0.255\n0.051\n0.172\n0.340\nInf\n1.000\n    (High Blur-Clear) - (Low Frequency-High Frequency) &gt; 0\n0.051\n0.043\n-0.019\n0.121\n7.611\n0.884\n    (High Blur-Low Blur) - (Low Frequency-High Frequency) &gt; 0\n0.103\n0.043\n0.032\n0.175\n124.984\n0.992\n    (Low Blur-Clear) - (Low Frequency-High Frequency) &gt; 0\n-0.051\n0.043\n-0.121\n0.020\n0.133\n0.118\n  \n  \n  \n\n\n\n\n\n6.1.0.1 Frequency\n\n\nCode\nlibrary(emmeans)\n# Dprimes for three groups\nemm_freq &lt;- emmeans(fit_sc_mem, ~isold + frequency) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\")) %&gt;% \n  parameters::parameters(centrality = \"mean\")\n\n\n\n6.1.0.1.1 Blur\n\n\nCode\n# Dprimes for three groups\nemm_blur &lt;- emmeans(fit_sc_mem, ~isold + blur) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\")) %&gt;% \n  parameters::parameters(centrality = \"mean\") %&gt;%\n  flextable()\n\n\n\n\n6.1.0.1.2 Blur * Frequency\n\n\nCode\nemm_m2_d2 &lt;- emmeans(fit_sc_mem_uc, ~isold + blur * frequency) %&gt;% \n  contrast(interaction = c(\"revpairwise\", \"pairwise\"), by = \"frequency\") %&gt;%\n    parameters::parameters(centrality = \"mean\")\n\nemm_m2_d2\n\n\nParameter            |  Mean | Mean.1 |         95% CI |     pd\n---------------------------------------------------------------\n1 - 0, C - HB, HIGH  | -0.13 |  -0.13 | [-0.20, -0.06] | 99.99%\n1 - 0, C - LB, HIGH  | -0.04 |  -0.04 | [-0.11,  0.03] | 83.93%\n1 - 0, HB - LB, HIGH |  0.09 |   0.09 | [ 0.03,  0.16] | 99.59%\n1 - 0, C - HB, LOW   |  0.01 |   0.01 | [-0.05,  0.08] | 65.49%\n1 - 0, C - LB, LOW   |  0.06 |   0.06 | [-0.01,  0.14] | 95.88%\n1 - 0, HB - LB, LOW  |  0.05 |   0.05 | [-0.02,  0.12] | 91.49%"
  },
  {
    "objectID": "General_Discussion.html#conclusion",
    "href": "General_Discussion.html#conclusion",
    "title": "6  General Discussion",
    "section": "6.1 Conclusion",
    "text": "6.1 Conclusion\nOur paper contributes nuanced insights to the intricate relationship between perceptual disfluency and memory encoding. We have shown that perceptual disfluency can aid in memory retention, but its efficacy is contingent upon the degree of disfluency and other contextual factors such as word frequency. Our findings endorse the stage-specific account, emphasizing the role of cognitive control mechanisms in the observed memory advantages with perceptual disfluency. Furthermore, our methodological contributions, employing an ex-Gaussian model and DDM, not only validate the benefits of examining RT distributions, but also open new avenues for future research in learning and memory studies. We caution, however, that the applicability of these findings in real-world educational settings remains an open question, and the effect sizes observed were relatively small, thus warranting further investigation.\nUltimately, this work stands as a call to action for a more comprehensive, nuanced approach to studying perceptual disfluency, incorporating both advanced statistical methods and a more exhaustive range of experimental conditions to better elucidate when and how disfluency can facilitate memory.\n\n\n\n\n\n\nCastel, A. D., Nazarian, M., & Blake, A. B. (n.d.). Attention and incidental memory in everyday settings PROPERTY OF THE MIT PRESS FOR PROOFREADING, INDEXING, AND PROMOTIONAL PURPOSES ONLY.\n\n\nGeller, J., Davis, S. D., & Peterson, D. J. (2020). Sans forgetica is not desirable for learning. Memory, 28(8), 957–967. https://doi.org/10.1080/09658211.2020.1797096\n\n\nGeller, J., & Peterson, D. (2021a). Is this going to be on the test? Test expectancy moderates the disfluency effect with sans forgetica. Journal of Experimental Psychology: Learning, Memory, and Cognition, 47(12), 1924–1938. https://doi.org/10.1037/xlm0001042\n\n\nGeller, J., & Peterson, D. (2021b). Is this going to be on the test? Test expectancy moderates the disfluency effect with sans forgetica. Journal of Experimental Psychology: Learning, Memory, and Cognition, 47(12), 1924–1938. https://doi.org/10.1037/xlm0001042\n\n\nGeller, J., Still, M. L., Dark, V. J., & Carpenter, S. K. (2018). Would disfluency by any other name still be disfluent? Examining the disfluency effect with cursive handwriting. Memory and Cognition, 46(7), 11091126. https://doi.org/10.3758/s13421-018-0824-6\n\n\nHu, X., Yang, C., & Luo, L. (2022). Retrospective confidence rating about memory performance is affected by both retrieval fluency and non-decision time. Metacognition and Learning, 17(2), 651–681. https://doi.org/10.1007/s11409-022-09303-0\n\n\nLehmann, J., Goussios, C., & Seufert, T. (2015). Working memory capacity and disfluency effect: an aptitude-treatment-interaction study. Metacognition and Learning, 11(1), 89–105. https://doi.org/10.1007/s11409-015-9149-z\n\n\nPtok, M. J., Hannah, K. E., & Watter, S. (2020). Memory effects of conflict and cognitive control are processing stage-specific: evidence from pupillometry. Psychological Research, 85(3), 1029–1046. https://doi.org/10.1007/s00426-020-01295-3\n\n\nPtok, M. J., Thomson, S. J., Humphreys, K. R., & Watter, S. (2019). Congruency encoding effects on recognition memory: A stage-specific account of desirable difficulty. Frontiers in Psychology, 10. https://doi.org/10.3389/fpsyg.2019.00858\n\n\nRosner, T. M., Davis, H., & Milliken, B. (2015). Perceptual blurring and recognition memory: A desirable difficulty effect revealed. Acta Psychologica, 160, 11–22. https://doi.org/10.1016/j.actpsy.2015.06.006\n\n\nWenzel, K., & Reinhard, M.-A. (2019). Relatively unintelligent individuals do not benefit from intentionally hindered learning: The role of desirable difficulties. Intelligence, 77, 101405. https://doi.org/10.1016/j.intell.2019.101405"
  }
]